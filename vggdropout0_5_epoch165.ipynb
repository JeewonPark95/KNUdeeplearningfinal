{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vggdropout0.5_epoch165.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrXOqrrPBeTA9atU8juwpP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70ca8a60049e4fa3b88686a4f9316049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9b753691b814b7eacd15ece18ee5e8d",
              "IPY_MODEL_4008e81d6a684fae95b14e622b724a09",
              "IPY_MODEL_933212de18dd44c8b97eac71e7123e86"
            ],
            "layout": "IPY_MODEL_58bd04d46ebc4c898390ce19c421f3cd"
          }
        },
        "a9b753691b814b7eacd15ece18ee5e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0537ba2134724207b9243c0acd7e0244",
            "placeholder": "​",
            "style": "IPY_MODEL_4bacc17395634a7bbd4723b12a0e732b",
            "value": ""
          }
        },
        "4008e81d6a684fae95b14e622b724a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a7df1e065f148f7b1b1c7b2a1418f6e",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_674d0518f7154ca7a9854a0f1defbc0a",
            "value": 170498071
          }
        },
        "933212de18dd44c8b97eac71e7123e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a323ef8b193341d58ee80e1e2de560be",
            "placeholder": "​",
            "style": "IPY_MODEL_9d988addcd6e4bffaa8d52db5fb2a9a9",
            "value": " 170499072/? [00:13&lt;00:00, 14266358.48it/s]"
          }
        },
        "58bd04d46ebc4c898390ce19c421f3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0537ba2134724207b9243c0acd7e0244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bacc17395634a7bbd4723b12a0e732b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a7df1e065f148f7b1b1c7b2a1418f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "674d0518f7154ca7a9854a0f1defbc0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a323ef8b193341d58ee80e1e2de560be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d988addcd6e4bffaa8d52db5fb2a9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeewonPark95/KNUdeeplearningfinal/blob/main/vggdropout0_5_epoch165.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "70ca8a60049e4fa3b88686a4f9316049",
            "a9b753691b814b7eacd15ece18ee5e8d",
            "4008e81d6a684fae95b14e622b724a09",
            "933212de18dd44c8b97eac71e7123e86",
            "58bd04d46ebc4c898390ce19c421f3cd",
            "0537ba2134724207b9243c0acd7e0244",
            "4bacc17395634a7bbd4723b12a0e732b",
            "7a7df1e065f148f7b1b1c7b2a1418f6e",
            "674d0518f7154ca7a9854a0f1defbc0a",
            "a323ef8b193341d58ee80e1e2de560be",
            "9d988addcd6e4bffaa8d52db5fb2a9a9"
          ]
        },
        "id": "yLAxLnRsEvAb",
        "outputId": "aedec28c-d929-4ca6-dcfc-2dde3c43a1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to drive/app/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70ca8a60049e4fa3b88686a4f9316049"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting drive/app/cifar10/cifar-10-python.tar.gz to drive/app/cifar10/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "# TEST : Loss: (0.4446) | Acc: (85.52%) (8552/10000)\n",
            "Epoch: 46 | Batch_idx: 0 |  Loss: (0.4428) | Acc: (86.72%) (111/128)\n",
            "Epoch: 46 | Batch_idx: 10 |  Loss: (0.4256) | Acc: (86.08%) (1212/1408)\n",
            "Epoch: 46 | Batch_idx: 20 |  Loss: (0.3971) | Acc: (86.87%) (2335/2688)\n",
            "Epoch: 46 | Batch_idx: 30 |  Loss: (0.3989) | Acc: (86.54%) (3434/3968)\n",
            "Epoch: 46 | Batch_idx: 40 |  Loss: (0.3881) | Acc: (86.78%) (4554/5248)\n",
            "Epoch: 46 | Batch_idx: 50 |  Loss: (0.3788) | Acc: (86.98%) (5678/6528)\n",
            "Epoch: 46 | Batch_idx: 60 |  Loss: (0.3804) | Acc: (87.05%) (6797/7808)\n",
            "Epoch: 46 | Batch_idx: 70 |  Loss: (0.3775) | Acc: (87.19%) (7924/9088)\n",
            "Epoch: 46 | Batch_idx: 80 |  Loss: (0.3831) | Acc: (86.98%) (9018/10368)\n",
            "Epoch: 46 | Batch_idx: 90 |  Loss: (0.3861) | Acc: (87.01%) (10135/11648)\n",
            "Epoch: 46 | Batch_idx: 100 |  Loss: (0.3827) | Acc: (87.20%) (11273/12928)\n",
            "Epoch: 46 | Batch_idx: 110 |  Loss: (0.3837) | Acc: (87.16%) (12383/14208)\n",
            "Epoch: 46 | Batch_idx: 120 |  Loss: (0.3854) | Acc: (87.14%) (13496/15488)\n",
            "Epoch: 46 | Batch_idx: 130 |  Loss: (0.3865) | Acc: (87.23%) (14626/16768)\n",
            "Epoch: 46 | Batch_idx: 140 |  Loss: (0.3871) | Acc: (87.20%) (15738/18048)\n",
            "Epoch: 46 | Batch_idx: 150 |  Loss: (0.3870) | Acc: (87.18%) (16851/19328)\n",
            "Epoch: 46 | Batch_idx: 160 |  Loss: (0.3882) | Acc: (87.16%) (17961/20608)\n",
            "Epoch: 46 | Batch_idx: 170 |  Loss: (0.3907) | Acc: (87.05%) (19054/21888)\n",
            "Epoch: 46 | Batch_idx: 180 |  Loss: (0.3923) | Acc: (87.03%) (20164/23168)\n",
            "Epoch: 46 | Batch_idx: 190 |  Loss: (0.3924) | Acc: (86.99%) (21267/24448)\n",
            "Epoch: 46 | Batch_idx: 200 |  Loss: (0.3931) | Acc: (86.99%) (22382/25728)\n",
            "Epoch: 46 | Batch_idx: 210 |  Loss: (0.3931) | Acc: (86.98%) (23492/27008)\n",
            "Epoch: 46 | Batch_idx: 220 |  Loss: (0.3940) | Acc: (86.97%) (24601/28288)\n",
            "Epoch: 46 | Batch_idx: 230 |  Loss: (0.3952) | Acc: (86.95%) (25710/29568)\n",
            "Epoch: 46 | Batch_idx: 240 |  Loss: (0.3947) | Acc: (86.98%) (26831/30848)\n",
            "Epoch: 46 | Batch_idx: 250 |  Loss: (0.3949) | Acc: (86.99%) (27947/32128)\n",
            "Epoch: 46 | Batch_idx: 260 |  Loss: (0.3969) | Acc: (86.89%) (29029/33408)\n",
            "Epoch: 46 | Batch_idx: 270 |  Loss: (0.3986) | Acc: (86.87%) (30132/34688)\n",
            "Epoch: 46 | Batch_idx: 280 |  Loss: (0.3978) | Acc: (86.89%) (31251/35968)\n",
            "Epoch: 46 | Batch_idx: 290 |  Loss: (0.3987) | Acc: (86.85%) (32350/37248)\n",
            "Epoch: 46 | Batch_idx: 300 |  Loss: (0.3990) | Acc: (86.85%) (33461/38528)\n",
            "Epoch: 46 | Batch_idx: 310 |  Loss: (0.4008) | Acc: (86.81%) (34559/39808)\n",
            "Epoch: 46 | Batch_idx: 320 |  Loss: (0.4015) | Acc: (86.79%) (35660/41088)\n",
            "Epoch: 46 | Batch_idx: 330 |  Loss: (0.4014) | Acc: (86.78%) (36769/42368)\n",
            "Epoch: 46 | Batch_idx: 340 |  Loss: (0.4004) | Acc: (86.83%) (37901/43648)\n",
            "Epoch: 46 | Batch_idx: 350 |  Loss: (0.4001) | Acc: (86.84%) (39015/44928)\n",
            "Epoch: 46 | Batch_idx: 360 |  Loss: (0.4014) | Acc: (86.81%) (40115/46208)\n",
            "Epoch: 46 | Batch_idx: 370 |  Loss: (0.4024) | Acc: (86.78%) (41210/47488)\n",
            "Epoch: 46 | Batch_idx: 380 |  Loss: (0.4018) | Acc: (86.79%) (42325/48768)\n",
            "Epoch: 46 | Batch_idx: 390 |  Loss: (0.4030) | Acc: (86.76%) (43379/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4085) | Acc: (86.47%) (8647/10000)\n",
            "Epoch: 47 | Batch_idx: 0 |  Loss: (0.3928) | Acc: (87.50%) (112/128)\n",
            "Epoch: 47 | Batch_idx: 10 |  Loss: (0.3680) | Acc: (87.57%) (1233/1408)\n",
            "Epoch: 47 | Batch_idx: 20 |  Loss: (0.3649) | Acc: (87.83%) (2361/2688)\n",
            "Epoch: 47 | Batch_idx: 30 |  Loss: (0.3726) | Acc: (87.65%) (3478/3968)\n",
            "Epoch: 47 | Batch_idx: 40 |  Loss: (0.3708) | Acc: (87.88%) (4612/5248)\n",
            "Epoch: 47 | Batch_idx: 50 |  Loss: (0.3777) | Acc: (87.59%) (5718/6528)\n",
            "Epoch: 47 | Batch_idx: 60 |  Loss: (0.3791) | Acc: (87.64%) (6843/7808)\n",
            "Epoch: 47 | Batch_idx: 70 |  Loss: (0.3833) | Acc: (87.48%) (7950/9088)\n",
            "Epoch: 47 | Batch_idx: 80 |  Loss: (0.3789) | Acc: (87.63%) (9085/10368)\n",
            "Epoch: 47 | Batch_idx: 90 |  Loss: (0.3827) | Acc: (87.55%) (10198/11648)\n",
            "Epoch: 47 | Batch_idx: 100 |  Loss: (0.3953) | Acc: (87.21%) (11275/12928)\n",
            "Epoch: 47 | Batch_idx: 110 |  Loss: (0.3997) | Acc: (87.15%) (12382/14208)\n",
            "Epoch: 47 | Batch_idx: 120 |  Loss: (0.4003) | Acc: (87.11%) (13491/15488)\n",
            "Epoch: 47 | Batch_idx: 130 |  Loss: (0.4006) | Acc: (87.08%) (14601/16768)\n",
            "Epoch: 47 | Batch_idx: 140 |  Loss: (0.4010) | Acc: (87.14%) (15727/18048)\n",
            "Epoch: 47 | Batch_idx: 150 |  Loss: (0.4012) | Acc: (87.10%) (16835/19328)\n",
            "Epoch: 47 | Batch_idx: 160 |  Loss: (0.3972) | Acc: (87.20%) (17971/20608)\n",
            "Epoch: 47 | Batch_idx: 170 |  Loss: (0.3961) | Acc: (87.25%) (19098/21888)\n",
            "Epoch: 47 | Batch_idx: 180 |  Loss: (0.3958) | Acc: (87.27%) (20218/23168)\n",
            "Epoch: 47 | Batch_idx: 190 |  Loss: (0.3936) | Acc: (87.29%) (21341/24448)\n",
            "Epoch: 47 | Batch_idx: 200 |  Loss: (0.3933) | Acc: (87.32%) (22466/25728)\n",
            "Epoch: 47 | Batch_idx: 210 |  Loss: (0.3923) | Acc: (87.35%) (23591/27008)\n",
            "Epoch: 47 | Batch_idx: 220 |  Loss: (0.3934) | Acc: (87.31%) (24698/28288)\n",
            "Epoch: 47 | Batch_idx: 230 |  Loss: (0.3911) | Acc: (87.37%) (25834/29568)\n",
            "Epoch: 47 | Batch_idx: 240 |  Loss: (0.3908) | Acc: (87.34%) (26942/30848)\n",
            "Epoch: 47 | Batch_idx: 250 |  Loss: (0.3925) | Acc: (87.30%) (28049/32128)\n",
            "Epoch: 47 | Batch_idx: 260 |  Loss: (0.3941) | Acc: (87.26%) (29152/33408)\n",
            "Epoch: 47 | Batch_idx: 270 |  Loss: (0.3932) | Acc: (87.27%) (30271/34688)\n",
            "Epoch: 47 | Batch_idx: 280 |  Loss: (0.3925) | Acc: (87.26%) (31387/35968)\n",
            "Epoch: 47 | Batch_idx: 290 |  Loss: (0.3941) | Acc: (87.20%) (32480/37248)\n",
            "Epoch: 47 | Batch_idx: 300 |  Loss: (0.3931) | Acc: (87.20%) (33595/38528)\n",
            "Epoch: 47 | Batch_idx: 310 |  Loss: (0.3940) | Acc: (87.20%) (34712/39808)\n",
            "Epoch: 47 | Batch_idx: 320 |  Loss: (0.3947) | Acc: (87.19%) (35825/41088)\n",
            "Epoch: 47 | Batch_idx: 330 |  Loss: (0.3952) | Acc: (87.21%) (36950/42368)\n",
            "Epoch: 47 | Batch_idx: 340 |  Loss: (0.3958) | Acc: (87.17%) (38049/43648)\n",
            "Epoch: 47 | Batch_idx: 350 |  Loss: (0.3957) | Acc: (87.16%) (39161/44928)\n",
            "Epoch: 47 | Batch_idx: 360 |  Loss: (0.3968) | Acc: (87.11%) (40254/46208)\n",
            "Epoch: 47 | Batch_idx: 370 |  Loss: (0.3963) | Acc: (87.14%) (41383/47488)\n",
            "Epoch: 47 | Batch_idx: 380 |  Loss: (0.3977) | Acc: (87.09%) (42472/48768)\n",
            "Epoch: 47 | Batch_idx: 390 |  Loss: (0.3981) | Acc: (87.07%) (43536/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4501) | Acc: (85.52%) (8552/10000)\n",
            "Epoch: 48 | Batch_idx: 0 |  Loss: (0.4049) | Acc: (88.28%) (113/128)\n",
            "Epoch: 48 | Batch_idx: 10 |  Loss: (0.3456) | Acc: (89.13%) (1255/1408)\n",
            "Epoch: 48 | Batch_idx: 20 |  Loss: (0.3628) | Acc: (88.39%) (2376/2688)\n",
            "Epoch: 48 | Batch_idx: 30 |  Loss: (0.3842) | Acc: (87.68%) (3479/3968)\n",
            "Epoch: 48 | Batch_idx: 40 |  Loss: (0.3834) | Acc: (87.79%) (4607/5248)\n",
            "Epoch: 48 | Batch_idx: 50 |  Loss: (0.3839) | Acc: (87.58%) (5717/6528)\n",
            "Epoch: 48 | Batch_idx: 60 |  Loss: (0.3832) | Acc: (87.49%) (6831/7808)\n",
            "Epoch: 48 | Batch_idx: 70 |  Loss: (0.3864) | Acc: (87.48%) (7950/9088)\n",
            "Epoch: 48 | Batch_idx: 80 |  Loss: (0.3907) | Acc: (87.36%) (9057/10368)\n",
            "Epoch: 48 | Batch_idx: 90 |  Loss: (0.3897) | Acc: (87.47%) (10189/11648)\n",
            "Epoch: 48 | Batch_idx: 100 |  Loss: (0.3940) | Acc: (87.30%) (11286/12928)\n",
            "Epoch: 48 | Batch_idx: 110 |  Loss: (0.3949) | Acc: (87.23%) (12394/14208)\n",
            "Epoch: 48 | Batch_idx: 120 |  Loss: (0.3954) | Acc: (87.32%) (13524/15488)\n",
            "Epoch: 48 | Batch_idx: 130 |  Loss: (0.3967) | Acc: (87.28%) (14635/16768)\n",
            "Epoch: 48 | Batch_idx: 140 |  Loss: (0.4012) | Acc: (87.07%) (15715/18048)\n",
            "Epoch: 48 | Batch_idx: 150 |  Loss: (0.4030) | Acc: (87.04%) (16824/19328)\n",
            "Epoch: 48 | Batch_idx: 160 |  Loss: (0.4036) | Acc: (87.00%) (17928/20608)\n",
            "Epoch: 48 | Batch_idx: 170 |  Loss: (0.4035) | Acc: (86.92%) (19026/21888)\n",
            "Epoch: 48 | Batch_idx: 180 |  Loss: (0.4013) | Acc: (86.96%) (20148/23168)\n",
            "Epoch: 48 | Batch_idx: 190 |  Loss: (0.3999) | Acc: (86.98%) (21264/24448)\n",
            "Epoch: 48 | Batch_idx: 200 |  Loss: (0.3990) | Acc: (87.06%) (22399/25728)\n",
            "Epoch: 48 | Batch_idx: 210 |  Loss: (0.4004) | Acc: (87.06%) (23512/27008)\n",
            "Epoch: 48 | Batch_idx: 220 |  Loss: (0.4000) | Acc: (87.05%) (24624/28288)\n",
            "Epoch: 48 | Batch_idx: 230 |  Loss: (0.4001) | Acc: (87.02%) (25731/29568)\n",
            "Epoch: 48 | Batch_idx: 240 |  Loss: (0.4009) | Acc: (87.01%) (26840/30848)\n",
            "Epoch: 48 | Batch_idx: 250 |  Loss: (0.4004) | Acc: (87.05%) (27966/32128)\n",
            "Epoch: 48 | Batch_idx: 260 |  Loss: (0.4005) | Acc: (87.02%) (29070/33408)\n",
            "Epoch: 48 | Batch_idx: 270 |  Loss: (0.4021) | Acc: (87.00%) (30179/34688)\n",
            "Epoch: 48 | Batch_idx: 280 |  Loss: (0.4021) | Acc: (87.00%) (31291/35968)\n",
            "Epoch: 48 | Batch_idx: 290 |  Loss: (0.4019) | Acc: (87.01%) (32410/37248)\n",
            "Epoch: 48 | Batch_idx: 300 |  Loss: (0.4035) | Acc: (86.98%) (33510/38528)\n",
            "Epoch: 48 | Batch_idx: 310 |  Loss: (0.4031) | Acc: (86.95%) (34615/39808)\n",
            "Epoch: 48 | Batch_idx: 320 |  Loss: (0.4023) | Acc: (86.96%) (35729/41088)\n",
            "Epoch: 48 | Batch_idx: 330 |  Loss: (0.4025) | Acc: (86.94%) (36835/42368)\n",
            "Epoch: 48 | Batch_idx: 340 |  Loss: (0.4016) | Acc: (87.01%) (37978/43648)\n",
            "Epoch: 48 | Batch_idx: 350 |  Loss: (0.4008) | Acc: (87.04%) (39106/44928)\n",
            "Epoch: 48 | Batch_idx: 360 |  Loss: (0.4007) | Acc: (87.05%) (40223/46208)\n",
            "Epoch: 48 | Batch_idx: 370 |  Loss: (0.4012) | Acc: (87.01%) (41321/47488)\n",
            "Epoch: 48 | Batch_idx: 380 |  Loss: (0.4016) | Acc: (87.02%) (42438/48768)\n",
            "Epoch: 48 | Batch_idx: 390 |  Loss: (0.4013) | Acc: (87.02%) (43509/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.5079) | Acc: (84.09%) (8409/10000)\n",
            "Epoch: 49 | Batch_idx: 0 |  Loss: (0.3425) | Acc: (89.06%) (114/128)\n",
            "Epoch: 49 | Batch_idx: 10 |  Loss: (0.3900) | Acc: (87.29%) (1229/1408)\n",
            "Epoch: 49 | Batch_idx: 20 |  Loss: (0.3898) | Acc: (87.13%) (2342/2688)\n",
            "Epoch: 49 | Batch_idx: 30 |  Loss: (0.3857) | Acc: (87.10%) (3456/3968)\n",
            "Epoch: 49 | Batch_idx: 40 |  Loss: (0.3842) | Acc: (87.25%) (4579/5248)\n",
            "Epoch: 49 | Batch_idx: 50 |  Loss: (0.3916) | Acc: (86.95%) (5676/6528)\n",
            "Epoch: 49 | Batch_idx: 60 |  Loss: (0.3872) | Acc: (87.06%) (6798/7808)\n",
            "Epoch: 49 | Batch_idx: 70 |  Loss: (0.3917) | Acc: (86.96%) (7903/9088)\n",
            "Epoch: 49 | Batch_idx: 80 |  Loss: (0.3967) | Acc: (86.82%) (9001/10368)\n",
            "Epoch: 49 | Batch_idx: 90 |  Loss: (0.3991) | Acc: (86.75%) (10105/11648)\n",
            "Epoch: 49 | Batch_idx: 100 |  Loss: (0.3977) | Acc: (86.94%) (11239/12928)\n",
            "Epoch: 49 | Batch_idx: 110 |  Loss: (0.3964) | Acc: (87.01%) (12363/14208)\n",
            "Epoch: 49 | Batch_idx: 120 |  Loss: (0.3922) | Acc: (87.09%) (13489/15488)\n",
            "Epoch: 49 | Batch_idx: 130 |  Loss: (0.3908) | Acc: (87.15%) (14614/16768)\n",
            "Epoch: 49 | Batch_idx: 140 |  Loss: (0.3923) | Acc: (87.22%) (15741/18048)\n",
            "Epoch: 49 | Batch_idx: 150 |  Loss: (0.3932) | Acc: (87.21%) (16856/19328)\n",
            "Epoch: 49 | Batch_idx: 160 |  Loss: (0.3935) | Acc: (87.25%) (17980/20608)\n",
            "Epoch: 49 | Batch_idx: 170 |  Loss: (0.3943) | Acc: (87.24%) (19095/21888)\n",
            "Epoch: 49 | Batch_idx: 180 |  Loss: (0.3941) | Acc: (87.25%) (20215/23168)\n",
            "Epoch: 49 | Batch_idx: 190 |  Loss: (0.3964) | Acc: (87.19%) (21316/24448)\n",
            "Epoch: 49 | Batch_idx: 200 |  Loss: (0.3982) | Acc: (87.14%) (22419/25728)\n",
            "Epoch: 49 | Batch_idx: 210 |  Loss: (0.3962) | Acc: (87.22%) (23557/27008)\n",
            "Epoch: 49 | Batch_idx: 220 |  Loss: (0.3959) | Acc: (87.23%) (24677/28288)\n",
            "Epoch: 49 | Batch_idx: 230 |  Loss: (0.3963) | Acc: (87.24%) (25795/29568)\n",
            "Epoch: 49 | Batch_idx: 240 |  Loss: (0.3973) | Acc: (87.21%) (26902/30848)\n",
            "Epoch: 49 | Batch_idx: 250 |  Loss: (0.3966) | Acc: (87.25%) (28032/32128)\n",
            "Epoch: 49 | Batch_idx: 260 |  Loss: (0.3946) | Acc: (87.33%) (29175/33408)\n",
            "Epoch: 49 | Batch_idx: 270 |  Loss: (0.3967) | Acc: (87.24%) (30262/34688)\n",
            "Epoch: 49 | Batch_idx: 280 |  Loss: (0.3956) | Acc: (87.25%) (31383/35968)\n",
            "Epoch: 49 | Batch_idx: 290 |  Loss: (0.3941) | Acc: (87.32%) (32526/37248)\n",
            "Epoch: 49 | Batch_idx: 300 |  Loss: (0.3950) | Acc: (87.29%) (33633/38528)\n",
            "Epoch: 49 | Batch_idx: 310 |  Loss: (0.3963) | Acc: (87.27%) (34740/39808)\n",
            "Epoch: 49 | Batch_idx: 320 |  Loss: (0.3965) | Acc: (87.23%) (35841/41088)\n",
            "Epoch: 49 | Batch_idx: 330 |  Loss: (0.3964) | Acc: (87.22%) (36955/42368)\n",
            "Epoch: 49 | Batch_idx: 340 |  Loss: (0.3963) | Acc: (87.21%) (38067/43648)\n",
            "Epoch: 49 | Batch_idx: 350 |  Loss: (0.3965) | Acc: (87.22%) (39188/44928)\n",
            "Epoch: 49 | Batch_idx: 360 |  Loss: (0.3973) | Acc: (87.19%) (40290/46208)\n",
            "Epoch: 49 | Batch_idx: 370 |  Loss: (0.3975) | Acc: (87.17%) (41394/47488)\n",
            "Epoch: 49 | Batch_idx: 380 |  Loss: (0.3969) | Acc: (87.17%) (42510/48768)\n",
            "Epoch: 49 | Batch_idx: 390 |  Loss: (0.3980) | Acc: (87.12%) (43561/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4416) | Acc: (85.65%) (8565/10000)\n",
            "Epoch: 50 | Batch_idx: 0 |  Loss: (0.4681) | Acc: (86.72%) (111/128)\n",
            "Epoch: 50 | Batch_idx: 10 |  Loss: (0.3727) | Acc: (86.93%) (1224/1408)\n",
            "Epoch: 50 | Batch_idx: 20 |  Loss: (0.3667) | Acc: (87.54%) (2353/2688)\n",
            "Epoch: 50 | Batch_idx: 30 |  Loss: (0.3708) | Acc: (87.73%) (3481/3968)\n",
            "Epoch: 50 | Batch_idx: 40 |  Loss: (0.3689) | Acc: (87.75%) (4605/5248)\n",
            "Epoch: 50 | Batch_idx: 50 |  Loss: (0.3591) | Acc: (88.13%) (5753/6528)\n",
            "Epoch: 50 | Batch_idx: 60 |  Loss: (0.3653) | Acc: (87.94%) (6866/7808)\n",
            "Epoch: 50 | Batch_idx: 70 |  Loss: (0.3671) | Acc: (87.92%) (7990/9088)\n",
            "Epoch: 50 | Batch_idx: 80 |  Loss: (0.3686) | Acc: (87.86%) (9109/10368)\n",
            "Epoch: 50 | Batch_idx: 90 |  Loss: (0.3724) | Acc: (87.60%) (10204/11648)\n",
            "Epoch: 50 | Batch_idx: 100 |  Loss: (0.3759) | Acc: (87.50%) (11312/12928)\n",
            "Epoch: 50 | Batch_idx: 110 |  Loss: (0.3763) | Acc: (87.53%) (12436/14208)\n",
            "Epoch: 50 | Batch_idx: 120 |  Loss: (0.3804) | Acc: (87.45%) (13545/15488)\n",
            "Epoch: 50 | Batch_idx: 130 |  Loss: (0.3827) | Acc: (87.36%) (14648/16768)\n",
            "Epoch: 50 | Batch_idx: 140 |  Loss: (0.3813) | Acc: (87.39%) (15772/18048)\n",
            "Epoch: 50 | Batch_idx: 150 |  Loss: (0.3822) | Acc: (87.32%) (16877/19328)\n",
            "Epoch: 50 | Batch_idx: 160 |  Loss: (0.3818) | Acc: (87.31%) (17993/20608)\n",
            "Epoch: 50 | Batch_idx: 170 |  Loss: (0.3847) | Acc: (87.24%) (19094/21888)\n",
            "Epoch: 50 | Batch_idx: 180 |  Loss: (0.3856) | Acc: (87.24%) (20212/23168)\n",
            "Epoch: 50 | Batch_idx: 190 |  Loss: (0.3861) | Acc: (87.25%) (21331/24448)\n",
            "Epoch: 50 | Batch_idx: 200 |  Loss: (0.3871) | Acc: (87.26%) (22449/25728)\n",
            "Epoch: 50 | Batch_idx: 210 |  Loss: (0.3857) | Acc: (87.26%) (23567/27008)\n",
            "Epoch: 50 | Batch_idx: 220 |  Loss: (0.3865) | Acc: (87.26%) (24685/28288)\n",
            "Epoch: 50 | Batch_idx: 230 |  Loss: (0.3893) | Acc: (87.19%) (25781/29568)\n",
            "Epoch: 50 | Batch_idx: 240 |  Loss: (0.3899) | Acc: (87.15%) (26885/30848)\n",
            "Epoch: 50 | Batch_idx: 250 |  Loss: (0.3897) | Acc: (87.16%) (28002/32128)\n",
            "Epoch: 50 | Batch_idx: 260 |  Loss: (0.3902) | Acc: (87.13%) (29107/33408)\n",
            "Epoch: 50 | Batch_idx: 270 |  Loss: (0.3920) | Acc: (87.10%) (30214/34688)\n",
            "Epoch: 50 | Batch_idx: 280 |  Loss: (0.3916) | Acc: (87.13%) (31340/35968)\n",
            "Epoch: 50 | Batch_idx: 290 |  Loss: (0.3931) | Acc: (87.12%) (32450/37248)\n",
            "Epoch: 50 | Batch_idx: 300 |  Loss: (0.3944) | Acc: (87.06%) (33541/38528)\n",
            "Epoch: 50 | Batch_idx: 310 |  Loss: (0.3941) | Acc: (87.08%) (34663/39808)\n",
            "Epoch: 50 | Batch_idx: 320 |  Loss: (0.3939) | Acc: (87.07%) (35775/41088)\n",
            "Epoch: 50 | Batch_idx: 330 |  Loss: (0.3942) | Acc: (87.04%) (36877/42368)\n",
            "Epoch: 50 | Batch_idx: 340 |  Loss: (0.3938) | Acc: (87.07%) (38006/43648)\n",
            "Epoch: 50 | Batch_idx: 350 |  Loss: (0.3943) | Acc: (87.08%) (39125/44928)\n",
            "Epoch: 50 | Batch_idx: 360 |  Loss: (0.3946) | Acc: (87.06%) (40228/46208)\n",
            "Epoch: 50 | Batch_idx: 370 |  Loss: (0.3953) | Acc: (87.06%) (41345/47488)\n",
            "Epoch: 50 | Batch_idx: 380 |  Loss: (0.3952) | Acc: (87.05%) (42453/48768)\n",
            "Epoch: 50 | Batch_idx: 390 |  Loss: (0.3952) | Acc: (87.06%) (43530/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4171) | Acc: (86.14%) (8614/10000)\n",
            "Epoch: 51 | Batch_idx: 0 |  Loss: (0.3977) | Acc: (86.72%) (111/128)\n",
            "Epoch: 51 | Batch_idx: 10 |  Loss: (0.3437) | Acc: (88.28%) (1243/1408)\n",
            "Epoch: 51 | Batch_idx: 20 |  Loss: (0.3781) | Acc: (86.87%) (2335/2688)\n",
            "Epoch: 51 | Batch_idx: 30 |  Loss: (0.3693) | Acc: (87.27%) (3463/3968)\n",
            "Epoch: 51 | Batch_idx: 40 |  Loss: (0.3672) | Acc: (87.77%) (4606/5248)\n",
            "Epoch: 51 | Batch_idx: 50 |  Loss: (0.3736) | Acc: (87.44%) (5708/6528)\n",
            "Epoch: 51 | Batch_idx: 60 |  Loss: (0.3781) | Acc: (87.22%) (6810/7808)\n",
            "Epoch: 51 | Batch_idx: 70 |  Loss: (0.3785) | Acc: (87.37%) (7940/9088)\n",
            "Epoch: 51 | Batch_idx: 80 |  Loss: (0.3760) | Acc: (87.36%) (9058/10368)\n",
            "Epoch: 51 | Batch_idx: 90 |  Loss: (0.3760) | Acc: (87.47%) (10188/11648)\n",
            "Epoch: 51 | Batch_idx: 100 |  Loss: (0.3757) | Acc: (87.46%) (11307/12928)\n",
            "Epoch: 51 | Batch_idx: 110 |  Loss: (0.3756) | Acc: (87.47%) (12428/14208)\n",
            "Epoch: 51 | Batch_idx: 120 |  Loss: (0.3742) | Acc: (87.58%) (13564/15488)\n",
            "Epoch: 51 | Batch_idx: 130 |  Loss: (0.3768) | Acc: (87.55%) (14681/16768)\n",
            "Epoch: 51 | Batch_idx: 140 |  Loss: (0.3784) | Acc: (87.58%) (15806/18048)\n",
            "Epoch: 51 | Batch_idx: 150 |  Loss: (0.3809) | Acc: (87.52%) (16916/19328)\n",
            "Epoch: 51 | Batch_idx: 160 |  Loss: (0.3775) | Acc: (87.63%) (18059/20608)\n",
            "Epoch: 51 | Batch_idx: 170 |  Loss: (0.3779) | Acc: (87.65%) (19185/21888)\n",
            "Epoch: 51 | Batch_idx: 180 |  Loss: (0.3812) | Acc: (87.60%) (20296/23168)\n",
            "Epoch: 51 | Batch_idx: 190 |  Loss: (0.3828) | Acc: (87.56%) (21406/24448)\n",
            "Epoch: 51 | Batch_idx: 200 |  Loss: (0.3846) | Acc: (87.52%) (22518/25728)\n",
            "Epoch: 51 | Batch_idx: 210 |  Loss: (0.3851) | Acc: (87.52%) (23637/27008)\n",
            "Epoch: 51 | Batch_idx: 220 |  Loss: (0.3861) | Acc: (87.53%) (24761/28288)\n",
            "Epoch: 51 | Batch_idx: 230 |  Loss: (0.3870) | Acc: (87.52%) (25878/29568)\n",
            "Epoch: 51 | Batch_idx: 240 |  Loss: (0.3865) | Acc: (87.50%) (26993/30848)\n",
            "Epoch: 51 | Batch_idx: 250 |  Loss: (0.3862) | Acc: (87.54%) (28125/32128)\n",
            "Epoch: 51 | Batch_idx: 260 |  Loss: (0.3860) | Acc: (87.53%) (29242/33408)\n",
            "Epoch: 51 | Batch_idx: 270 |  Loss: (0.3856) | Acc: (87.53%) (30361/34688)\n",
            "Epoch: 51 | Batch_idx: 280 |  Loss: (0.3866) | Acc: (87.47%) (31460/35968)\n",
            "Epoch: 51 | Batch_idx: 290 |  Loss: (0.3871) | Acc: (87.49%) (32588/37248)\n",
            "Epoch: 51 | Batch_idx: 300 |  Loss: (0.3875) | Acc: (87.49%) (33708/38528)\n",
            "Epoch: 51 | Batch_idx: 310 |  Loss: (0.3867) | Acc: (87.55%) (34851/39808)\n",
            "Epoch: 51 | Batch_idx: 320 |  Loss: (0.3876) | Acc: (87.53%) (35966/41088)\n",
            "Epoch: 51 | Batch_idx: 330 |  Loss: (0.3866) | Acc: (87.56%) (37098/42368)\n",
            "Epoch: 51 | Batch_idx: 340 |  Loss: (0.3865) | Acc: (87.56%) (38217/43648)\n",
            "Epoch: 51 | Batch_idx: 350 |  Loss: (0.3877) | Acc: (87.54%) (39328/44928)\n",
            "Epoch: 51 | Batch_idx: 360 |  Loss: (0.3873) | Acc: (87.57%) (40463/46208)\n",
            "Epoch: 51 | Batch_idx: 370 |  Loss: (0.3870) | Acc: (87.57%) (41584/47488)\n",
            "Epoch: 51 | Batch_idx: 380 |  Loss: (0.3875) | Acc: (87.56%) (42701/48768)\n",
            "Epoch: 51 | Batch_idx: 390 |  Loss: (0.3857) | Acc: (87.61%) (43807/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4454) | Acc: (86.02%) (8602/10000)\n",
            "Epoch: 52 | Batch_idx: 0 |  Loss: (0.2352) | Acc: (92.97%) (119/128)\n",
            "Epoch: 52 | Batch_idx: 10 |  Loss: (0.4297) | Acc: (87.43%) (1231/1408)\n",
            "Epoch: 52 | Batch_idx: 20 |  Loss: (0.4269) | Acc: (86.68%) (2330/2688)\n",
            "Epoch: 52 | Batch_idx: 30 |  Loss: (0.4196) | Acc: (86.82%) (3445/3968)\n",
            "Epoch: 52 | Batch_idx: 40 |  Loss: (0.4148) | Acc: (86.74%) (4552/5248)\n",
            "Epoch: 52 | Batch_idx: 50 |  Loss: (0.4032) | Acc: (87.04%) (5682/6528)\n",
            "Epoch: 52 | Batch_idx: 60 |  Loss: (0.3983) | Acc: (87.26%) (6813/7808)\n",
            "Epoch: 52 | Batch_idx: 70 |  Loss: (0.3944) | Acc: (87.35%) (7938/9088)\n",
            "Epoch: 52 | Batch_idx: 80 |  Loss: (0.3926) | Acc: (87.29%) (9050/10368)\n",
            "Epoch: 52 | Batch_idx: 90 |  Loss: (0.3870) | Acc: (87.45%) (10186/11648)\n",
            "Epoch: 52 | Batch_idx: 100 |  Loss: (0.3903) | Acc: (87.32%) (11289/12928)\n",
            "Epoch: 52 | Batch_idx: 110 |  Loss: (0.3927) | Acc: (87.32%) (12406/14208)\n",
            "Epoch: 52 | Batch_idx: 120 |  Loss: (0.3970) | Acc: (87.16%) (13499/15488)\n",
            "Epoch: 52 | Batch_idx: 130 |  Loss: (0.3990) | Acc: (87.00%) (14588/16768)\n",
            "Epoch: 52 | Batch_idx: 140 |  Loss: (0.3950) | Acc: (87.16%) (15730/18048)\n",
            "Epoch: 52 | Batch_idx: 150 |  Loss: (0.3939) | Acc: (87.09%) (16833/19328)\n",
            "Epoch: 52 | Batch_idx: 160 |  Loss: (0.3919) | Acc: (87.16%) (17962/20608)\n",
            "Epoch: 52 | Batch_idx: 170 |  Loss: (0.3916) | Acc: (87.16%) (19078/21888)\n",
            "Epoch: 52 | Batch_idx: 180 |  Loss: (0.3901) | Acc: (87.18%) (20199/23168)\n",
            "Epoch: 52 | Batch_idx: 190 |  Loss: (0.3903) | Acc: (87.23%) (21325/24448)\n",
            "Epoch: 52 | Batch_idx: 200 |  Loss: (0.3892) | Acc: (87.31%) (22463/25728)\n",
            "Epoch: 52 | Batch_idx: 210 |  Loss: (0.3893) | Acc: (87.30%) (23577/27008)\n",
            "Epoch: 52 | Batch_idx: 220 |  Loss: (0.3883) | Acc: (87.38%) (24717/28288)\n",
            "Epoch: 52 | Batch_idx: 230 |  Loss: (0.3881) | Acc: (87.38%) (25837/29568)\n",
            "Epoch: 52 | Batch_idx: 240 |  Loss: (0.3896) | Acc: (87.27%) (26922/30848)\n",
            "Epoch: 52 | Batch_idx: 250 |  Loss: (0.3908) | Acc: (87.29%) (28045/32128)\n",
            "Epoch: 52 | Batch_idx: 260 |  Loss: (0.3907) | Acc: (87.30%) (29165/33408)\n",
            "Epoch: 52 | Batch_idx: 270 |  Loss: (0.3914) | Acc: (87.29%) (30279/34688)\n",
            "Epoch: 52 | Batch_idx: 280 |  Loss: (0.3905) | Acc: (87.31%) (31402/35968)\n",
            "Epoch: 52 | Batch_idx: 290 |  Loss: (0.3900) | Acc: (87.31%) (32520/37248)\n",
            "Epoch: 52 | Batch_idx: 300 |  Loss: (0.3890) | Acc: (87.35%) (33653/38528)\n",
            "Epoch: 52 | Batch_idx: 310 |  Loss: (0.3894) | Acc: (87.32%) (34762/39808)\n",
            "Epoch: 52 | Batch_idx: 320 |  Loss: (0.3883) | Acc: (87.33%) (35881/41088)\n",
            "Epoch: 52 | Batch_idx: 330 |  Loss: (0.3869) | Acc: (87.35%) (37008/42368)\n",
            "Epoch: 52 | Batch_idx: 340 |  Loss: (0.3874) | Acc: (87.34%) (38120/43648)\n",
            "Epoch: 52 | Batch_idx: 350 |  Loss: (0.3873) | Acc: (87.33%) (39237/44928)\n",
            "Epoch: 52 | Batch_idx: 360 |  Loss: (0.3870) | Acc: (87.39%) (40379/46208)\n",
            "Epoch: 52 | Batch_idx: 370 |  Loss: (0.3879) | Acc: (87.32%) (41466/47488)\n",
            "Epoch: 52 | Batch_idx: 380 |  Loss: (0.3879) | Acc: (87.34%) (42592/48768)\n",
            "Epoch: 52 | Batch_idx: 390 |  Loss: (0.3871) | Acc: (87.37%) (43683/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4388) | Acc: (86.20%) (8620/10000)\n",
            "Epoch: 53 | Batch_idx: 0 |  Loss: (0.3380) | Acc: (88.28%) (113/128)\n",
            "Epoch: 53 | Batch_idx: 10 |  Loss: (0.3399) | Acc: (88.99%) (1253/1408)\n",
            "Epoch: 53 | Batch_idx: 20 |  Loss: (0.3708) | Acc: (88.24%) (2372/2688)\n",
            "Epoch: 53 | Batch_idx: 30 |  Loss: (0.3751) | Acc: (87.90%) (3488/3968)\n",
            "Epoch: 53 | Batch_idx: 40 |  Loss: (0.3684) | Acc: (88.19%) (4628/5248)\n",
            "Epoch: 53 | Batch_idx: 50 |  Loss: (0.3838) | Acc: (87.88%) (5737/6528)\n",
            "Epoch: 53 | Batch_idx: 60 |  Loss: (0.3881) | Acc: (87.79%) (6855/7808)\n",
            "Epoch: 53 | Batch_idx: 70 |  Loss: (0.3961) | Acc: (87.51%) (7953/9088)\n",
            "Epoch: 53 | Batch_idx: 80 |  Loss: (0.3977) | Acc: (87.37%) (9059/10368)\n",
            "Epoch: 53 | Batch_idx: 90 |  Loss: (0.3952) | Acc: (87.38%) (10178/11648)\n",
            "Epoch: 53 | Batch_idx: 100 |  Loss: (0.3906) | Acc: (87.45%) (11305/12928)\n",
            "Epoch: 53 | Batch_idx: 110 |  Loss: (0.3871) | Acc: (87.56%) (12441/14208)\n",
            "Epoch: 53 | Batch_idx: 120 |  Loss: (0.3891) | Acc: (87.53%) (13556/15488)\n",
            "Epoch: 53 | Batch_idx: 130 |  Loss: (0.3894) | Acc: (87.48%) (14668/16768)\n",
            "Epoch: 53 | Batch_idx: 140 |  Loss: (0.3879) | Acc: (87.50%) (15792/18048)\n",
            "Epoch: 53 | Batch_idx: 150 |  Loss: (0.3886) | Acc: (87.48%) (16909/19328)\n",
            "Epoch: 53 | Batch_idx: 160 |  Loss: (0.3867) | Acc: (87.48%) (18028/20608)\n",
            "Epoch: 53 | Batch_idx: 170 |  Loss: (0.3860) | Acc: (87.51%) (19155/21888)\n",
            "Epoch: 53 | Batch_idx: 180 |  Loss: (0.3868) | Acc: (87.48%) (20268/23168)\n",
            "Epoch: 53 | Batch_idx: 190 |  Loss: (0.3885) | Acc: (87.46%) (21383/24448)\n",
            "Epoch: 53 | Batch_idx: 200 |  Loss: (0.3859) | Acc: (87.51%) (22515/25728)\n",
            "Epoch: 53 | Batch_idx: 210 |  Loss: (0.3856) | Acc: (87.52%) (23638/27008)\n",
            "Epoch: 53 | Batch_idx: 220 |  Loss: (0.3844) | Acc: (87.51%) (24754/28288)\n",
            "Epoch: 53 | Batch_idx: 230 |  Loss: (0.3860) | Acc: (87.47%) (25864/29568)\n",
            "Epoch: 53 | Batch_idx: 240 |  Loss: (0.3864) | Acc: (87.47%) (26984/30848)\n",
            "Epoch: 53 | Batch_idx: 250 |  Loss: (0.3877) | Acc: (87.42%) (28087/32128)\n",
            "Epoch: 53 | Batch_idx: 260 |  Loss: (0.3872) | Acc: (87.44%) (29211/33408)\n",
            "Epoch: 53 | Batch_idx: 270 |  Loss: (0.3869) | Acc: (87.47%) (30340/34688)\n",
            "Epoch: 53 | Batch_idx: 280 |  Loss: (0.3876) | Acc: (87.43%) (31448/35968)\n",
            "Epoch: 53 | Batch_idx: 290 |  Loss: (0.3874) | Acc: (87.45%) (32574/37248)\n",
            "Epoch: 53 | Batch_idx: 300 |  Loss: (0.3873) | Acc: (87.46%) (33695/38528)\n",
            "Epoch: 53 | Batch_idx: 310 |  Loss: (0.3867) | Acc: (87.46%) (34816/39808)\n",
            "Epoch: 53 | Batch_idx: 320 |  Loss: (0.3870) | Acc: (87.45%) (35932/41088)\n",
            "Epoch: 53 | Batch_idx: 330 |  Loss: (0.3865) | Acc: (87.46%) (37054/42368)\n",
            "Epoch: 53 | Batch_idx: 340 |  Loss: (0.3866) | Acc: (87.44%) (38167/43648)\n",
            "Epoch: 53 | Batch_idx: 350 |  Loss: (0.3876) | Acc: (87.42%) (39278/44928)\n",
            "Epoch: 53 | Batch_idx: 360 |  Loss: (0.3882) | Acc: (87.41%) (40390/46208)\n",
            "Epoch: 53 | Batch_idx: 370 |  Loss: (0.3890) | Acc: (87.37%) (41491/47488)\n",
            "Epoch: 53 | Batch_idx: 380 |  Loss: (0.3892) | Acc: (87.37%) (42608/48768)\n",
            "Epoch: 53 | Batch_idx: 390 |  Loss: (0.3904) | Acc: (87.35%) (43677/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4478) | Acc: (85.76%) (8576/10000)\n",
            "Epoch: 54 | Batch_idx: 0 |  Loss: (0.3318) | Acc: (90.62%) (116/128)\n",
            "Epoch: 54 | Batch_idx: 10 |  Loss: (0.3759) | Acc: (87.43%) (1231/1408)\n",
            "Epoch: 54 | Batch_idx: 20 |  Loss: (0.3856) | Acc: (87.50%) (2352/2688)\n",
            "Epoch: 54 | Batch_idx: 30 |  Loss: (0.3750) | Acc: (88.03%) (3493/3968)\n",
            "Epoch: 54 | Batch_idx: 40 |  Loss: (0.3716) | Acc: (88.01%) (4619/5248)\n",
            "Epoch: 54 | Batch_idx: 50 |  Loss: (0.3705) | Acc: (88.01%) (5745/6528)\n",
            "Epoch: 54 | Batch_idx: 60 |  Loss: (0.3652) | Acc: (88.05%) (6875/7808)\n",
            "Epoch: 54 | Batch_idx: 70 |  Loss: (0.3610) | Acc: (88.08%) (8005/9088)\n",
            "Epoch: 54 | Batch_idx: 80 |  Loss: (0.3604) | Acc: (88.12%) (9136/10368)\n",
            "Epoch: 54 | Batch_idx: 90 |  Loss: (0.3581) | Acc: (88.14%) (10267/11648)\n",
            "Epoch: 54 | Batch_idx: 100 |  Loss: (0.3622) | Acc: (88.04%) (11382/12928)\n",
            "Epoch: 54 | Batch_idx: 110 |  Loss: (0.3664) | Acc: (87.89%) (12487/14208)\n",
            "Epoch: 54 | Batch_idx: 120 |  Loss: (0.3680) | Acc: (87.87%) (13609/15488)\n",
            "Epoch: 54 | Batch_idx: 130 |  Loss: (0.3690) | Acc: (87.74%) (14712/16768)\n",
            "Epoch: 54 | Batch_idx: 140 |  Loss: (0.3693) | Acc: (87.74%) (15836/18048)\n",
            "Epoch: 54 | Batch_idx: 150 |  Loss: (0.3689) | Acc: (87.83%) (16976/19328)\n",
            "Epoch: 54 | Batch_idx: 160 |  Loss: (0.3733) | Acc: (87.78%) (18090/20608)\n",
            "Epoch: 54 | Batch_idx: 170 |  Loss: (0.3765) | Acc: (87.74%) (19205/21888)\n",
            "Epoch: 54 | Batch_idx: 180 |  Loss: (0.3778) | Acc: (87.72%) (20324/23168)\n",
            "Epoch: 54 | Batch_idx: 190 |  Loss: (0.3777) | Acc: (87.76%) (21456/24448)\n",
            "Epoch: 54 | Batch_idx: 200 |  Loss: (0.3772) | Acc: (87.78%) (22583/25728)\n",
            "Epoch: 54 | Batch_idx: 210 |  Loss: (0.3774) | Acc: (87.70%) (23687/27008)\n",
            "Epoch: 54 | Batch_idx: 220 |  Loss: (0.3774) | Acc: (87.71%) (24812/28288)\n",
            "Epoch: 54 | Batch_idx: 230 |  Loss: (0.3779) | Acc: (87.69%) (25928/29568)\n",
            "Epoch: 54 | Batch_idx: 240 |  Loss: (0.3814) | Acc: (87.53%) (27001/30848)\n",
            "Epoch: 54 | Batch_idx: 250 |  Loss: (0.3810) | Acc: (87.57%) (28134/32128)\n",
            "Epoch: 54 | Batch_idx: 260 |  Loss: (0.3809) | Acc: (87.56%) (29253/33408)\n",
            "Epoch: 54 | Batch_idx: 270 |  Loss: (0.3818) | Acc: (87.53%) (30364/34688)\n",
            "Epoch: 54 | Batch_idx: 280 |  Loss: (0.3809) | Acc: (87.54%) (31486/35968)\n",
            "Epoch: 54 | Batch_idx: 290 |  Loss: (0.3815) | Acc: (87.53%) (32603/37248)\n",
            "Epoch: 54 | Batch_idx: 300 |  Loss: (0.3818) | Acc: (87.53%) (33723/38528)\n",
            "Epoch: 54 | Batch_idx: 310 |  Loss: (0.3842) | Acc: (87.42%) (34800/39808)\n",
            "Epoch: 54 | Batch_idx: 320 |  Loss: (0.3845) | Acc: (87.40%) (35910/41088)\n",
            "Epoch: 54 | Batch_idx: 330 |  Loss: (0.3868) | Acc: (87.33%) (36999/42368)\n",
            "Epoch: 54 | Batch_idx: 340 |  Loss: (0.3870) | Acc: (87.28%) (38094/43648)\n",
            "Epoch: 54 | Batch_idx: 350 |  Loss: (0.3870) | Acc: (87.29%) (39217/44928)\n",
            "Epoch: 54 | Batch_idx: 360 |  Loss: (0.3868) | Acc: (87.29%) (40336/46208)\n",
            "Epoch: 54 | Batch_idx: 370 |  Loss: (0.3867) | Acc: (87.28%) (41449/47488)\n",
            "Epoch: 54 | Batch_idx: 380 |  Loss: (0.3871) | Acc: (87.28%) (42567/48768)\n",
            "Epoch: 54 | Batch_idx: 390 |  Loss: (0.3879) | Acc: (87.28%) (43641/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4341) | Acc: (85.61%) (8561/10000)\n",
            "Epoch: 55 | Batch_idx: 0 |  Loss: (0.4334) | Acc: (84.38%) (108/128)\n",
            "Epoch: 55 | Batch_idx: 10 |  Loss: (0.3432) | Acc: (88.35%) (1244/1408)\n",
            "Epoch: 55 | Batch_idx: 20 |  Loss: (0.3502) | Acc: (88.10%) (2368/2688)\n",
            "Epoch: 55 | Batch_idx: 30 |  Loss: (0.3457) | Acc: (88.46%) (3510/3968)\n",
            "Epoch: 55 | Batch_idx: 40 |  Loss: (0.3791) | Acc: (87.60%) (4597/5248)\n",
            "Epoch: 55 | Batch_idx: 50 |  Loss: (0.3714) | Acc: (87.73%) (5727/6528)\n",
            "Epoch: 55 | Batch_idx: 60 |  Loss: (0.3712) | Acc: (87.82%) (6857/7808)\n",
            "Epoch: 55 | Batch_idx: 70 |  Loss: (0.3759) | Acc: (87.69%) (7969/9088)\n",
            "Epoch: 55 | Batch_idx: 80 |  Loss: (0.3740) | Acc: (87.73%) (9096/10368)\n",
            "Epoch: 55 | Batch_idx: 90 |  Loss: (0.3721) | Acc: (87.89%) (10237/11648)\n",
            "Epoch: 55 | Batch_idx: 100 |  Loss: (0.3736) | Acc: (87.80%) (11351/12928)\n",
            "Epoch: 55 | Batch_idx: 110 |  Loss: (0.3725) | Acc: (87.81%) (12476/14208)\n",
            "Epoch: 55 | Batch_idx: 120 |  Loss: (0.3739) | Acc: (87.70%) (13583/15488)\n",
            "Epoch: 55 | Batch_idx: 130 |  Loss: (0.3742) | Acc: (87.64%) (14695/16768)\n",
            "Epoch: 55 | Batch_idx: 140 |  Loss: (0.3744) | Acc: (87.68%) (15825/18048)\n",
            "Epoch: 55 | Batch_idx: 150 |  Loss: (0.3774) | Acc: (87.58%) (16927/19328)\n",
            "Epoch: 55 | Batch_idx: 160 |  Loss: (0.3782) | Acc: (87.57%) (18046/20608)\n",
            "Epoch: 55 | Batch_idx: 170 |  Loss: (0.3774) | Acc: (87.61%) (19175/21888)\n",
            "Epoch: 55 | Batch_idx: 180 |  Loss: (0.3786) | Acc: (87.62%) (20300/23168)\n",
            "Epoch: 55 | Batch_idx: 190 |  Loss: (0.3802) | Acc: (87.61%) (21419/24448)\n",
            "Epoch: 55 | Batch_idx: 200 |  Loss: (0.3799) | Acc: (87.61%) (22541/25728)\n",
            "Epoch: 55 | Batch_idx: 210 |  Loss: (0.3787) | Acc: (87.64%) (23670/27008)\n",
            "Epoch: 55 | Batch_idx: 220 |  Loss: (0.3786) | Acc: (87.64%) (24793/28288)\n",
            "Epoch: 55 | Batch_idx: 230 |  Loss: (0.3775) | Acc: (87.67%) (25923/29568)\n",
            "Epoch: 55 | Batch_idx: 240 |  Loss: (0.3780) | Acc: (87.64%) (27034/30848)\n",
            "Epoch: 55 | Batch_idx: 250 |  Loss: (0.3793) | Acc: (87.63%) (28154/32128)\n",
            "Epoch: 55 | Batch_idx: 260 |  Loss: (0.3792) | Acc: (87.61%) (29268/33408)\n",
            "Epoch: 55 | Batch_idx: 270 |  Loss: (0.3803) | Acc: (87.61%) (30390/34688)\n",
            "Epoch: 55 | Batch_idx: 280 |  Loss: (0.3816) | Acc: (87.58%) (31502/35968)\n",
            "Epoch: 55 | Batch_idx: 290 |  Loss: (0.3813) | Acc: (87.61%) (32632/37248)\n",
            "Epoch: 55 | Batch_idx: 300 |  Loss: (0.3801) | Acc: (87.67%) (33778/38528)\n",
            "Epoch: 55 | Batch_idx: 310 |  Loss: (0.3785) | Acc: (87.73%) (34922/39808)\n",
            "Epoch: 55 | Batch_idx: 320 |  Loss: (0.3797) | Acc: (87.70%) (36034/41088)\n",
            "Epoch: 55 | Batch_idx: 330 |  Loss: (0.3791) | Acc: (87.74%) (37173/42368)\n",
            "Epoch: 55 | Batch_idx: 340 |  Loss: (0.3791) | Acc: (87.73%) (38292/43648)\n",
            "Epoch: 55 | Batch_idx: 350 |  Loss: (0.3795) | Acc: (87.74%) (39419/44928)\n",
            "Epoch: 55 | Batch_idx: 360 |  Loss: (0.3804) | Acc: (87.72%) (40533/46208)\n",
            "Epoch: 55 | Batch_idx: 370 |  Loss: (0.3804) | Acc: (87.71%) (41650/47488)\n",
            "Epoch: 55 | Batch_idx: 380 |  Loss: (0.3797) | Acc: (87.73%) (42783/48768)\n",
            "Epoch: 55 | Batch_idx: 390 |  Loss: (0.3797) | Acc: (87.73%) (43866/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4012) | Acc: (87.05%) (8705/10000)\n",
            "Epoch: 56 | Batch_idx: 0 |  Loss: (0.3868) | Acc: (90.62%) (116/128)\n",
            "Epoch: 56 | Batch_idx: 10 |  Loss: (0.3786) | Acc: (87.64%) (1234/1408)\n",
            "Epoch: 56 | Batch_idx: 20 |  Loss: (0.3902) | Acc: (87.46%) (2351/2688)\n",
            "Epoch: 56 | Batch_idx: 30 |  Loss: (0.3683) | Acc: (88.00%) (3492/3968)\n",
            "Epoch: 56 | Batch_idx: 40 |  Loss: (0.3634) | Acc: (87.88%) (4612/5248)\n",
            "Epoch: 56 | Batch_idx: 50 |  Loss: (0.3684) | Acc: (87.78%) (5730/6528)\n",
            "Epoch: 56 | Batch_idx: 60 |  Loss: (0.3721) | Acc: (87.63%) (6842/7808)\n",
            "Epoch: 56 | Batch_idx: 70 |  Loss: (0.3698) | Acc: (87.73%) (7973/9088)\n",
            "Epoch: 56 | Batch_idx: 80 |  Loss: (0.3696) | Acc: (87.66%) (9089/10368)\n",
            "Epoch: 56 | Batch_idx: 90 |  Loss: (0.3682) | Acc: (87.70%) (10215/11648)\n",
            "Epoch: 56 | Batch_idx: 100 |  Loss: (0.3669) | Acc: (87.67%) (11334/12928)\n",
            "Epoch: 56 | Batch_idx: 110 |  Loss: (0.3690) | Acc: (87.61%) (12447/14208)\n",
            "Epoch: 56 | Batch_idx: 120 |  Loss: (0.3720) | Acc: (87.49%) (13551/15488)\n",
            "Epoch: 56 | Batch_idx: 130 |  Loss: (0.3733) | Acc: (87.49%) (14670/16768)\n",
            "Epoch: 56 | Batch_idx: 140 |  Loss: (0.3761) | Acc: (87.40%) (15774/18048)\n",
            "Epoch: 56 | Batch_idx: 150 |  Loss: (0.3753) | Acc: (87.45%) (16902/19328)\n",
            "Epoch: 56 | Batch_idx: 160 |  Loss: (0.3742) | Acc: (87.46%) (18023/20608)\n",
            "Epoch: 56 | Batch_idx: 170 |  Loss: (0.3728) | Acc: (87.49%) (19149/21888)\n",
            "Epoch: 56 | Batch_idx: 180 |  Loss: (0.3748) | Acc: (87.48%) (20267/23168)\n",
            "Epoch: 56 | Batch_idx: 190 |  Loss: (0.3783) | Acc: (87.39%) (21366/24448)\n",
            "Epoch: 56 | Batch_idx: 200 |  Loss: (0.3802) | Acc: (87.34%) (22470/25728)\n",
            "Epoch: 56 | Batch_idx: 210 |  Loss: (0.3797) | Acc: (87.32%) (23584/27008)\n",
            "Epoch: 56 | Batch_idx: 220 |  Loss: (0.3793) | Acc: (87.32%) (24702/28288)\n",
            "Epoch: 56 | Batch_idx: 230 |  Loss: (0.3774) | Acc: (87.37%) (25834/29568)\n",
            "Epoch: 56 | Batch_idx: 240 |  Loss: (0.3805) | Acc: (87.28%) (26925/30848)\n",
            "Epoch: 56 | Batch_idx: 250 |  Loss: (0.3805) | Acc: (87.29%) (28045/32128)\n",
            "Epoch: 56 | Batch_idx: 260 |  Loss: (0.3803) | Acc: (87.33%) (29176/33408)\n",
            "Epoch: 56 | Batch_idx: 270 |  Loss: (0.3798) | Acc: (87.38%) (30309/34688)\n",
            "Epoch: 56 | Batch_idx: 280 |  Loss: (0.3796) | Acc: (87.36%) (31423/35968)\n",
            "Epoch: 56 | Batch_idx: 290 |  Loss: (0.3805) | Acc: (87.34%) (32534/37248)\n",
            "Epoch: 56 | Batch_idx: 300 |  Loss: (0.3812) | Acc: (87.36%) (33658/38528)\n",
            "Epoch: 56 | Batch_idx: 310 |  Loss: (0.3836) | Acc: (87.29%) (34750/39808)\n",
            "Epoch: 56 | Batch_idx: 320 |  Loss: (0.3823) | Acc: (87.34%) (35887/41088)\n",
            "Epoch: 56 | Batch_idx: 330 |  Loss: (0.3843) | Acc: (87.29%) (36981/42368)\n",
            "Epoch: 56 | Batch_idx: 340 |  Loss: (0.3837) | Acc: (87.31%) (38110/43648)\n",
            "Epoch: 56 | Batch_idx: 350 |  Loss: (0.3861) | Acc: (87.29%) (39216/44928)\n",
            "Epoch: 56 | Batch_idx: 360 |  Loss: (0.3858) | Acc: (87.30%) (40340/46208)\n",
            "Epoch: 56 | Batch_idx: 370 |  Loss: (0.3852) | Acc: (87.31%) (41463/47488)\n",
            "Epoch: 56 | Batch_idx: 380 |  Loss: (0.3851) | Acc: (87.31%) (42577/48768)\n",
            "Epoch: 56 | Batch_idx: 390 |  Loss: (0.3858) | Acc: (87.29%) (43643/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4021) | Acc: (86.28%) (8628/10000)\n",
            "Epoch: 57 | Batch_idx: 0 |  Loss: (0.4333) | Acc: (89.06%) (114/128)\n",
            "Epoch: 57 | Batch_idx: 10 |  Loss: (0.3574) | Acc: (88.64%) (1248/1408)\n",
            "Epoch: 57 | Batch_idx: 20 |  Loss: (0.3573) | Acc: (88.28%) (2373/2688)\n",
            "Epoch: 57 | Batch_idx: 30 |  Loss: (0.3688) | Acc: (88.00%) (3492/3968)\n",
            "Epoch: 57 | Batch_idx: 40 |  Loss: (0.3687) | Acc: (88.11%) (4624/5248)\n",
            "Epoch: 57 | Batch_idx: 50 |  Loss: (0.3654) | Acc: (88.17%) (5756/6528)\n",
            "Epoch: 57 | Batch_idx: 60 |  Loss: (0.3646) | Acc: (88.26%) (6891/7808)\n",
            "Epoch: 57 | Batch_idx: 70 |  Loss: (0.3686) | Acc: (88.18%) (8014/9088)\n",
            "Epoch: 57 | Batch_idx: 80 |  Loss: (0.3704) | Acc: (88.11%) (9135/10368)\n",
            "Epoch: 57 | Batch_idx: 90 |  Loss: (0.3699) | Acc: (88.11%) (10263/11648)\n",
            "Epoch: 57 | Batch_idx: 100 |  Loss: (0.3691) | Acc: (88.27%) (11411/12928)\n",
            "Epoch: 57 | Batch_idx: 110 |  Loss: (0.3653) | Acc: (88.37%) (12556/14208)\n",
            "Epoch: 57 | Batch_idx: 120 |  Loss: (0.3687) | Acc: (88.24%) (13667/15488)\n",
            "Epoch: 57 | Batch_idx: 130 |  Loss: (0.3693) | Acc: (88.22%) (14792/16768)\n",
            "Epoch: 57 | Batch_idx: 140 |  Loss: (0.3727) | Acc: (88.12%) (15903/18048)\n",
            "Epoch: 57 | Batch_idx: 150 |  Loss: (0.3741) | Acc: (88.05%) (17019/19328)\n",
            "Epoch: 57 | Batch_idx: 160 |  Loss: (0.3760) | Acc: (88.04%) (18144/20608)\n",
            "Epoch: 57 | Batch_idx: 170 |  Loss: (0.3762) | Acc: (88.01%) (19263/21888)\n",
            "Epoch: 57 | Batch_idx: 180 |  Loss: (0.3786) | Acc: (87.96%) (20379/23168)\n",
            "Epoch: 57 | Batch_idx: 190 |  Loss: (0.3777) | Acc: (87.94%) (21500/24448)\n",
            "Epoch: 57 | Batch_idx: 200 |  Loss: (0.3790) | Acc: (87.91%) (22618/25728)\n",
            "Epoch: 57 | Batch_idx: 210 |  Loss: (0.3787) | Acc: (87.90%) (23741/27008)\n",
            "Epoch: 57 | Batch_idx: 220 |  Loss: (0.3780) | Acc: (87.93%) (24873/28288)\n",
            "Epoch: 57 | Batch_idx: 230 |  Loss: (0.3804) | Acc: (87.91%) (25992/29568)\n",
            "Epoch: 57 | Batch_idx: 240 |  Loss: (0.3798) | Acc: (87.89%) (27113/30848)\n",
            "Epoch: 57 | Batch_idx: 250 |  Loss: (0.3791) | Acc: (87.90%) (28239/32128)\n",
            "Epoch: 57 | Batch_idx: 260 |  Loss: (0.3792) | Acc: (87.87%) (29356/33408)\n",
            "Epoch: 57 | Batch_idx: 270 |  Loss: (0.3791) | Acc: (87.87%) (30481/34688)\n",
            "Epoch: 57 | Batch_idx: 280 |  Loss: (0.3792) | Acc: (87.86%) (31603/35968)\n",
            "Epoch: 57 | Batch_idx: 290 |  Loss: (0.3789) | Acc: (87.85%) (32724/37248)\n",
            "Epoch: 57 | Batch_idx: 300 |  Loss: (0.3791) | Acc: (87.81%) (33831/38528)\n",
            "Epoch: 57 | Batch_idx: 310 |  Loss: (0.3784) | Acc: (87.82%) (34959/39808)\n",
            "Epoch: 57 | Batch_idx: 320 |  Loss: (0.3785) | Acc: (87.83%) (36087/41088)\n",
            "Epoch: 57 | Batch_idx: 330 |  Loss: (0.3766) | Acc: (87.88%) (37231/42368)\n",
            "Epoch: 57 | Batch_idx: 340 |  Loss: (0.3764) | Acc: (87.86%) (38350/43648)\n",
            "Epoch: 57 | Batch_idx: 350 |  Loss: (0.3766) | Acc: (87.86%) (39473/44928)\n",
            "Epoch: 57 | Batch_idx: 360 |  Loss: (0.3762) | Acc: (87.86%) (40599/46208)\n",
            "Epoch: 57 | Batch_idx: 370 |  Loss: (0.3765) | Acc: (87.86%) (41724/47488)\n",
            "Epoch: 57 | Batch_idx: 380 |  Loss: (0.3764) | Acc: (87.86%) (42849/48768)\n",
            "Epoch: 57 | Batch_idx: 390 |  Loss: (0.3757) | Acc: (87.88%) (43939/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4478) | Acc: (85.89%) (8589/10000)\n",
            "Epoch: 58 | Batch_idx: 0 |  Loss: (0.3267) | Acc: (88.28%) (113/128)\n",
            "Epoch: 58 | Batch_idx: 10 |  Loss: (0.3734) | Acc: (87.78%) (1236/1408)\n",
            "Epoch: 58 | Batch_idx: 20 |  Loss: (0.3639) | Acc: (87.83%) (2361/2688)\n",
            "Epoch: 58 | Batch_idx: 30 |  Loss: (0.3442) | Acc: (88.56%) (3514/3968)\n",
            "Epoch: 58 | Batch_idx: 40 |  Loss: (0.3515) | Acc: (88.43%) (4641/5248)\n",
            "Epoch: 58 | Batch_idx: 50 |  Loss: (0.3516) | Acc: (88.56%) (5781/6528)\n",
            "Epoch: 58 | Batch_idx: 60 |  Loss: (0.3518) | Acc: (88.49%) (6909/7808)\n",
            "Epoch: 58 | Batch_idx: 70 |  Loss: (0.3551) | Acc: (88.36%) (8030/9088)\n",
            "Epoch: 58 | Batch_idx: 80 |  Loss: (0.3567) | Acc: (88.21%) (9146/10368)\n",
            "Epoch: 58 | Batch_idx: 90 |  Loss: (0.3602) | Acc: (88.09%) (10261/11648)\n",
            "Epoch: 58 | Batch_idx: 100 |  Loss: (0.3664) | Acc: (87.90%) (11364/12928)\n",
            "Epoch: 58 | Batch_idx: 110 |  Loss: (0.3688) | Acc: (87.89%) (12487/14208)\n",
            "Epoch: 58 | Batch_idx: 120 |  Loss: (0.3710) | Acc: (87.87%) (13609/15488)\n",
            "Epoch: 58 | Batch_idx: 130 |  Loss: (0.3698) | Acc: (87.89%) (14737/16768)\n",
            "Epoch: 58 | Batch_idx: 140 |  Loss: (0.3714) | Acc: (87.80%) (15847/18048)\n",
            "Epoch: 58 | Batch_idx: 150 |  Loss: (0.3706) | Acc: (87.85%) (16979/19328)\n",
            "Epoch: 58 | Batch_idx: 160 |  Loss: (0.3722) | Acc: (87.78%) (18090/20608)\n",
            "Epoch: 58 | Batch_idx: 170 |  Loss: (0.3707) | Acc: (87.82%) (19223/21888)\n",
            "Epoch: 58 | Batch_idx: 180 |  Loss: (0.3709) | Acc: (87.84%) (20351/23168)\n",
            "Epoch: 58 | Batch_idx: 190 |  Loss: (0.3701) | Acc: (87.88%) (21484/24448)\n",
            "Epoch: 58 | Batch_idx: 200 |  Loss: (0.3723) | Acc: (87.78%) (22585/25728)\n",
            "Epoch: 58 | Batch_idx: 210 |  Loss: (0.3727) | Acc: (87.79%) (23709/27008)\n",
            "Epoch: 58 | Batch_idx: 220 |  Loss: (0.3741) | Acc: (87.69%) (24805/28288)\n",
            "Epoch: 58 | Batch_idx: 230 |  Loss: (0.3731) | Acc: (87.72%) (25936/29568)\n",
            "Epoch: 58 | Batch_idx: 240 |  Loss: (0.3735) | Acc: (87.72%) (27059/30848)\n",
            "Epoch: 58 | Batch_idx: 250 |  Loss: (0.3748) | Acc: (87.68%) (28170/32128)\n",
            "Epoch: 58 | Batch_idx: 260 |  Loss: (0.3754) | Acc: (87.66%) (29285/33408)\n",
            "Epoch: 58 | Batch_idx: 270 |  Loss: (0.3766) | Acc: (87.62%) (30395/34688)\n",
            "Epoch: 58 | Batch_idx: 280 |  Loss: (0.3779) | Acc: (87.58%) (31500/35968)\n",
            "Epoch: 58 | Batch_idx: 290 |  Loss: (0.3781) | Acc: (87.60%) (32630/37248)\n",
            "Epoch: 58 | Batch_idx: 300 |  Loss: (0.3783) | Acc: (87.62%) (33757/38528)\n",
            "Epoch: 58 | Batch_idx: 310 |  Loss: (0.3782) | Acc: (87.61%) (34875/39808)\n",
            "Epoch: 58 | Batch_idx: 320 |  Loss: (0.3778) | Acc: (87.59%) (35990/41088)\n",
            "Epoch: 58 | Batch_idx: 330 |  Loss: (0.3787) | Acc: (87.57%) (37100/42368)\n",
            "Epoch: 58 | Batch_idx: 340 |  Loss: (0.3792) | Acc: (87.56%) (38217/43648)\n",
            "Epoch: 58 | Batch_idx: 350 |  Loss: (0.3788) | Acc: (87.55%) (39336/44928)\n",
            "Epoch: 58 | Batch_idx: 360 |  Loss: (0.3798) | Acc: (87.54%) (40450/46208)\n",
            "Epoch: 58 | Batch_idx: 370 |  Loss: (0.3797) | Acc: (87.53%) (41568/47488)\n",
            "Epoch: 58 | Batch_idx: 380 |  Loss: (0.3798) | Acc: (87.51%) (42679/48768)\n",
            "Epoch: 58 | Batch_idx: 390 |  Loss: (0.3797) | Acc: (87.51%) (43755/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4612) | Acc: (85.33%) (8533/10000)\n",
            "Epoch: 59 | Batch_idx: 0 |  Loss: (0.2927) | Acc: (89.84%) (115/128)\n",
            "Epoch: 59 | Batch_idx: 10 |  Loss: (0.3706) | Acc: (87.29%) (1229/1408)\n",
            "Epoch: 59 | Batch_idx: 20 |  Loss: (0.3496) | Acc: (88.43%) (2377/2688)\n",
            "Epoch: 59 | Batch_idx: 30 |  Loss: (0.3513) | Acc: (88.21%) (3500/3968)\n",
            "Epoch: 59 | Batch_idx: 40 |  Loss: (0.3605) | Acc: (87.90%) (4613/5248)\n",
            "Epoch: 59 | Batch_idx: 50 |  Loss: (0.3674) | Acc: (87.94%) (5741/6528)\n",
            "Epoch: 59 | Batch_idx: 60 |  Loss: (0.3672) | Acc: (88.01%) (6872/7808)\n",
            "Epoch: 59 | Batch_idx: 70 |  Loss: (0.3714) | Acc: (88.05%) (8002/9088)\n",
            "Epoch: 59 | Batch_idx: 80 |  Loss: (0.3668) | Acc: (88.21%) (9146/10368)\n",
            "Epoch: 59 | Batch_idx: 90 |  Loss: (0.3661) | Acc: (88.24%) (10278/11648)\n",
            "Epoch: 59 | Batch_idx: 100 |  Loss: (0.3696) | Acc: (88.20%) (11403/12928)\n",
            "Epoch: 59 | Batch_idx: 110 |  Loss: (0.3690) | Acc: (88.17%) (12527/14208)\n",
            "Epoch: 59 | Batch_idx: 120 |  Loss: (0.3666) | Acc: (88.17%) (13655/15488)\n",
            "Epoch: 59 | Batch_idx: 130 |  Loss: (0.3661) | Acc: (88.21%) (14791/16768)\n",
            "Epoch: 59 | Batch_idx: 140 |  Loss: (0.3669) | Acc: (88.19%) (15917/18048)\n",
            "Epoch: 59 | Batch_idx: 150 |  Loss: (0.3679) | Acc: (88.15%) (17037/19328)\n",
            "Epoch: 59 | Batch_idx: 160 |  Loss: (0.3710) | Acc: (88.04%) (18143/20608)\n",
            "Epoch: 59 | Batch_idx: 170 |  Loss: (0.3727) | Acc: (87.94%) (19249/21888)\n",
            "Epoch: 59 | Batch_idx: 180 |  Loss: (0.3725) | Acc: (87.92%) (20370/23168)\n",
            "Epoch: 59 | Batch_idx: 190 |  Loss: (0.3725) | Acc: (87.85%) (21478/24448)\n",
            "Epoch: 59 | Batch_idx: 200 |  Loss: (0.3718) | Acc: (87.83%) (22596/25728)\n",
            "Epoch: 59 | Batch_idx: 210 |  Loss: (0.3697) | Acc: (87.89%) (23737/27008)\n",
            "Epoch: 59 | Batch_idx: 220 |  Loss: (0.3697) | Acc: (87.85%) (24852/28288)\n",
            "Epoch: 59 | Batch_idx: 230 |  Loss: (0.3696) | Acc: (87.87%) (25980/29568)\n",
            "Epoch: 59 | Batch_idx: 240 |  Loss: (0.3701) | Acc: (87.81%) (27089/30848)\n",
            "Epoch: 59 | Batch_idx: 250 |  Loss: (0.3710) | Acc: (87.79%) (28206/32128)\n",
            "Epoch: 59 | Batch_idx: 260 |  Loss: (0.3696) | Acc: (87.83%) (29341/33408)\n",
            "Epoch: 59 | Batch_idx: 270 |  Loss: (0.3702) | Acc: (87.80%) (30456/34688)\n",
            "Epoch: 59 | Batch_idx: 280 |  Loss: (0.3713) | Acc: (87.79%) (31576/35968)\n",
            "Epoch: 59 | Batch_idx: 290 |  Loss: (0.3721) | Acc: (87.75%) (32686/37248)\n",
            "Epoch: 59 | Batch_idx: 300 |  Loss: (0.3733) | Acc: (87.71%) (33792/38528)\n",
            "Epoch: 59 | Batch_idx: 310 |  Loss: (0.3733) | Acc: (87.71%) (34915/39808)\n",
            "Epoch: 59 | Batch_idx: 320 |  Loss: (0.3734) | Acc: (87.72%) (36044/41088)\n",
            "Epoch: 59 | Batch_idx: 330 |  Loss: (0.3739) | Acc: (87.73%) (37169/42368)\n",
            "Epoch: 59 | Batch_idx: 340 |  Loss: (0.3735) | Acc: (87.76%) (38305/43648)\n",
            "Epoch: 59 | Batch_idx: 350 |  Loss: (0.3754) | Acc: (87.71%) (39408/44928)\n",
            "Epoch: 59 | Batch_idx: 360 |  Loss: (0.3769) | Acc: (87.68%) (40514/46208)\n",
            "Epoch: 59 | Batch_idx: 370 |  Loss: (0.3776) | Acc: (87.63%) (41614/47488)\n",
            "Epoch: 59 | Batch_idx: 380 |  Loss: (0.3786) | Acc: (87.61%) (42726/48768)\n",
            "Epoch: 59 | Batch_idx: 390 |  Loss: (0.3797) | Acc: (87.57%) (43784/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4068) | Acc: (86.57%) (8657/10000)\n",
            "Epoch: 60 | Batch_idx: 0 |  Loss: (0.4022) | Acc: (86.72%) (111/128)\n",
            "Epoch: 60 | Batch_idx: 10 |  Loss: (0.3350) | Acc: (89.20%) (1256/1408)\n",
            "Epoch: 60 | Batch_idx: 20 |  Loss: (0.3367) | Acc: (89.21%) (2398/2688)\n",
            "Epoch: 60 | Batch_idx: 30 |  Loss: (0.3456) | Acc: (88.86%) (3526/3968)\n",
            "Epoch: 60 | Batch_idx: 40 |  Loss: (0.3430) | Acc: (89.10%) (4676/5248)\n",
            "Epoch: 60 | Batch_idx: 50 |  Loss: (0.3529) | Acc: (88.86%) (5801/6528)\n",
            "Epoch: 60 | Batch_idx: 60 |  Loss: (0.3570) | Acc: (88.64%) (6921/7808)\n",
            "Epoch: 60 | Batch_idx: 70 |  Loss: (0.3582) | Acc: (88.51%) (8044/9088)\n",
            "Epoch: 60 | Batch_idx: 80 |  Loss: (0.3532) | Acc: (88.74%) (9201/10368)\n",
            "Epoch: 60 | Batch_idx: 90 |  Loss: (0.3508) | Acc: (88.75%) (10338/11648)\n",
            "Epoch: 60 | Batch_idx: 100 |  Loss: (0.3513) | Acc: (88.70%) (11467/12928)\n",
            "Epoch: 60 | Batch_idx: 110 |  Loss: (0.3517) | Acc: (88.67%) (12598/14208)\n",
            "Epoch: 60 | Batch_idx: 120 |  Loss: (0.3519) | Acc: (88.63%) (13727/15488)\n",
            "Epoch: 60 | Batch_idx: 130 |  Loss: (0.3557) | Acc: (88.51%) (14841/16768)\n",
            "Epoch: 60 | Batch_idx: 140 |  Loss: (0.3569) | Acc: (88.47%) (15967/18048)\n",
            "Epoch: 60 | Batch_idx: 150 |  Loss: (0.3580) | Acc: (88.39%) (17084/19328)\n",
            "Epoch: 60 | Batch_idx: 160 |  Loss: (0.3578) | Acc: (88.44%) (18226/20608)\n",
            "Epoch: 60 | Batch_idx: 170 |  Loss: (0.3594) | Acc: (88.39%) (19346/21888)\n",
            "Epoch: 60 | Batch_idx: 180 |  Loss: (0.3632) | Acc: (88.24%) (20443/23168)\n",
            "Epoch: 60 | Batch_idx: 190 |  Loss: (0.3650) | Acc: (88.23%) (21571/24448)\n",
            "Epoch: 60 | Batch_idx: 200 |  Loss: (0.3644) | Acc: (88.25%) (22706/25728)\n",
            "Epoch: 60 | Batch_idx: 210 |  Loss: (0.3660) | Acc: (88.19%) (23819/27008)\n",
            "Epoch: 60 | Batch_idx: 220 |  Loss: (0.3675) | Acc: (88.19%) (24946/28288)\n",
            "Epoch: 60 | Batch_idx: 230 |  Loss: (0.3689) | Acc: (88.15%) (26063/29568)\n",
            "Epoch: 60 | Batch_idx: 240 |  Loss: (0.3706) | Acc: (88.09%) (27174/30848)\n",
            "Epoch: 60 | Batch_idx: 250 |  Loss: (0.3699) | Acc: (88.08%) (28298/32128)\n",
            "Epoch: 60 | Batch_idx: 260 |  Loss: (0.3701) | Acc: (88.03%) (29409/33408)\n",
            "Epoch: 60 | Batch_idx: 270 |  Loss: (0.3707) | Acc: (88.00%) (30525/34688)\n",
            "Epoch: 60 | Batch_idx: 280 |  Loss: (0.3709) | Acc: (87.99%) (31648/35968)\n",
            "Epoch: 60 | Batch_idx: 290 |  Loss: (0.3715) | Acc: (87.98%) (32770/37248)\n",
            "Epoch: 60 | Batch_idx: 300 |  Loss: (0.3739) | Acc: (87.93%) (33876/38528)\n",
            "Epoch: 60 | Batch_idx: 310 |  Loss: (0.3760) | Acc: (87.87%) (34981/39808)\n",
            "Epoch: 60 | Batch_idx: 320 |  Loss: (0.3761) | Acc: (87.87%) (36103/41088)\n",
            "Epoch: 60 | Batch_idx: 330 |  Loss: (0.3759) | Acc: (87.87%) (37228/42368)\n",
            "Epoch: 60 | Batch_idx: 340 |  Loss: (0.3758) | Acc: (87.88%) (38357/43648)\n",
            "Epoch: 60 | Batch_idx: 350 |  Loss: (0.3759) | Acc: (87.87%) (39477/44928)\n",
            "Epoch: 60 | Batch_idx: 360 |  Loss: (0.3756) | Acc: (87.88%) (40608/46208)\n",
            "Epoch: 60 | Batch_idx: 370 |  Loss: (0.3768) | Acc: (87.85%) (41716/47488)\n",
            "Epoch: 60 | Batch_idx: 380 |  Loss: (0.3771) | Acc: (87.84%) (42836/48768)\n",
            "Epoch: 60 | Batch_idx: 390 |  Loss: (0.3777) | Acc: (87.84%) (43921/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3912) | Acc: (87.21%) (8721/10000)\n",
            "Epoch: 61 | Batch_idx: 0 |  Loss: (0.4139) | Acc: (86.72%) (111/128)\n",
            "Epoch: 61 | Batch_idx: 10 |  Loss: (0.3711) | Acc: (87.14%) (1227/1408)\n",
            "Epoch: 61 | Batch_idx: 20 |  Loss: (0.3601) | Acc: (87.83%) (2361/2688)\n",
            "Epoch: 61 | Batch_idx: 30 |  Loss: (0.3580) | Acc: (87.95%) (3490/3968)\n",
            "Epoch: 61 | Batch_idx: 40 |  Loss: (0.3584) | Acc: (87.96%) (4616/5248)\n",
            "Epoch: 61 | Batch_idx: 50 |  Loss: (0.3652) | Acc: (87.67%) (5723/6528)\n",
            "Epoch: 61 | Batch_idx: 60 |  Loss: (0.3690) | Acc: (87.83%) (6858/7808)\n",
            "Epoch: 61 | Batch_idx: 70 |  Loss: (0.3689) | Acc: (87.86%) (7985/9088)\n",
            "Epoch: 61 | Batch_idx: 80 |  Loss: (0.3721) | Acc: (87.64%) (9086/10368)\n",
            "Epoch: 61 | Batch_idx: 90 |  Loss: (0.3656) | Acc: (87.91%) (10240/11648)\n",
            "Epoch: 61 | Batch_idx: 100 |  Loss: (0.3623) | Acc: (88.03%) (11381/12928)\n",
            "Epoch: 61 | Batch_idx: 110 |  Loss: (0.3629) | Acc: (88.04%) (12509/14208)\n",
            "Epoch: 61 | Batch_idx: 120 |  Loss: (0.3619) | Acc: (88.00%) (13630/15488)\n",
            "Epoch: 61 | Batch_idx: 130 |  Loss: (0.3618) | Acc: (87.94%) (14746/16768)\n",
            "Epoch: 61 | Batch_idx: 140 |  Loss: (0.3613) | Acc: (87.99%) (15881/18048)\n",
            "Epoch: 61 | Batch_idx: 150 |  Loss: (0.3620) | Acc: (88.00%) (17008/19328)\n",
            "Epoch: 61 | Batch_idx: 160 |  Loss: (0.3625) | Acc: (87.95%) (18124/20608)\n",
            "Epoch: 61 | Batch_idx: 170 |  Loss: (0.3630) | Acc: (87.88%) (19235/21888)\n",
            "Epoch: 61 | Batch_idx: 180 |  Loss: (0.3621) | Acc: (87.91%) (20367/23168)\n",
            "Epoch: 61 | Batch_idx: 190 |  Loss: (0.3624) | Acc: (87.93%) (21497/24448)\n",
            "Epoch: 61 | Batch_idx: 200 |  Loss: (0.3634) | Acc: (87.89%) (22613/25728)\n",
            "Epoch: 61 | Batch_idx: 210 |  Loss: (0.3654) | Acc: (87.80%) (23714/27008)\n",
            "Epoch: 61 | Batch_idx: 220 |  Loss: (0.3658) | Acc: (87.81%) (24840/28288)\n",
            "Epoch: 61 | Batch_idx: 230 |  Loss: (0.3682) | Acc: (87.75%) (25945/29568)\n",
            "Epoch: 61 | Batch_idx: 240 |  Loss: (0.3686) | Acc: (87.71%) (27057/30848)\n",
            "Epoch: 61 | Batch_idx: 250 |  Loss: (0.3697) | Acc: (87.71%) (28181/32128)\n",
            "Epoch: 61 | Batch_idx: 260 |  Loss: (0.3699) | Acc: (87.72%) (29306/33408)\n",
            "Epoch: 61 | Batch_idx: 270 |  Loss: (0.3698) | Acc: (87.73%) (30431/34688)\n",
            "Epoch: 61 | Batch_idx: 280 |  Loss: (0.3711) | Acc: (87.67%) (31532/35968)\n",
            "Epoch: 61 | Batch_idx: 290 |  Loss: (0.3723) | Acc: (87.63%) (32642/37248)\n",
            "Epoch: 61 | Batch_idx: 300 |  Loss: (0.3726) | Acc: (87.63%) (33762/38528)\n",
            "Epoch: 61 | Batch_idx: 310 |  Loss: (0.3727) | Acc: (87.65%) (34892/39808)\n",
            "Epoch: 61 | Batch_idx: 320 |  Loss: (0.3736) | Acc: (87.64%) (36008/41088)\n",
            "Epoch: 61 | Batch_idx: 330 |  Loss: (0.3734) | Acc: (87.64%) (37130/42368)\n",
            "Epoch: 61 | Batch_idx: 340 |  Loss: (0.3734) | Acc: (87.65%) (38258/43648)\n",
            "Epoch: 61 | Batch_idx: 350 |  Loss: (0.3736) | Acc: (87.64%) (39374/44928)\n",
            "Epoch: 61 | Batch_idx: 360 |  Loss: (0.3735) | Acc: (87.66%) (40504/46208)\n",
            "Epoch: 61 | Batch_idx: 370 |  Loss: (0.3743) | Acc: (87.63%) (41614/47488)\n",
            "Epoch: 61 | Batch_idx: 380 |  Loss: (0.3747) | Acc: (87.61%) (42728/48768)\n",
            "Epoch: 61 | Batch_idx: 390 |  Loss: (0.3745) | Acc: (87.62%) (43810/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4140) | Acc: (87.01%) (8701/10000)\n",
            "Epoch: 62 | Batch_idx: 0 |  Loss: (0.4501) | Acc: (85.94%) (110/128)\n",
            "Epoch: 62 | Batch_idx: 10 |  Loss: (0.3309) | Acc: (89.35%) (1258/1408)\n",
            "Epoch: 62 | Batch_idx: 20 |  Loss: (0.3545) | Acc: (88.58%) (2381/2688)\n",
            "Epoch: 62 | Batch_idx: 30 |  Loss: (0.3606) | Acc: (88.56%) (3514/3968)\n",
            "Epoch: 62 | Batch_idx: 40 |  Loss: (0.3680) | Acc: (88.22%) (4630/5248)\n",
            "Epoch: 62 | Batch_idx: 50 |  Loss: (0.3631) | Acc: (88.16%) (5755/6528)\n",
            "Epoch: 62 | Batch_idx: 60 |  Loss: (0.3618) | Acc: (88.10%) (6879/7808)\n",
            "Epoch: 62 | Batch_idx: 70 |  Loss: (0.3622) | Acc: (88.13%) (8009/9088)\n",
            "Epoch: 62 | Batch_idx: 80 |  Loss: (0.3632) | Acc: (88.07%) (9131/10368)\n",
            "Epoch: 62 | Batch_idx: 90 |  Loss: (0.3650) | Acc: (87.96%) (10246/11648)\n",
            "Epoch: 62 | Batch_idx: 100 |  Loss: (0.3669) | Acc: (87.90%) (11364/12928)\n",
            "Epoch: 62 | Batch_idx: 110 |  Loss: (0.3663) | Acc: (87.89%) (12488/14208)\n",
            "Epoch: 62 | Batch_idx: 120 |  Loss: (0.3640) | Acc: (87.96%) (13624/15488)\n",
            "Epoch: 62 | Batch_idx: 130 |  Loss: (0.3618) | Acc: (88.10%) (14772/16768)\n",
            "Epoch: 62 | Batch_idx: 140 |  Loss: (0.3668) | Acc: (87.99%) (15880/18048)\n",
            "Epoch: 62 | Batch_idx: 150 |  Loss: (0.3694) | Acc: (87.92%) (16994/19328)\n",
            "Epoch: 62 | Batch_idx: 160 |  Loss: (0.3675) | Acc: (87.96%) (18126/20608)\n",
            "Epoch: 62 | Batch_idx: 170 |  Loss: (0.3682) | Acc: (87.92%) (19243/21888)\n",
            "Epoch: 62 | Batch_idx: 180 |  Loss: (0.3669) | Acc: (87.93%) (20372/23168)\n",
            "Epoch: 62 | Batch_idx: 190 |  Loss: (0.3660) | Acc: (87.95%) (21502/24448)\n",
            "Epoch: 62 | Batch_idx: 200 |  Loss: (0.3694) | Acc: (87.85%) (22602/25728)\n",
            "Epoch: 62 | Batch_idx: 210 |  Loss: (0.3712) | Acc: (87.81%) (23717/27008)\n",
            "Epoch: 62 | Batch_idx: 220 |  Loss: (0.3711) | Acc: (87.84%) (24847/28288)\n",
            "Epoch: 62 | Batch_idx: 230 |  Loss: (0.3723) | Acc: (87.80%) (25960/29568)\n",
            "Epoch: 62 | Batch_idx: 240 |  Loss: (0.3721) | Acc: (87.79%) (27083/30848)\n",
            "Epoch: 62 | Batch_idx: 250 |  Loss: (0.3722) | Acc: (87.82%) (28214/32128)\n",
            "Epoch: 62 | Batch_idx: 260 |  Loss: (0.3715) | Acc: (87.83%) (29342/33408)\n",
            "Epoch: 62 | Batch_idx: 270 |  Loss: (0.3712) | Acc: (87.84%) (30469/34688)\n",
            "Epoch: 62 | Batch_idx: 280 |  Loss: (0.3716) | Acc: (87.83%) (31591/35968)\n",
            "Epoch: 62 | Batch_idx: 290 |  Loss: (0.3720) | Acc: (87.83%) (32716/37248)\n",
            "Epoch: 62 | Batch_idx: 300 |  Loss: (0.3741) | Acc: (87.79%) (33822/38528)\n",
            "Epoch: 62 | Batch_idx: 310 |  Loss: (0.3750) | Acc: (87.78%) (34942/39808)\n",
            "Epoch: 62 | Batch_idx: 320 |  Loss: (0.3746) | Acc: (87.82%) (36083/41088)\n",
            "Epoch: 62 | Batch_idx: 330 |  Loss: (0.3757) | Acc: (87.77%) (37186/42368)\n",
            "Epoch: 62 | Batch_idx: 340 |  Loss: (0.3755) | Acc: (87.77%) (38310/43648)\n",
            "Epoch: 62 | Batch_idx: 350 |  Loss: (0.3745) | Acc: (87.78%) (39437/44928)\n",
            "Epoch: 62 | Batch_idx: 360 |  Loss: (0.3743) | Acc: (87.80%) (40570/46208)\n",
            "Epoch: 62 | Batch_idx: 370 |  Loss: (0.3748) | Acc: (87.80%) (41695/47488)\n",
            "Epoch: 62 | Batch_idx: 380 |  Loss: (0.3757) | Acc: (87.76%) (42797/48768)\n",
            "Epoch: 62 | Batch_idx: 390 |  Loss: (0.3755) | Acc: (87.77%) (43885/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3964) | Acc: (86.90%) (8690/10000)\n",
            "Epoch: 63 | Batch_idx: 0 |  Loss: (0.5060) | Acc: (85.16%) (109/128)\n",
            "Epoch: 63 | Batch_idx: 10 |  Loss: (0.3404) | Acc: (89.13%) (1255/1408)\n",
            "Epoch: 63 | Batch_idx: 20 |  Loss: (0.3549) | Acc: (88.58%) (2381/2688)\n",
            "Epoch: 63 | Batch_idx: 30 |  Loss: (0.3565) | Acc: (88.16%) (3498/3968)\n",
            "Epoch: 63 | Batch_idx: 40 |  Loss: (0.3587) | Acc: (88.00%) (4618/5248)\n",
            "Epoch: 63 | Batch_idx: 50 |  Loss: (0.3622) | Acc: (87.71%) (5726/6528)\n",
            "Epoch: 63 | Batch_idx: 60 |  Loss: (0.3611) | Acc: (87.87%) (6861/7808)\n",
            "Epoch: 63 | Batch_idx: 70 |  Loss: (0.3558) | Acc: (88.11%) (8007/9088)\n",
            "Epoch: 63 | Batch_idx: 80 |  Loss: (0.3538) | Acc: (88.26%) (9151/10368)\n",
            "Epoch: 63 | Batch_idx: 90 |  Loss: (0.3525) | Acc: (88.39%) (10296/11648)\n",
            "Epoch: 63 | Batch_idx: 100 |  Loss: (0.3491) | Acc: (88.50%) (11441/12928)\n",
            "Epoch: 63 | Batch_idx: 110 |  Loss: (0.3556) | Acc: (88.35%) (12553/14208)\n",
            "Epoch: 63 | Batch_idx: 120 |  Loss: (0.3558) | Acc: (88.39%) (13690/15488)\n",
            "Epoch: 63 | Batch_idx: 130 |  Loss: (0.3539) | Acc: (88.43%) (14828/16768)\n",
            "Epoch: 63 | Batch_idx: 140 |  Loss: (0.3532) | Acc: (88.53%) (15978/18048)\n",
            "Epoch: 63 | Batch_idx: 150 |  Loss: (0.3594) | Acc: (88.35%) (17077/19328)\n",
            "Epoch: 63 | Batch_idx: 160 |  Loss: (0.3617) | Acc: (88.33%) (18204/20608)\n",
            "Epoch: 63 | Batch_idx: 170 |  Loss: (0.3609) | Acc: (88.28%) (19323/21888)\n",
            "Epoch: 63 | Batch_idx: 180 |  Loss: (0.3605) | Acc: (88.32%) (20462/23168)\n",
            "Epoch: 63 | Batch_idx: 190 |  Loss: (0.3600) | Acc: (88.32%) (21592/24448)\n",
            "Epoch: 63 | Batch_idx: 200 |  Loss: (0.3631) | Acc: (88.24%) (22703/25728)\n",
            "Epoch: 63 | Batch_idx: 210 |  Loss: (0.3619) | Acc: (88.28%) (23842/27008)\n",
            "Epoch: 63 | Batch_idx: 220 |  Loss: (0.3615) | Acc: (88.28%) (24973/28288)\n",
            "Epoch: 63 | Batch_idx: 230 |  Loss: (0.3611) | Acc: (88.26%) (26096/29568)\n",
            "Epoch: 63 | Batch_idx: 240 |  Loss: (0.3616) | Acc: (88.22%) (27214/30848)\n",
            "Epoch: 63 | Batch_idx: 250 |  Loss: (0.3631) | Acc: (88.18%) (28330/32128)\n",
            "Epoch: 63 | Batch_idx: 260 |  Loss: (0.3619) | Acc: (88.22%) (29472/33408)\n",
            "Epoch: 63 | Batch_idx: 270 |  Loss: (0.3637) | Acc: (88.16%) (30580/34688)\n",
            "Epoch: 63 | Batch_idx: 280 |  Loss: (0.3655) | Acc: (88.11%) (31691/35968)\n",
            "Epoch: 63 | Batch_idx: 290 |  Loss: (0.3671) | Acc: (88.06%) (32799/37248)\n",
            "Epoch: 63 | Batch_idx: 300 |  Loss: (0.3680) | Acc: (88.02%) (33913/38528)\n",
            "Epoch: 63 | Batch_idx: 310 |  Loss: (0.3681) | Acc: (88.00%) (35033/39808)\n",
            "Epoch: 63 | Batch_idx: 320 |  Loss: (0.3690) | Acc: (88.00%) (36158/41088)\n",
            "Epoch: 63 | Batch_idx: 330 |  Loss: (0.3678) | Acc: (88.02%) (37293/42368)\n",
            "Epoch: 63 | Batch_idx: 340 |  Loss: (0.3683) | Acc: (88.02%) (38420/43648)\n",
            "Epoch: 63 | Batch_idx: 350 |  Loss: (0.3688) | Acc: (88.01%) (39542/44928)\n",
            "Epoch: 63 | Batch_idx: 360 |  Loss: (0.3698) | Acc: (87.97%) (40651/46208)\n",
            "Epoch: 63 | Batch_idx: 370 |  Loss: (0.3700) | Acc: (87.96%) (41770/47488)\n",
            "Epoch: 63 | Batch_idx: 380 |  Loss: (0.3702) | Acc: (87.97%) (42900/48768)\n",
            "Epoch: 63 | Batch_idx: 390 |  Loss: (0.3713) | Acc: (87.97%) (43984/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4073) | Acc: (86.72%) (8672/10000)\n",
            "Epoch: 64 | Batch_idx: 0 |  Loss: (0.2981) | Acc: (90.62%) (116/128)\n",
            "Epoch: 64 | Batch_idx: 10 |  Loss: (0.3334) | Acc: (88.71%) (1249/1408)\n",
            "Epoch: 64 | Batch_idx: 20 |  Loss: (0.3284) | Acc: (88.95%) (2391/2688)\n",
            "Epoch: 64 | Batch_idx: 30 |  Loss: (0.3313) | Acc: (89.04%) (3533/3968)\n",
            "Epoch: 64 | Batch_idx: 40 |  Loss: (0.3369) | Acc: (88.83%) (4662/5248)\n",
            "Epoch: 64 | Batch_idx: 50 |  Loss: (0.3412) | Acc: (88.68%) (5789/6528)\n",
            "Epoch: 64 | Batch_idx: 60 |  Loss: (0.3479) | Acc: (88.51%) (6911/7808)\n",
            "Epoch: 64 | Batch_idx: 70 |  Loss: (0.3540) | Acc: (88.30%) (8025/9088)\n",
            "Epoch: 64 | Batch_idx: 80 |  Loss: (0.3536) | Acc: (88.37%) (9162/10368)\n",
            "Epoch: 64 | Batch_idx: 90 |  Loss: (0.3583) | Acc: (88.24%) (10278/11648)\n",
            "Epoch: 64 | Batch_idx: 100 |  Loss: (0.3609) | Acc: (88.17%) (11399/12928)\n",
            "Epoch: 64 | Batch_idx: 110 |  Loss: (0.3585) | Acc: (88.22%) (12535/14208)\n",
            "Epoch: 64 | Batch_idx: 120 |  Loss: (0.3602) | Acc: (88.16%) (13654/15488)\n",
            "Epoch: 64 | Batch_idx: 130 |  Loss: (0.3571) | Acc: (88.26%) (14799/16768)\n",
            "Epoch: 64 | Batch_idx: 140 |  Loss: (0.3548) | Acc: (88.29%) (15935/18048)\n",
            "Epoch: 64 | Batch_idx: 150 |  Loss: (0.3579) | Acc: (88.18%) (17044/19328)\n",
            "Epoch: 64 | Batch_idx: 160 |  Loss: (0.3560) | Acc: (88.22%) (18181/20608)\n",
            "Epoch: 64 | Batch_idx: 170 |  Loss: (0.3556) | Acc: (88.25%) (19317/21888)\n",
            "Epoch: 64 | Batch_idx: 180 |  Loss: (0.3581) | Acc: (88.23%) (20440/23168)\n",
            "Epoch: 64 | Batch_idx: 190 |  Loss: (0.3590) | Acc: (88.20%) (21563/24448)\n",
            "Epoch: 64 | Batch_idx: 200 |  Loss: (0.3602) | Acc: (88.13%) (22675/25728)\n",
            "Epoch: 64 | Batch_idx: 210 |  Loss: (0.3600) | Acc: (88.11%) (23796/27008)\n",
            "Epoch: 64 | Batch_idx: 220 |  Loss: (0.3611) | Acc: (88.09%) (24919/28288)\n",
            "Epoch: 64 | Batch_idx: 230 |  Loss: (0.3609) | Acc: (88.10%) (26049/29568)\n",
            "Epoch: 64 | Batch_idx: 240 |  Loss: (0.3600) | Acc: (88.13%) (27185/30848)\n",
            "Epoch: 64 | Batch_idx: 250 |  Loss: (0.3594) | Acc: (88.14%) (28319/32128)\n",
            "Epoch: 64 | Batch_idx: 260 |  Loss: (0.3606) | Acc: (88.09%) (29429/33408)\n",
            "Epoch: 64 | Batch_idx: 270 |  Loss: (0.3618) | Acc: (88.04%) (30539/34688)\n",
            "Epoch: 64 | Batch_idx: 280 |  Loss: (0.3616) | Acc: (88.07%) (31677/35968)\n",
            "Epoch: 64 | Batch_idx: 290 |  Loss: (0.3622) | Acc: (88.05%) (32798/37248)\n",
            "Epoch: 64 | Batch_idx: 300 |  Loss: (0.3621) | Acc: (88.05%) (33925/38528)\n",
            "Epoch: 64 | Batch_idx: 310 |  Loss: (0.3638) | Acc: (88.01%) (35034/39808)\n",
            "Epoch: 64 | Batch_idx: 320 |  Loss: (0.3634) | Acc: (88.00%) (36156/41088)\n",
            "Epoch: 64 | Batch_idx: 330 |  Loss: (0.3641) | Acc: (87.96%) (37268/42368)\n",
            "Epoch: 64 | Batch_idx: 340 |  Loss: (0.3641) | Acc: (87.95%) (38388/43648)\n",
            "Epoch: 64 | Batch_idx: 350 |  Loss: (0.3647) | Acc: (87.93%) (39505/44928)\n",
            "Epoch: 64 | Batch_idx: 360 |  Loss: (0.3666) | Acc: (87.86%) (40597/46208)\n",
            "Epoch: 64 | Batch_idx: 370 |  Loss: (0.3665) | Acc: (87.86%) (41725/47488)\n",
            "Epoch: 64 | Batch_idx: 380 |  Loss: (0.3662) | Acc: (87.91%) (42871/48768)\n",
            "Epoch: 64 | Batch_idx: 390 |  Loss: (0.3661) | Acc: (87.93%) (43967/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4215) | Acc: (86.56%) (8656/10000)\n",
            "Epoch: 65 | Batch_idx: 0 |  Loss: (0.2490) | Acc: (91.41%) (117/128)\n",
            "Epoch: 65 | Batch_idx: 10 |  Loss: (0.3161) | Acc: (89.20%) (1256/1408)\n",
            "Epoch: 65 | Batch_idx: 20 |  Loss: (0.3452) | Acc: (88.80%) (2387/2688)\n",
            "Epoch: 65 | Batch_idx: 30 |  Loss: (0.3314) | Acc: (89.29%) (3543/3968)\n",
            "Epoch: 65 | Batch_idx: 40 |  Loss: (0.3348) | Acc: (89.16%) (4679/5248)\n",
            "Epoch: 65 | Batch_idx: 50 |  Loss: (0.3317) | Acc: (89.12%) (5818/6528)\n",
            "Epoch: 65 | Batch_idx: 60 |  Loss: (0.3291) | Acc: (89.28%) (6971/7808)\n",
            "Epoch: 65 | Batch_idx: 70 |  Loss: (0.3347) | Acc: (89.16%) (8103/9088)\n",
            "Epoch: 65 | Batch_idx: 80 |  Loss: (0.3407) | Acc: (89.03%) (9231/10368)\n",
            "Epoch: 65 | Batch_idx: 90 |  Loss: (0.3436) | Acc: (88.89%) (10354/11648)\n",
            "Epoch: 65 | Batch_idx: 100 |  Loss: (0.3433) | Acc: (88.90%) (11493/12928)\n",
            "Epoch: 65 | Batch_idx: 110 |  Loss: (0.3471) | Acc: (88.81%) (12618/14208)\n",
            "Epoch: 65 | Batch_idx: 120 |  Loss: (0.3477) | Acc: (88.79%) (13752/15488)\n",
            "Epoch: 65 | Batch_idx: 130 |  Loss: (0.3484) | Acc: (88.76%) (14884/16768)\n",
            "Epoch: 65 | Batch_idx: 140 |  Loss: (0.3503) | Acc: (88.64%) (15998/18048)\n",
            "Epoch: 65 | Batch_idx: 150 |  Loss: (0.3501) | Acc: (88.64%) (17132/19328)\n",
            "Epoch: 65 | Batch_idx: 160 |  Loss: (0.3500) | Acc: (88.59%) (18256/20608)\n",
            "Epoch: 65 | Batch_idx: 170 |  Loss: (0.3489) | Acc: (88.64%) (19402/21888)\n",
            "Epoch: 65 | Batch_idx: 180 |  Loss: (0.3501) | Acc: (88.55%) (20515/23168)\n",
            "Epoch: 65 | Batch_idx: 190 |  Loss: (0.3532) | Acc: (88.47%) (21628/24448)\n",
            "Epoch: 65 | Batch_idx: 200 |  Loss: (0.3529) | Acc: (88.50%) (22769/25728)\n",
            "Epoch: 65 | Batch_idx: 210 |  Loss: (0.3553) | Acc: (88.44%) (23886/27008)\n",
            "Epoch: 65 | Batch_idx: 220 |  Loss: (0.3554) | Acc: (88.41%) (25010/28288)\n",
            "Epoch: 65 | Batch_idx: 230 |  Loss: (0.3543) | Acc: (88.46%) (26157/29568)\n",
            "Epoch: 65 | Batch_idx: 240 |  Loss: (0.3566) | Acc: (88.39%) (27267/30848)\n",
            "Epoch: 65 | Batch_idx: 250 |  Loss: (0.3591) | Acc: (88.28%) (28363/32128)\n",
            "Epoch: 65 | Batch_idx: 260 |  Loss: (0.3606) | Acc: (88.24%) (29480/33408)\n",
            "Epoch: 65 | Batch_idx: 270 |  Loss: (0.3599) | Acc: (88.27%) (30620/34688)\n",
            "Epoch: 65 | Batch_idx: 280 |  Loss: (0.3605) | Acc: (88.26%) (31745/35968)\n",
            "Epoch: 65 | Batch_idx: 290 |  Loss: (0.3598) | Acc: (88.29%) (32885/37248)\n",
            "Epoch: 65 | Batch_idx: 300 |  Loss: (0.3603) | Acc: (88.25%) (34001/38528)\n",
            "Epoch: 65 | Batch_idx: 310 |  Loss: (0.3601) | Acc: (88.24%) (35127/39808)\n",
            "Epoch: 65 | Batch_idx: 320 |  Loss: (0.3603) | Acc: (88.21%) (36245/41088)\n",
            "Epoch: 65 | Batch_idx: 330 |  Loss: (0.3595) | Acc: (88.26%) (37393/42368)\n",
            "Epoch: 65 | Batch_idx: 340 |  Loss: (0.3599) | Acc: (88.27%) (38526/43648)\n",
            "Epoch: 65 | Batch_idx: 350 |  Loss: (0.3612) | Acc: (88.24%) (39644/44928)\n",
            "Epoch: 65 | Batch_idx: 360 |  Loss: (0.3615) | Acc: (88.23%) (40769/46208)\n",
            "Epoch: 65 | Batch_idx: 370 |  Loss: (0.3618) | Acc: (88.20%) (41884/47488)\n",
            "Epoch: 65 | Batch_idx: 380 |  Loss: (0.3615) | Acc: (88.20%) (43015/48768)\n",
            "Epoch: 65 | Batch_idx: 390 |  Loss: (0.3626) | Acc: (88.17%) (44087/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3951) | Acc: (87.52%) (8752/10000)\n",
            "Epoch: 66 | Batch_idx: 0 |  Loss: (0.4294) | Acc: (86.72%) (111/128)\n",
            "Epoch: 66 | Batch_idx: 10 |  Loss: (0.3564) | Acc: (88.85%) (1251/1408)\n",
            "Epoch: 66 | Batch_idx: 20 |  Loss: (0.3494) | Acc: (88.43%) (2377/2688)\n",
            "Epoch: 66 | Batch_idx: 30 |  Loss: (0.3490) | Acc: (88.63%) (3517/3968)\n",
            "Epoch: 66 | Batch_idx: 40 |  Loss: (0.3478) | Acc: (88.55%) (4647/5248)\n",
            "Epoch: 66 | Batch_idx: 50 |  Loss: (0.3445) | Acc: (88.59%) (5783/6528)\n",
            "Epoch: 66 | Batch_idx: 60 |  Loss: (0.3470) | Acc: (88.60%) (6918/7808)\n",
            "Epoch: 66 | Batch_idx: 70 |  Loss: (0.3453) | Acc: (88.62%) (8054/9088)\n",
            "Epoch: 66 | Batch_idx: 80 |  Loss: (0.3433) | Acc: (88.68%) (9194/10368)\n",
            "Epoch: 66 | Batch_idx: 90 |  Loss: (0.3476) | Acc: (88.62%) (10323/11648)\n",
            "Epoch: 66 | Batch_idx: 100 |  Loss: (0.3456) | Acc: (88.61%) (11455/12928)\n",
            "Epoch: 66 | Batch_idx: 110 |  Loss: (0.3465) | Acc: (88.64%) (12594/14208)\n",
            "Epoch: 66 | Batch_idx: 120 |  Loss: (0.3452) | Acc: (88.71%) (13740/15488)\n",
            "Epoch: 66 | Batch_idx: 130 |  Loss: (0.3471) | Acc: (88.65%) (14865/16768)\n",
            "Epoch: 66 | Batch_idx: 140 |  Loss: (0.3502) | Acc: (88.57%) (15985/18048)\n",
            "Epoch: 66 | Batch_idx: 150 |  Loss: (0.3513) | Acc: (88.53%) (17111/19328)\n",
            "Epoch: 66 | Batch_idx: 160 |  Loss: (0.3523) | Acc: (88.48%) (18234/20608)\n",
            "Epoch: 66 | Batch_idx: 170 |  Loss: (0.3499) | Acc: (88.57%) (19386/21888)\n",
            "Epoch: 66 | Batch_idx: 180 |  Loss: (0.3517) | Acc: (88.47%) (20497/23168)\n",
            "Epoch: 66 | Batch_idx: 190 |  Loss: (0.3496) | Acc: (88.53%) (21645/24448)\n",
            "Epoch: 66 | Batch_idx: 200 |  Loss: (0.3511) | Acc: (88.51%) (22771/25728)\n",
            "Epoch: 66 | Batch_idx: 210 |  Loss: (0.3501) | Acc: (88.53%) (23909/27008)\n",
            "Epoch: 66 | Batch_idx: 220 |  Loss: (0.3493) | Acc: (88.54%) (25045/28288)\n",
            "Epoch: 66 | Batch_idx: 230 |  Loss: (0.3496) | Acc: (88.53%) (26178/29568)\n",
            "Epoch: 66 | Batch_idx: 240 |  Loss: (0.3502) | Acc: (88.53%) (27309/30848)\n",
            "Epoch: 66 | Batch_idx: 250 |  Loss: (0.3501) | Acc: (88.52%) (28441/32128)\n",
            "Epoch: 66 | Batch_idx: 260 |  Loss: (0.3524) | Acc: (88.44%) (29545/33408)\n",
            "Epoch: 66 | Batch_idx: 270 |  Loss: (0.3531) | Acc: (88.40%) (30664/34688)\n",
            "Epoch: 66 | Batch_idx: 280 |  Loss: (0.3537) | Acc: (88.42%) (31802/35968)\n",
            "Epoch: 66 | Batch_idx: 290 |  Loss: (0.3545) | Acc: (88.38%) (32919/37248)\n",
            "Epoch: 66 | Batch_idx: 300 |  Loss: (0.3557) | Acc: (88.33%) (34033/38528)\n",
            "Epoch: 66 | Batch_idx: 310 |  Loss: (0.3565) | Acc: (88.32%) (35159/39808)\n",
            "Epoch: 66 | Batch_idx: 320 |  Loss: (0.3570) | Acc: (88.30%) (36281/41088)\n",
            "Epoch: 66 | Batch_idx: 330 |  Loss: (0.3574) | Acc: (88.31%) (37414/42368)\n",
            "Epoch: 66 | Batch_idx: 340 |  Loss: (0.3583) | Acc: (88.27%) (38529/43648)\n",
            "Epoch: 66 | Batch_idx: 350 |  Loss: (0.3590) | Acc: (88.24%) (39645/44928)\n",
            "Epoch: 66 | Batch_idx: 360 |  Loss: (0.3598) | Acc: (88.19%) (40753/46208)\n",
            "Epoch: 66 | Batch_idx: 370 |  Loss: (0.3600) | Acc: (88.20%) (41885/47488)\n",
            "Epoch: 66 | Batch_idx: 380 |  Loss: (0.3615) | Acc: (88.16%) (42996/48768)\n",
            "Epoch: 66 | Batch_idx: 390 |  Loss: (0.3613) | Acc: (88.18%) (44091/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3771) | Acc: (87.61%) (8761/10000)\n",
            "Epoch: 67 | Batch_idx: 0 |  Loss: (0.3799) | Acc: (84.38%) (108/128)\n",
            "Epoch: 67 | Batch_idx: 10 |  Loss: (0.3547) | Acc: (88.21%) (1242/1408)\n",
            "Epoch: 67 | Batch_idx: 20 |  Loss: (0.3720) | Acc: (87.50%) (2352/2688)\n",
            "Epoch: 67 | Batch_idx: 30 |  Loss: (0.3620) | Acc: (87.95%) (3490/3968)\n",
            "Epoch: 67 | Batch_idx: 40 |  Loss: (0.3546) | Acc: (88.24%) (4631/5248)\n",
            "Epoch: 67 | Batch_idx: 50 |  Loss: (0.3481) | Acc: (88.57%) (5782/6528)\n",
            "Epoch: 67 | Batch_idx: 60 |  Loss: (0.3489) | Acc: (88.52%) (6912/7808)\n",
            "Epoch: 67 | Batch_idx: 70 |  Loss: (0.3443) | Acc: (88.80%) (8070/9088)\n",
            "Epoch: 67 | Batch_idx: 80 |  Loss: (0.3453) | Acc: (88.74%) (9201/10368)\n",
            "Epoch: 67 | Batch_idx: 90 |  Loss: (0.3441) | Acc: (88.68%) (10329/11648)\n",
            "Epoch: 67 | Batch_idx: 100 |  Loss: (0.3477) | Acc: (88.49%) (11440/12928)\n",
            "Epoch: 67 | Batch_idx: 110 |  Loss: (0.3495) | Acc: (88.38%) (12557/14208)\n",
            "Epoch: 67 | Batch_idx: 120 |  Loss: (0.3528) | Acc: (88.27%) (13672/15488)\n",
            "Epoch: 67 | Batch_idx: 130 |  Loss: (0.3538) | Acc: (88.22%) (14792/16768)\n",
            "Epoch: 67 | Batch_idx: 140 |  Loss: (0.3520) | Acc: (88.28%) (15932/18048)\n",
            "Epoch: 67 | Batch_idx: 150 |  Loss: (0.3537) | Acc: (88.26%) (17059/19328)\n",
            "Epoch: 67 | Batch_idx: 160 |  Loss: (0.3543) | Acc: (88.26%) (18189/20608)\n",
            "Epoch: 67 | Batch_idx: 170 |  Loss: (0.3552) | Acc: (88.28%) (19323/21888)\n",
            "Epoch: 67 | Batch_idx: 180 |  Loss: (0.3550) | Acc: (88.29%) (20454/23168)\n",
            "Epoch: 67 | Batch_idx: 190 |  Loss: (0.3553) | Acc: (88.27%) (21581/24448)\n",
            "Epoch: 67 | Batch_idx: 200 |  Loss: (0.3567) | Acc: (88.22%) (22698/25728)\n",
            "Epoch: 67 | Batch_idx: 210 |  Loss: (0.3565) | Acc: (88.26%) (23836/27008)\n",
            "Epoch: 67 | Batch_idx: 220 |  Loss: (0.3585) | Acc: (88.21%) (24952/28288)\n",
            "Epoch: 67 | Batch_idx: 230 |  Loss: (0.3582) | Acc: (88.21%) (26081/29568)\n",
            "Epoch: 67 | Batch_idx: 240 |  Loss: (0.3571) | Acc: (88.21%) (27212/30848)\n",
            "Epoch: 67 | Batch_idx: 250 |  Loss: (0.3574) | Acc: (88.20%) (28338/32128)\n",
            "Epoch: 67 | Batch_idx: 260 |  Loss: (0.3585) | Acc: (88.19%) (29461/33408)\n",
            "Epoch: 67 | Batch_idx: 270 |  Loss: (0.3573) | Acc: (88.22%) (30601/34688)\n",
            "Epoch: 67 | Batch_idx: 280 |  Loss: (0.3579) | Acc: (88.20%) (31722/35968)\n",
            "Epoch: 67 | Batch_idx: 290 |  Loss: (0.3587) | Acc: (88.20%) (32854/37248)\n",
            "Epoch: 67 | Batch_idx: 300 |  Loss: (0.3600) | Acc: (88.16%) (33968/38528)\n",
            "Epoch: 67 | Batch_idx: 310 |  Loss: (0.3619) | Acc: (88.12%) (35080/39808)\n",
            "Epoch: 67 | Batch_idx: 320 |  Loss: (0.3627) | Acc: (88.07%) (36187/41088)\n",
            "Epoch: 67 | Batch_idx: 330 |  Loss: (0.3621) | Acc: (88.08%) (37318/42368)\n",
            "Epoch: 67 | Batch_idx: 340 |  Loss: (0.3628) | Acc: (88.08%) (38444/43648)\n",
            "Epoch: 67 | Batch_idx: 350 |  Loss: (0.3643) | Acc: (88.05%) (39557/44928)\n",
            "Epoch: 67 | Batch_idx: 360 |  Loss: (0.3652) | Acc: (88.00%) (40665/46208)\n",
            "Epoch: 67 | Batch_idx: 370 |  Loss: (0.3653) | Acc: (88.00%) (41791/47488)\n",
            "Epoch: 67 | Batch_idx: 380 |  Loss: (0.3650) | Acc: (88.03%) (42932/48768)\n",
            "Epoch: 67 | Batch_idx: 390 |  Loss: (0.3663) | Acc: (88.00%) (44000/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4035) | Acc: (86.98%) (8698/10000)\n",
            "Epoch: 68 | Batch_idx: 0 |  Loss: (0.4490) | Acc: (83.59%) (107/128)\n",
            "Epoch: 68 | Batch_idx: 10 |  Loss: (0.3867) | Acc: (86.86%) (1223/1408)\n",
            "Epoch: 68 | Batch_idx: 20 |  Loss: (0.3855) | Acc: (87.31%) (2347/2688)\n",
            "Epoch: 68 | Batch_idx: 30 |  Loss: (0.3721) | Acc: (87.58%) (3475/3968)\n",
            "Epoch: 68 | Batch_idx: 40 |  Loss: (0.3614) | Acc: (88.01%) (4619/5248)\n",
            "Epoch: 68 | Batch_idx: 50 |  Loss: (0.3566) | Acc: (88.10%) (5751/6528)\n",
            "Epoch: 68 | Batch_idx: 60 |  Loss: (0.3636) | Acc: (87.97%) (6869/7808)\n",
            "Epoch: 68 | Batch_idx: 70 |  Loss: (0.3665) | Acc: (87.74%) (7974/9088)\n",
            "Epoch: 68 | Batch_idx: 80 |  Loss: (0.3685) | Acc: (87.58%) (9080/10368)\n",
            "Epoch: 68 | Batch_idx: 90 |  Loss: (0.3693) | Acc: (87.62%) (10206/11648)\n",
            "Epoch: 68 | Batch_idx: 100 |  Loss: (0.3653) | Acc: (87.83%) (11355/12928)\n",
            "Epoch: 68 | Batch_idx: 110 |  Loss: (0.3647) | Acc: (87.89%) (12487/14208)\n",
            "Epoch: 68 | Batch_idx: 120 |  Loss: (0.3635) | Acc: (87.98%) (13627/15488)\n",
            "Epoch: 68 | Batch_idx: 130 |  Loss: (0.3624) | Acc: (87.99%) (14755/16768)\n",
            "Epoch: 68 | Batch_idx: 140 |  Loss: (0.3611) | Acc: (88.05%) (15891/18048)\n",
            "Epoch: 68 | Batch_idx: 150 |  Loss: (0.3595) | Acc: (88.12%) (17032/19328)\n",
            "Epoch: 68 | Batch_idx: 160 |  Loss: (0.3609) | Acc: (88.10%) (18156/20608)\n",
            "Epoch: 68 | Batch_idx: 170 |  Loss: (0.3595) | Acc: (88.14%) (19292/21888)\n",
            "Epoch: 68 | Batch_idx: 180 |  Loss: (0.3590) | Acc: (88.15%) (20422/23168)\n",
            "Epoch: 68 | Batch_idx: 190 |  Loss: (0.3603) | Acc: (88.13%) (21545/24448)\n",
            "Epoch: 68 | Batch_idx: 200 |  Loss: (0.3635) | Acc: (88.05%) (22653/25728)\n",
            "Epoch: 68 | Batch_idx: 210 |  Loss: (0.3626) | Acc: (88.10%) (23794/27008)\n",
            "Epoch: 68 | Batch_idx: 220 |  Loss: (0.3617) | Acc: (88.17%) (24942/28288)\n",
            "Epoch: 68 | Batch_idx: 230 |  Loss: (0.3637) | Acc: (88.09%) (26046/29568)\n",
            "Epoch: 68 | Batch_idx: 240 |  Loss: (0.3631) | Acc: (88.09%) (27174/30848)\n",
            "Epoch: 68 | Batch_idx: 250 |  Loss: (0.3631) | Acc: (88.06%) (28292/32128)\n",
            "Epoch: 68 | Batch_idx: 260 |  Loss: (0.3631) | Acc: (88.09%) (29430/33408)\n",
            "Epoch: 68 | Batch_idx: 270 |  Loss: (0.3642) | Acc: (88.07%) (30551/34688)\n",
            "Epoch: 68 | Batch_idx: 280 |  Loss: (0.3647) | Acc: (88.08%) (31680/35968)\n",
            "Epoch: 68 | Batch_idx: 290 |  Loss: (0.3652) | Acc: (88.08%) (32809/37248)\n",
            "Epoch: 68 | Batch_idx: 300 |  Loss: (0.3644) | Acc: (88.10%) (33945/38528)\n",
            "Epoch: 68 | Batch_idx: 310 |  Loss: (0.3634) | Acc: (88.15%) (35092/39808)\n",
            "Epoch: 68 | Batch_idx: 320 |  Loss: (0.3656) | Acc: (88.08%) (36190/41088)\n",
            "Epoch: 68 | Batch_idx: 330 |  Loss: (0.3652) | Acc: (88.09%) (37324/42368)\n",
            "Epoch: 68 | Batch_idx: 340 |  Loss: (0.3649) | Acc: (88.10%) (38455/43648)\n",
            "Epoch: 68 | Batch_idx: 350 |  Loss: (0.3647) | Acc: (88.11%) (39587/44928)\n",
            "Epoch: 68 | Batch_idx: 360 |  Loss: (0.3660) | Acc: (88.09%) (40706/46208)\n",
            "Epoch: 68 | Batch_idx: 370 |  Loss: (0.3652) | Acc: (88.10%) (41837/47488)\n",
            "Epoch: 68 | Batch_idx: 380 |  Loss: (0.3648) | Acc: (88.10%) (42966/48768)\n",
            "Epoch: 68 | Batch_idx: 390 |  Loss: (0.3636) | Acc: (88.12%) (44058/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4118) | Acc: (87.16%) (8716/10000)\n",
            "Epoch: 69 | Batch_idx: 0 |  Loss: (0.2851) | Acc: (89.06%) (114/128)\n",
            "Epoch: 69 | Batch_idx: 10 |  Loss: (0.3396) | Acc: (89.70%) (1263/1408)\n",
            "Epoch: 69 | Batch_idx: 20 |  Loss: (0.3552) | Acc: (89.21%) (2398/2688)\n",
            "Epoch: 69 | Batch_idx: 30 |  Loss: (0.3590) | Acc: (88.86%) (3526/3968)\n",
            "Epoch: 69 | Batch_idx: 40 |  Loss: (0.3539) | Acc: (89.06%) (4674/5248)\n",
            "Epoch: 69 | Batch_idx: 50 |  Loss: (0.3445) | Acc: (89.28%) (5828/6528)\n",
            "Epoch: 69 | Batch_idx: 60 |  Loss: (0.3453) | Acc: (89.05%) (6953/7808)\n",
            "Epoch: 69 | Batch_idx: 70 |  Loss: (0.3454) | Acc: (88.96%) (8085/9088)\n",
            "Epoch: 69 | Batch_idx: 80 |  Loss: (0.3452) | Acc: (88.97%) (9224/10368)\n",
            "Epoch: 69 | Batch_idx: 90 |  Loss: (0.3418) | Acc: (89.06%) (10374/11648)\n",
            "Epoch: 69 | Batch_idx: 100 |  Loss: (0.3477) | Acc: (88.85%) (11486/12928)\n",
            "Epoch: 69 | Batch_idx: 110 |  Loss: (0.3525) | Acc: (88.63%) (12593/14208)\n",
            "Epoch: 69 | Batch_idx: 120 |  Loss: (0.3503) | Acc: (88.66%) (13732/15488)\n",
            "Epoch: 69 | Batch_idx: 130 |  Loss: (0.3561) | Acc: (88.50%) (14839/16768)\n",
            "Epoch: 69 | Batch_idx: 140 |  Loss: (0.3583) | Acc: (88.41%) (15957/18048)\n",
            "Epoch: 69 | Batch_idx: 150 |  Loss: (0.3554) | Acc: (88.49%) (17103/19328)\n",
            "Epoch: 69 | Batch_idx: 160 |  Loss: (0.3546) | Acc: (88.54%) (18246/20608)\n",
            "Epoch: 69 | Batch_idx: 170 |  Loss: (0.3537) | Acc: (88.56%) (19384/21888)\n",
            "Epoch: 69 | Batch_idx: 180 |  Loss: (0.3550) | Acc: (88.49%) (20502/23168)\n",
            "Epoch: 69 | Batch_idx: 190 |  Loss: (0.3556) | Acc: (88.45%) (21625/24448)\n",
            "Epoch: 69 | Batch_idx: 200 |  Loss: (0.3560) | Acc: (88.40%) (22743/25728)\n",
            "Epoch: 69 | Batch_idx: 210 |  Loss: (0.3570) | Acc: (88.36%) (23864/27008)\n",
            "Epoch: 69 | Batch_idx: 220 |  Loss: (0.3578) | Acc: (88.38%) (25001/28288)\n",
            "Epoch: 69 | Batch_idx: 230 |  Loss: (0.3610) | Acc: (88.25%) (26093/29568)\n",
            "Epoch: 69 | Batch_idx: 240 |  Loss: (0.3606) | Acc: (88.27%) (27230/30848)\n",
            "Epoch: 69 | Batch_idx: 250 |  Loss: (0.3613) | Acc: (88.27%) (28359/32128)\n",
            "Epoch: 69 | Batch_idx: 260 |  Loss: (0.3628) | Acc: (88.26%) (29487/33408)\n",
            "Epoch: 69 | Batch_idx: 270 |  Loss: (0.3640) | Acc: (88.25%) (30611/34688)\n",
            "Epoch: 69 | Batch_idx: 280 |  Loss: (0.3640) | Acc: (88.25%) (31743/35968)\n",
            "Epoch: 69 | Batch_idx: 290 |  Loss: (0.3644) | Acc: (88.23%) (32865/37248)\n",
            "Epoch: 69 | Batch_idx: 300 |  Loss: (0.3674) | Acc: (88.15%) (33962/38528)\n",
            "Epoch: 69 | Batch_idx: 310 |  Loss: (0.3668) | Acc: (88.13%) (35082/39808)\n",
            "Epoch: 69 | Batch_idx: 320 |  Loss: (0.3677) | Acc: (88.09%) (36194/41088)\n",
            "Epoch: 69 | Batch_idx: 330 |  Loss: (0.3668) | Acc: (88.10%) (37327/42368)\n",
            "Epoch: 69 | Batch_idx: 340 |  Loss: (0.3661) | Acc: (88.14%) (38470/43648)\n",
            "Epoch: 69 | Batch_idx: 350 |  Loss: (0.3658) | Acc: (88.13%) (39597/44928)\n",
            "Epoch: 69 | Batch_idx: 360 |  Loss: (0.3655) | Acc: (88.14%) (40730/46208)\n",
            "Epoch: 69 | Batch_idx: 370 |  Loss: (0.3662) | Acc: (88.15%) (41861/47488)\n",
            "Epoch: 69 | Batch_idx: 380 |  Loss: (0.3664) | Acc: (88.14%) (42986/48768)\n",
            "Epoch: 69 | Batch_idx: 390 |  Loss: (0.3660) | Acc: (88.15%) (44074/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4332) | Acc: (86.42%) (8642/10000)\n",
            "Epoch: 70 | Batch_idx: 0 |  Loss: (0.3887) | Acc: (88.28%) (113/128)\n",
            "Epoch: 70 | Batch_idx: 10 |  Loss: (0.3395) | Acc: (88.71%) (1249/1408)\n",
            "Epoch: 70 | Batch_idx: 20 |  Loss: (0.3416) | Acc: (88.36%) (2375/2688)\n",
            "Epoch: 70 | Batch_idx: 30 |  Loss: (0.3355) | Acc: (88.61%) (3516/3968)\n",
            "Epoch: 70 | Batch_idx: 40 |  Loss: (0.3415) | Acc: (88.43%) (4641/5248)\n",
            "Epoch: 70 | Batch_idx: 50 |  Loss: (0.3449) | Acc: (88.30%) (5764/6528)\n",
            "Epoch: 70 | Batch_idx: 60 |  Loss: (0.3461) | Acc: (88.23%) (6889/7808)\n",
            "Epoch: 70 | Batch_idx: 70 |  Loss: (0.3544) | Acc: (88.01%) (7998/9088)\n",
            "Epoch: 70 | Batch_idx: 80 |  Loss: (0.3501) | Acc: (88.24%) (9149/10368)\n",
            "Epoch: 70 | Batch_idx: 90 |  Loss: (0.3496) | Acc: (88.26%) (10281/11648)\n",
            "Epoch: 70 | Batch_idx: 100 |  Loss: (0.3522) | Acc: (88.23%) (11407/12928)\n",
            "Epoch: 70 | Batch_idx: 110 |  Loss: (0.3528) | Acc: (88.20%) (12532/14208)\n",
            "Epoch: 70 | Batch_idx: 120 |  Loss: (0.3531) | Acc: (88.26%) (13669/15488)\n",
            "Epoch: 70 | Batch_idx: 130 |  Loss: (0.3561) | Acc: (88.18%) (14786/16768)\n",
            "Epoch: 70 | Batch_idx: 140 |  Loss: (0.3572) | Acc: (88.15%) (15909/18048)\n",
            "Epoch: 70 | Batch_idx: 150 |  Loss: (0.3578) | Acc: (88.19%) (17046/19328)\n",
            "Epoch: 70 | Batch_idx: 160 |  Loss: (0.3562) | Acc: (88.30%) (18196/20608)\n",
            "Epoch: 70 | Batch_idx: 170 |  Loss: (0.3555) | Acc: (88.43%) (19356/21888)\n",
            "Epoch: 70 | Batch_idx: 180 |  Loss: (0.3565) | Acc: (88.41%) (20483/23168)\n",
            "Epoch: 70 | Batch_idx: 190 |  Loss: (0.3571) | Acc: (88.37%) (21605/24448)\n",
            "Epoch: 70 | Batch_idx: 200 |  Loss: (0.3563) | Acc: (88.36%) (22734/25728)\n",
            "Epoch: 70 | Batch_idx: 210 |  Loss: (0.3570) | Acc: (88.30%) (23849/27008)\n",
            "Epoch: 70 | Batch_idx: 220 |  Loss: (0.3580) | Acc: (88.27%) (24969/28288)\n",
            "Epoch: 70 | Batch_idx: 230 |  Loss: (0.3577) | Acc: (88.27%) (26099/29568)\n",
            "Epoch: 70 | Batch_idx: 240 |  Loss: (0.3561) | Acc: (88.29%) (27237/30848)\n",
            "Epoch: 70 | Batch_idx: 250 |  Loss: (0.3569) | Acc: (88.29%) (28365/32128)\n",
            "Epoch: 70 | Batch_idx: 260 |  Loss: (0.3552) | Acc: (88.36%) (29520/33408)\n",
            "Epoch: 70 | Batch_idx: 270 |  Loss: (0.3564) | Acc: (88.32%) (30638/34688)\n",
            "Epoch: 70 | Batch_idx: 280 |  Loss: (0.3565) | Acc: (88.33%) (31771/35968)\n",
            "Epoch: 70 | Batch_idx: 290 |  Loss: (0.3576) | Acc: (88.29%) (32887/37248)\n",
            "Epoch: 70 | Batch_idx: 300 |  Loss: (0.3574) | Acc: (88.33%) (34030/38528)\n",
            "Epoch: 70 | Batch_idx: 310 |  Loss: (0.3569) | Acc: (88.31%) (35153/39808)\n",
            "Epoch: 70 | Batch_idx: 320 |  Loss: (0.3568) | Acc: (88.29%) (36278/41088)\n",
            "Epoch: 70 | Batch_idx: 330 |  Loss: (0.3582) | Acc: (88.25%) (37390/42368)\n",
            "Epoch: 70 | Batch_idx: 340 |  Loss: (0.3577) | Acc: (88.26%) (38522/43648)\n",
            "Epoch: 70 | Batch_idx: 350 |  Loss: (0.3585) | Acc: (88.25%) (39647/44928)\n",
            "Epoch: 70 | Batch_idx: 360 |  Loss: (0.3578) | Acc: (88.29%) (40797/46208)\n",
            "Epoch: 70 | Batch_idx: 370 |  Loss: (0.3588) | Acc: (88.24%) (41905/47488)\n",
            "Epoch: 70 | Batch_idx: 380 |  Loss: (0.3584) | Acc: (88.26%) (43045/48768)\n",
            "Epoch: 70 | Batch_idx: 390 |  Loss: (0.3582) | Acc: (88.26%) (44131/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4372) | Acc: (85.98%) (8598/10000)\n",
            "Epoch: 71 | Batch_idx: 0 |  Loss: (0.3394) | Acc: (89.06%) (114/128)\n",
            "Epoch: 71 | Batch_idx: 10 |  Loss: (0.3042) | Acc: (89.28%) (1257/1408)\n",
            "Epoch: 71 | Batch_idx: 20 |  Loss: (0.3165) | Acc: (89.66%) (2410/2688)\n",
            "Epoch: 71 | Batch_idx: 30 |  Loss: (0.3305) | Acc: (88.94%) (3529/3968)\n",
            "Epoch: 71 | Batch_idx: 40 |  Loss: (0.3557) | Acc: (88.07%) (4622/5248)\n",
            "Epoch: 71 | Batch_idx: 50 |  Loss: (0.3461) | Acc: (88.50%) (5777/6528)\n",
            "Epoch: 71 | Batch_idx: 60 |  Loss: (0.3431) | Acc: (88.59%) (6917/7808)\n",
            "Epoch: 71 | Batch_idx: 70 |  Loss: (0.3432) | Acc: (88.61%) (8053/9088)\n",
            "Epoch: 71 | Batch_idx: 80 |  Loss: (0.3474) | Acc: (88.51%) (9177/10368)\n",
            "Epoch: 71 | Batch_idx: 90 |  Loss: (0.3470) | Acc: (88.56%) (10315/11648)\n",
            "Epoch: 71 | Batch_idx: 100 |  Loss: (0.3459) | Acc: (88.58%) (11452/12928)\n",
            "Epoch: 71 | Batch_idx: 110 |  Loss: (0.3470) | Acc: (88.59%) (12587/14208)\n",
            "Epoch: 71 | Batch_idx: 120 |  Loss: (0.3491) | Acc: (88.49%) (13706/15488)\n",
            "Epoch: 71 | Batch_idx: 130 |  Loss: (0.3538) | Acc: (88.42%) (14827/16768)\n",
            "Epoch: 71 | Batch_idx: 140 |  Loss: (0.3550) | Acc: (88.39%) (15953/18048)\n",
            "Epoch: 71 | Batch_idx: 150 |  Loss: (0.3544) | Acc: (88.44%) (17094/19328)\n",
            "Epoch: 71 | Batch_idx: 160 |  Loss: (0.3538) | Acc: (88.46%) (18229/20608)\n",
            "Epoch: 71 | Batch_idx: 170 |  Loss: (0.3533) | Acc: (88.50%) (19370/21888)\n",
            "Epoch: 71 | Batch_idx: 180 |  Loss: (0.3526) | Acc: (88.55%) (20516/23168)\n",
            "Epoch: 71 | Batch_idx: 190 |  Loss: (0.3525) | Acc: (88.60%) (21660/24448)\n",
            "Epoch: 71 | Batch_idx: 200 |  Loss: (0.3560) | Acc: (88.50%) (22770/25728)\n",
            "Epoch: 71 | Batch_idx: 210 |  Loss: (0.3590) | Acc: (88.40%) (23876/27008)\n",
            "Epoch: 71 | Batch_idx: 220 |  Loss: (0.3616) | Acc: (88.33%) (24988/28288)\n",
            "Epoch: 71 | Batch_idx: 230 |  Loss: (0.3614) | Acc: (88.37%) (26130/29568)\n",
            "Epoch: 71 | Batch_idx: 240 |  Loss: (0.3617) | Acc: (88.38%) (27264/30848)\n",
            "Epoch: 71 | Batch_idx: 250 |  Loss: (0.3625) | Acc: (88.33%) (28379/32128)\n",
            "Epoch: 71 | Batch_idx: 260 |  Loss: (0.3623) | Acc: (88.31%) (29502/33408)\n",
            "Epoch: 71 | Batch_idx: 270 |  Loss: (0.3627) | Acc: (88.30%) (30628/34688)\n",
            "Epoch: 71 | Batch_idx: 280 |  Loss: (0.3628) | Acc: (88.30%) (31758/35968)\n",
            "Epoch: 71 | Batch_idx: 290 |  Loss: (0.3636) | Acc: (88.27%) (32877/37248)\n",
            "Epoch: 71 | Batch_idx: 300 |  Loss: (0.3636) | Acc: (88.27%) (34007/38528)\n",
            "Epoch: 71 | Batch_idx: 310 |  Loss: (0.3626) | Acc: (88.31%) (35154/39808)\n",
            "Epoch: 71 | Batch_idx: 320 |  Loss: (0.3620) | Acc: (88.33%) (36291/41088)\n",
            "Epoch: 71 | Batch_idx: 330 |  Loss: (0.3623) | Acc: (88.31%) (37415/42368)\n",
            "Epoch: 71 | Batch_idx: 340 |  Loss: (0.3614) | Acc: (88.35%) (38563/43648)\n",
            "Epoch: 71 | Batch_idx: 350 |  Loss: (0.3600) | Acc: (88.39%) (39714/44928)\n",
            "Epoch: 71 | Batch_idx: 360 |  Loss: (0.3613) | Acc: (88.35%) (40827/46208)\n",
            "Epoch: 71 | Batch_idx: 370 |  Loss: (0.3609) | Acc: (88.37%) (41965/47488)\n",
            "Epoch: 71 | Batch_idx: 380 |  Loss: (0.3601) | Acc: (88.39%) (43105/48768)\n",
            "Epoch: 71 | Batch_idx: 390 |  Loss: (0.3606) | Acc: (88.39%) (44195/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4456) | Acc: (85.36%) (8536/10000)\n",
            "Epoch: 72 | Batch_idx: 0 |  Loss: (0.4381) | Acc: (84.38%) (108/128)\n",
            "Epoch: 72 | Batch_idx: 10 |  Loss: (0.3785) | Acc: (88.35%) (1244/1408)\n",
            "Epoch: 72 | Batch_idx: 20 |  Loss: (0.3726) | Acc: (88.65%) (2383/2688)\n",
            "Epoch: 72 | Batch_idx: 30 |  Loss: (0.3769) | Acc: (88.48%) (3511/3968)\n",
            "Epoch: 72 | Batch_idx: 40 |  Loss: (0.3600) | Acc: (88.97%) (4669/5248)\n",
            "Epoch: 72 | Batch_idx: 50 |  Loss: (0.3581) | Acc: (89.02%) (5811/6528)\n",
            "Epoch: 72 | Batch_idx: 60 |  Loss: (0.3503) | Acc: (89.24%) (6968/7808)\n",
            "Epoch: 72 | Batch_idx: 70 |  Loss: (0.3533) | Acc: (89.11%) (8098/9088)\n",
            "Epoch: 72 | Batch_idx: 80 |  Loss: (0.3552) | Acc: (89.02%) (9230/10368)\n",
            "Epoch: 72 | Batch_idx: 90 |  Loss: (0.3562) | Acc: (88.91%) (10356/11648)\n",
            "Epoch: 72 | Batch_idx: 100 |  Loss: (0.3538) | Acc: (88.88%) (11490/12928)\n",
            "Epoch: 72 | Batch_idx: 110 |  Loss: (0.3549) | Acc: (88.80%) (12616/14208)\n",
            "Epoch: 72 | Batch_idx: 120 |  Loss: (0.3569) | Acc: (88.68%) (13734/15488)\n",
            "Epoch: 72 | Batch_idx: 130 |  Loss: (0.3562) | Acc: (88.62%) (14860/16768)\n",
            "Epoch: 72 | Batch_idx: 140 |  Loss: (0.3560) | Acc: (88.56%) (15984/18048)\n",
            "Epoch: 72 | Batch_idx: 150 |  Loss: (0.3552) | Acc: (88.53%) (17112/19328)\n",
            "Epoch: 72 | Batch_idx: 160 |  Loss: (0.3530) | Acc: (88.58%) (18254/20608)\n",
            "Epoch: 72 | Batch_idx: 170 |  Loss: (0.3533) | Acc: (88.59%) (19391/21888)\n",
            "Epoch: 72 | Batch_idx: 180 |  Loss: (0.3526) | Acc: (88.65%) (20538/23168)\n",
            "Epoch: 72 | Batch_idx: 190 |  Loss: (0.3512) | Acc: (88.69%) (21684/24448)\n",
            "Epoch: 72 | Batch_idx: 200 |  Loss: (0.3544) | Acc: (88.58%) (22789/25728)\n",
            "Epoch: 72 | Batch_idx: 210 |  Loss: (0.3554) | Acc: (88.51%) (23906/27008)\n",
            "Epoch: 72 | Batch_idx: 220 |  Loss: (0.3559) | Acc: (88.50%) (25035/28288)\n",
            "Epoch: 72 | Batch_idx: 230 |  Loss: (0.3565) | Acc: (88.50%) (26169/29568)\n",
            "Epoch: 72 | Batch_idx: 240 |  Loss: (0.3550) | Acc: (88.56%) (27318/30848)\n",
            "Epoch: 72 | Batch_idx: 250 |  Loss: (0.3556) | Acc: (88.51%) (28435/32128)\n",
            "Epoch: 72 | Batch_idx: 260 |  Loss: (0.3551) | Acc: (88.51%) (29571/33408)\n",
            "Epoch: 72 | Batch_idx: 270 |  Loss: (0.3555) | Acc: (88.52%) (30705/34688)\n",
            "Epoch: 72 | Batch_idx: 280 |  Loss: (0.3555) | Acc: (88.50%) (31830/35968)\n",
            "Epoch: 72 | Batch_idx: 290 |  Loss: (0.3534) | Acc: (88.57%) (32989/37248)\n",
            "Epoch: 72 | Batch_idx: 300 |  Loss: (0.3536) | Acc: (88.57%) (34123/38528)\n",
            "Epoch: 72 | Batch_idx: 310 |  Loss: (0.3525) | Acc: (88.58%) (35260/39808)\n",
            "Epoch: 72 | Batch_idx: 320 |  Loss: (0.3524) | Acc: (88.53%) (36374/41088)\n",
            "Epoch: 72 | Batch_idx: 330 |  Loss: (0.3530) | Acc: (88.52%) (37506/42368)\n",
            "Epoch: 72 | Batch_idx: 340 |  Loss: (0.3537) | Acc: (88.49%) (38625/43648)\n",
            "Epoch: 72 | Batch_idx: 350 |  Loss: (0.3538) | Acc: (88.47%) (39750/44928)\n",
            "Epoch: 72 | Batch_idx: 360 |  Loss: (0.3544) | Acc: (88.46%) (40877/46208)\n",
            "Epoch: 72 | Batch_idx: 370 |  Loss: (0.3529) | Acc: (88.51%) (42031/47488)\n",
            "Epoch: 72 | Batch_idx: 380 |  Loss: (0.3543) | Acc: (88.46%) (43140/48768)\n",
            "Epoch: 72 | Batch_idx: 390 |  Loss: (0.3547) | Acc: (88.48%) (44240/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4160) | Acc: (86.81%) (8681/10000)\n",
            "Epoch: 73 | Batch_idx: 0 |  Loss: (0.3607) | Acc: (89.06%) (114/128)\n",
            "Epoch: 73 | Batch_idx: 10 |  Loss: (0.3353) | Acc: (88.99%) (1253/1408)\n",
            "Epoch: 73 | Batch_idx: 20 |  Loss: (0.3251) | Acc: (89.55%) (2407/2688)\n",
            "Epoch: 73 | Batch_idx: 30 |  Loss: (0.3281) | Acc: (89.54%) (3553/3968)\n",
            "Epoch: 73 | Batch_idx: 40 |  Loss: (0.3314) | Acc: (89.37%) (4690/5248)\n",
            "Epoch: 73 | Batch_idx: 50 |  Loss: (0.3386) | Acc: (89.22%) (5824/6528)\n",
            "Epoch: 73 | Batch_idx: 60 |  Loss: (0.3401) | Acc: (89.16%) (6962/7808)\n",
            "Epoch: 73 | Batch_idx: 70 |  Loss: (0.3446) | Acc: (89.06%) (8094/9088)\n",
            "Epoch: 73 | Batch_idx: 80 |  Loss: (0.3461) | Acc: (88.89%) (9216/10368)\n",
            "Epoch: 73 | Batch_idx: 90 |  Loss: (0.3439) | Acc: (88.93%) (10358/11648)\n",
            "Epoch: 73 | Batch_idx: 100 |  Loss: (0.3408) | Acc: (88.92%) (11496/12928)\n",
            "Epoch: 73 | Batch_idx: 110 |  Loss: (0.3379) | Acc: (88.92%) (12634/14208)\n",
            "Epoch: 73 | Batch_idx: 120 |  Loss: (0.3382) | Acc: (88.89%) (13768/15488)\n",
            "Epoch: 73 | Batch_idx: 130 |  Loss: (0.3389) | Acc: (88.92%) (14910/16768)\n",
            "Epoch: 73 | Batch_idx: 140 |  Loss: (0.3403) | Acc: (88.86%) (16038/18048)\n",
            "Epoch: 73 | Batch_idx: 150 |  Loss: (0.3401) | Acc: (88.90%) (17182/19328)\n",
            "Epoch: 73 | Batch_idx: 160 |  Loss: (0.3418) | Acc: (88.83%) (18307/20608)\n",
            "Epoch: 73 | Batch_idx: 170 |  Loss: (0.3423) | Acc: (88.82%) (19441/21888)\n",
            "Epoch: 73 | Batch_idx: 180 |  Loss: (0.3425) | Acc: (88.78%) (20568/23168)\n",
            "Epoch: 73 | Batch_idx: 190 |  Loss: (0.3418) | Acc: (88.79%) (21707/24448)\n",
            "Epoch: 73 | Batch_idx: 200 |  Loss: (0.3430) | Acc: (88.75%) (22833/25728)\n",
            "Epoch: 73 | Batch_idx: 210 |  Loss: (0.3432) | Acc: (88.70%) (23955/27008)\n",
            "Epoch: 73 | Batch_idx: 220 |  Loss: (0.3423) | Acc: (88.68%) (25087/28288)\n",
            "Epoch: 73 | Batch_idx: 230 |  Loss: (0.3419) | Acc: (88.69%) (26223/29568)\n",
            "Epoch: 73 | Batch_idx: 240 |  Loss: (0.3435) | Acc: (88.65%) (27348/30848)\n",
            "Epoch: 73 | Batch_idx: 250 |  Loss: (0.3445) | Acc: (88.59%) (28461/32128)\n",
            "Epoch: 73 | Batch_idx: 260 |  Loss: (0.3443) | Acc: (88.61%) (29604/33408)\n",
            "Epoch: 73 | Batch_idx: 270 |  Loss: (0.3461) | Acc: (88.57%) (30723/34688)\n",
            "Epoch: 73 | Batch_idx: 280 |  Loss: (0.3453) | Acc: (88.59%) (31864/35968)\n",
            "Epoch: 73 | Batch_idx: 290 |  Loss: (0.3447) | Acc: (88.60%) (33002/37248)\n",
            "Epoch: 73 | Batch_idx: 300 |  Loss: (0.3459) | Acc: (88.59%) (34131/38528)\n",
            "Epoch: 73 | Batch_idx: 310 |  Loss: (0.3467) | Acc: (88.60%) (35269/39808)\n",
            "Epoch: 73 | Batch_idx: 320 |  Loss: (0.3494) | Acc: (88.51%) (36369/41088)\n",
            "Epoch: 73 | Batch_idx: 330 |  Loss: (0.3492) | Acc: (88.51%) (37501/42368)\n",
            "Epoch: 73 | Batch_idx: 340 |  Loss: (0.3495) | Acc: (88.51%) (38635/43648)\n",
            "Epoch: 73 | Batch_idx: 350 |  Loss: (0.3500) | Acc: (88.50%) (39762/44928)\n",
            "Epoch: 73 | Batch_idx: 360 |  Loss: (0.3499) | Acc: (88.48%) (40886/46208)\n",
            "Epoch: 73 | Batch_idx: 370 |  Loss: (0.3502) | Acc: (88.48%) (42017/47488)\n",
            "Epoch: 73 | Batch_idx: 380 |  Loss: (0.3505) | Acc: (88.47%) (43143/48768)\n",
            "Epoch: 73 | Batch_idx: 390 |  Loss: (0.3517) | Acc: (88.43%) (44217/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.5120) | Acc: (83.24%) (8324/10000)\n",
            "Epoch: 74 | Batch_idx: 0 |  Loss: (0.4302) | Acc: (86.72%) (111/128)\n",
            "Epoch: 74 | Batch_idx: 10 |  Loss: (0.3253) | Acc: (89.49%) (1260/1408)\n",
            "Epoch: 74 | Batch_idx: 20 |  Loss: (0.3221) | Acc: (89.36%) (2402/2688)\n",
            "Epoch: 74 | Batch_idx: 30 |  Loss: (0.3285) | Acc: (89.26%) (3542/3968)\n",
            "Epoch: 74 | Batch_idx: 40 |  Loss: (0.3270) | Acc: (89.25%) (4684/5248)\n",
            "Epoch: 74 | Batch_idx: 50 |  Loss: (0.3321) | Acc: (89.08%) (5815/6528)\n",
            "Epoch: 74 | Batch_idx: 60 |  Loss: (0.3363) | Acc: (88.96%) (6946/7808)\n",
            "Epoch: 74 | Batch_idx: 70 |  Loss: (0.3354) | Acc: (88.88%) (8077/9088)\n",
            "Epoch: 74 | Batch_idx: 80 |  Loss: (0.3376) | Acc: (88.83%) (9210/10368)\n",
            "Epoch: 74 | Batch_idx: 90 |  Loss: (0.3386) | Acc: (88.74%) (10337/11648)\n",
            "Epoch: 74 | Batch_idx: 100 |  Loss: (0.3397) | Acc: (88.54%) (11447/12928)\n",
            "Epoch: 74 | Batch_idx: 110 |  Loss: (0.3421) | Acc: (88.50%) (12574/14208)\n",
            "Epoch: 74 | Batch_idx: 120 |  Loss: (0.3454) | Acc: (88.42%) (13694/15488)\n",
            "Epoch: 74 | Batch_idx: 130 |  Loss: (0.3444) | Acc: (88.40%) (14823/16768)\n",
            "Epoch: 74 | Batch_idx: 140 |  Loss: (0.3438) | Acc: (88.49%) (15971/18048)\n",
            "Epoch: 74 | Batch_idx: 150 |  Loss: (0.3436) | Acc: (88.51%) (17108/19328)\n",
            "Epoch: 74 | Batch_idx: 160 |  Loss: (0.3415) | Acc: (88.60%) (18259/20608)\n",
            "Epoch: 74 | Batch_idx: 170 |  Loss: (0.3423) | Acc: (88.57%) (19387/21888)\n",
            "Epoch: 74 | Batch_idx: 180 |  Loss: (0.3435) | Acc: (88.57%) (20519/23168)\n",
            "Epoch: 74 | Batch_idx: 190 |  Loss: (0.3426) | Acc: (88.60%) (21662/24448)\n",
            "Epoch: 74 | Batch_idx: 200 |  Loss: (0.3430) | Acc: (88.61%) (22797/25728)\n",
            "Epoch: 74 | Batch_idx: 210 |  Loss: (0.3450) | Acc: (88.52%) (23908/27008)\n",
            "Epoch: 74 | Batch_idx: 220 |  Loss: (0.3466) | Acc: (88.49%) (25032/28288)\n",
            "Epoch: 74 | Batch_idx: 230 |  Loss: (0.3468) | Acc: (88.52%) (26173/29568)\n",
            "Epoch: 74 | Batch_idx: 240 |  Loss: (0.3472) | Acc: (88.50%) (27300/30848)\n",
            "Epoch: 74 | Batch_idx: 250 |  Loss: (0.3492) | Acc: (88.42%) (28409/32128)\n",
            "Epoch: 74 | Batch_idx: 260 |  Loss: (0.3485) | Acc: (88.47%) (29556/33408)\n",
            "Epoch: 74 | Batch_idx: 270 |  Loss: (0.3490) | Acc: (88.48%) (30692/34688)\n",
            "Epoch: 74 | Batch_idx: 280 |  Loss: (0.3481) | Acc: (88.51%) (31835/35968)\n",
            "Epoch: 74 | Batch_idx: 290 |  Loss: (0.3490) | Acc: (88.47%) (32955/37248)\n",
            "Epoch: 74 | Batch_idx: 300 |  Loss: (0.3501) | Acc: (88.45%) (34078/38528)\n",
            "Epoch: 74 | Batch_idx: 310 |  Loss: (0.3515) | Acc: (88.44%) (35205/39808)\n",
            "Epoch: 74 | Batch_idx: 320 |  Loss: (0.3521) | Acc: (88.43%) (36335/41088)\n",
            "Epoch: 74 | Batch_idx: 330 |  Loss: (0.3510) | Acc: (88.46%) (37480/42368)\n",
            "Epoch: 74 | Batch_idx: 340 |  Loss: (0.3503) | Acc: (88.49%) (38624/43648)\n",
            "Epoch: 74 | Batch_idx: 350 |  Loss: (0.3514) | Acc: (88.48%) (39753/44928)\n",
            "Epoch: 74 | Batch_idx: 360 |  Loss: (0.3513) | Acc: (88.48%) (40885/46208)\n",
            "Epoch: 74 | Batch_idx: 370 |  Loss: (0.3520) | Acc: (88.47%) (42011/47488)\n",
            "Epoch: 74 | Batch_idx: 380 |  Loss: (0.3537) | Acc: (88.41%) (43118/48768)\n",
            "Epoch: 74 | Batch_idx: 390 |  Loss: (0.3531) | Acc: (88.44%) (44219/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4007) | Acc: (87.12%) (8712/10000)\n",
            "Epoch: 75 | Batch_idx: 0 |  Loss: (0.4539) | Acc: (87.50%) (112/128)\n",
            "Epoch: 75 | Batch_idx: 10 |  Loss: (0.3079) | Acc: (89.63%) (1262/1408)\n",
            "Epoch: 75 | Batch_idx: 20 |  Loss: (0.3320) | Acc: (89.40%) (2403/2688)\n",
            "Epoch: 75 | Batch_idx: 30 |  Loss: (0.3422) | Acc: (88.89%) (3527/3968)\n",
            "Epoch: 75 | Batch_idx: 40 |  Loss: (0.3459) | Acc: (88.93%) (4667/5248)\n",
            "Epoch: 75 | Batch_idx: 50 |  Loss: (0.3485) | Acc: (88.77%) (5795/6528)\n",
            "Epoch: 75 | Batch_idx: 60 |  Loss: (0.3534) | Acc: (88.68%) (6924/7808)\n",
            "Epoch: 75 | Batch_idx: 70 |  Loss: (0.3554) | Acc: (88.69%) (8060/9088)\n",
            "Epoch: 75 | Batch_idx: 80 |  Loss: (0.3567) | Acc: (88.55%) (9181/10368)\n",
            "Epoch: 75 | Batch_idx: 90 |  Loss: (0.3580) | Acc: (88.50%) (10308/11648)\n",
            "Epoch: 75 | Batch_idx: 100 |  Loss: (0.3583) | Acc: (88.48%) (11439/12928)\n",
            "Epoch: 75 | Batch_idx: 110 |  Loss: (0.3568) | Acc: (88.63%) (12592/14208)\n",
            "Epoch: 75 | Batch_idx: 120 |  Loss: (0.3537) | Acc: (88.70%) (13738/15488)\n",
            "Epoch: 75 | Batch_idx: 130 |  Loss: (0.3513) | Acc: (88.69%) (14871/16768)\n",
            "Epoch: 75 | Batch_idx: 140 |  Loss: (0.3521) | Acc: (88.67%) (16004/18048)\n",
            "Epoch: 75 | Batch_idx: 150 |  Loss: (0.3496) | Acc: (88.74%) (17152/19328)\n",
            "Epoch: 75 | Batch_idx: 160 |  Loss: (0.3499) | Acc: (88.73%) (18285/20608)\n",
            "Epoch: 75 | Batch_idx: 170 |  Loss: (0.3500) | Acc: (88.78%) (19433/21888)\n",
            "Epoch: 75 | Batch_idx: 180 |  Loss: (0.3519) | Acc: (88.68%) (20546/23168)\n",
            "Epoch: 75 | Batch_idx: 190 |  Loss: (0.3508) | Acc: (88.71%) (21688/24448)\n",
            "Epoch: 75 | Batch_idx: 200 |  Loss: (0.3512) | Acc: (88.74%) (22830/25728)\n",
            "Epoch: 75 | Batch_idx: 210 |  Loss: (0.3519) | Acc: (88.71%) (23959/27008)\n",
            "Epoch: 75 | Batch_idx: 220 |  Loss: (0.3541) | Acc: (88.66%) (25079/28288)\n",
            "Epoch: 75 | Batch_idx: 230 |  Loss: (0.3539) | Acc: (88.62%) (26204/29568)\n",
            "Epoch: 75 | Batch_idx: 240 |  Loss: (0.3521) | Acc: (88.68%) (27357/30848)\n",
            "Epoch: 75 | Batch_idx: 250 |  Loss: (0.3530) | Acc: (88.63%) (28476/32128)\n",
            "Epoch: 75 | Batch_idx: 260 |  Loss: (0.3532) | Acc: (88.63%) (29609/33408)\n",
            "Epoch: 75 | Batch_idx: 270 |  Loss: (0.3538) | Acc: (88.61%) (30736/34688)\n",
            "Epoch: 75 | Batch_idx: 280 |  Loss: (0.3527) | Acc: (88.68%) (31895/35968)\n",
            "Epoch: 75 | Batch_idx: 290 |  Loss: (0.3511) | Acc: (88.71%) (33041/37248)\n",
            "Epoch: 75 | Batch_idx: 300 |  Loss: (0.3504) | Acc: (88.74%) (34188/38528)\n",
            "Epoch: 75 | Batch_idx: 310 |  Loss: (0.3500) | Acc: (88.76%) (35332/39808)\n",
            "Epoch: 75 | Batch_idx: 320 |  Loss: (0.3492) | Acc: (88.76%) (36471/41088)\n",
            "Epoch: 75 | Batch_idx: 330 |  Loss: (0.3499) | Acc: (88.75%) (37601/42368)\n",
            "Epoch: 75 | Batch_idx: 340 |  Loss: (0.3487) | Acc: (88.78%) (38749/43648)\n",
            "Epoch: 75 | Batch_idx: 350 |  Loss: (0.3493) | Acc: (88.74%) (39868/44928)\n",
            "Epoch: 75 | Batch_idx: 360 |  Loss: (0.3492) | Acc: (88.77%) (41019/46208)\n",
            "Epoch: 75 | Batch_idx: 370 |  Loss: (0.3485) | Acc: (88.80%) (42170/47488)\n",
            "Epoch: 75 | Batch_idx: 380 |  Loss: (0.3492) | Acc: (88.79%) (43301/48768)\n",
            "Epoch: 75 | Batch_idx: 390 |  Loss: (0.3507) | Acc: (88.76%) (44382/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4483) | Acc: (85.89%) (8589/10000)\n",
            "Epoch: 76 | Batch_idx: 0 |  Loss: (0.2987) | Acc: (90.62%) (116/128)\n",
            "Epoch: 76 | Batch_idx: 10 |  Loss: (0.3961) | Acc: (87.29%) (1229/1408)\n",
            "Epoch: 76 | Batch_idx: 20 |  Loss: (0.3680) | Acc: (88.47%) (2378/2688)\n",
            "Epoch: 76 | Batch_idx: 30 |  Loss: (0.3670) | Acc: (88.05%) (3494/3968)\n",
            "Epoch: 76 | Batch_idx: 40 |  Loss: (0.3646) | Acc: (88.03%) (4620/5248)\n",
            "Epoch: 76 | Batch_idx: 50 |  Loss: (0.3485) | Acc: (88.56%) (5781/6528)\n",
            "Epoch: 76 | Batch_idx: 60 |  Loss: (0.3446) | Acc: (88.70%) (6926/7808)\n",
            "Epoch: 76 | Batch_idx: 70 |  Loss: (0.3495) | Acc: (88.67%) (8058/9088)\n",
            "Epoch: 76 | Batch_idx: 80 |  Loss: (0.3501) | Acc: (88.67%) (9193/10368)\n",
            "Epoch: 76 | Batch_idx: 90 |  Loss: (0.3546) | Acc: (88.42%) (10299/11648)\n",
            "Epoch: 76 | Batch_idx: 100 |  Loss: (0.3540) | Acc: (88.43%) (11432/12928)\n",
            "Epoch: 76 | Batch_idx: 110 |  Loss: (0.3525) | Acc: (88.50%) (12574/14208)\n",
            "Epoch: 76 | Batch_idx: 120 |  Loss: (0.3512) | Acc: (88.57%) (13718/15488)\n",
            "Epoch: 76 | Batch_idx: 130 |  Loss: (0.3488) | Acc: (88.66%) (14866/16768)\n",
            "Epoch: 76 | Batch_idx: 140 |  Loss: (0.3475) | Acc: (88.62%) (15995/18048)\n",
            "Epoch: 76 | Batch_idx: 150 |  Loss: (0.3496) | Acc: (88.58%) (17121/19328)\n",
            "Epoch: 76 | Batch_idx: 160 |  Loss: (0.3498) | Acc: (88.57%) (18253/20608)\n",
            "Epoch: 76 | Batch_idx: 170 |  Loss: (0.3509) | Acc: (88.54%) (19379/21888)\n",
            "Epoch: 76 | Batch_idx: 180 |  Loss: (0.3493) | Acc: (88.58%) (20522/23168)\n",
            "Epoch: 76 | Batch_idx: 190 |  Loss: (0.3474) | Acc: (88.64%) (21670/24448)\n",
            "Epoch: 76 | Batch_idx: 200 |  Loss: (0.3500) | Acc: (88.55%) (22781/25728)\n",
            "Epoch: 76 | Batch_idx: 210 |  Loss: (0.3508) | Acc: (88.51%) (23904/27008)\n",
            "Epoch: 76 | Batch_idx: 220 |  Loss: (0.3530) | Acc: (88.46%) (25024/28288)\n",
            "Epoch: 76 | Batch_idx: 230 |  Loss: (0.3534) | Acc: (88.41%) (26142/29568)\n",
            "Epoch: 76 | Batch_idx: 240 |  Loss: (0.3520) | Acc: (88.46%) (27287/30848)\n",
            "Epoch: 76 | Batch_idx: 250 |  Loss: (0.3542) | Acc: (88.40%) (28402/32128)\n",
            "Epoch: 76 | Batch_idx: 260 |  Loss: (0.3558) | Acc: (88.31%) (29504/33408)\n",
            "Epoch: 76 | Batch_idx: 270 |  Loss: (0.3552) | Acc: (88.34%) (30643/34688)\n",
            "Epoch: 76 | Batch_idx: 280 |  Loss: (0.3562) | Acc: (88.29%) (31755/35968)\n",
            "Epoch: 76 | Batch_idx: 290 |  Loss: (0.3564) | Acc: (88.28%) (32883/37248)\n",
            "Epoch: 76 | Batch_idx: 300 |  Loss: (0.3557) | Acc: (88.29%) (34018/38528)\n",
            "Epoch: 76 | Batch_idx: 310 |  Loss: (0.3566) | Acc: (88.25%) (35131/39808)\n",
            "Epoch: 76 | Batch_idx: 320 |  Loss: (0.3577) | Acc: (88.24%) (36255/41088)\n",
            "Epoch: 76 | Batch_idx: 330 |  Loss: (0.3584) | Acc: (88.24%) (37385/42368)\n",
            "Epoch: 76 | Batch_idx: 340 |  Loss: (0.3589) | Acc: (88.21%) (38503/43648)\n",
            "Epoch: 76 | Batch_idx: 350 |  Loss: (0.3581) | Acc: (88.25%) (39648/44928)\n",
            "Epoch: 76 | Batch_idx: 360 |  Loss: (0.3570) | Acc: (88.29%) (40799/46208)\n",
            "Epoch: 76 | Batch_idx: 370 |  Loss: (0.3571) | Acc: (88.30%) (41931/47488)\n",
            "Epoch: 76 | Batch_idx: 380 |  Loss: (0.3562) | Acc: (88.33%) (43075/48768)\n",
            "Epoch: 76 | Batch_idx: 390 |  Loss: (0.3562) | Acc: (88.32%) (44159/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4623) | Acc: (85.16%) (8516/10000)\n",
            "Epoch: 77 | Batch_idx: 0 |  Loss: (0.3450) | Acc: (89.06%) (114/128)\n",
            "Epoch: 77 | Batch_idx: 10 |  Loss: (0.3421) | Acc: (89.49%) (1260/1408)\n",
            "Epoch: 77 | Batch_idx: 20 |  Loss: (0.3292) | Acc: (89.73%) (2412/2688)\n",
            "Epoch: 77 | Batch_idx: 30 |  Loss: (0.3251) | Acc: (89.77%) (3562/3968)\n",
            "Epoch: 77 | Batch_idx: 40 |  Loss: (0.3255) | Acc: (89.67%) (4706/5248)\n",
            "Epoch: 77 | Batch_idx: 50 |  Loss: (0.3370) | Acc: (89.29%) (5829/6528)\n",
            "Epoch: 77 | Batch_idx: 60 |  Loss: (0.3339) | Acc: (89.29%) (6972/7808)\n",
            "Epoch: 77 | Batch_idx: 70 |  Loss: (0.3368) | Acc: (89.22%) (8108/9088)\n",
            "Epoch: 77 | Batch_idx: 80 |  Loss: (0.3385) | Acc: (89.17%) (9245/10368)\n",
            "Epoch: 77 | Batch_idx: 90 |  Loss: (0.3357) | Acc: (89.24%) (10395/11648)\n",
            "Epoch: 77 | Batch_idx: 100 |  Loss: (0.3335) | Acc: (89.34%) (11550/12928)\n",
            "Epoch: 77 | Batch_idx: 110 |  Loss: (0.3357) | Acc: (89.20%) (12673/14208)\n",
            "Epoch: 77 | Batch_idx: 120 |  Loss: (0.3343) | Acc: (89.22%) (13819/15488)\n",
            "Epoch: 77 | Batch_idx: 130 |  Loss: (0.3357) | Acc: (89.20%) (14957/16768)\n",
            "Epoch: 77 | Batch_idx: 140 |  Loss: (0.3393) | Acc: (89.07%) (16076/18048)\n",
            "Epoch: 77 | Batch_idx: 150 |  Loss: (0.3407) | Acc: (89.11%) (17223/19328)\n",
            "Epoch: 77 | Batch_idx: 160 |  Loss: (0.3412) | Acc: (89.12%) (18365/20608)\n",
            "Epoch: 77 | Batch_idx: 170 |  Loss: (0.3429) | Acc: (89.10%) (19502/21888)\n",
            "Epoch: 77 | Batch_idx: 180 |  Loss: (0.3434) | Acc: (89.10%) (20642/23168)\n",
            "Epoch: 77 | Batch_idx: 190 |  Loss: (0.3448) | Acc: (89.03%) (21766/24448)\n",
            "Epoch: 77 | Batch_idx: 200 |  Loss: (0.3457) | Acc: (88.97%) (22889/25728)\n",
            "Epoch: 77 | Batch_idx: 210 |  Loss: (0.3472) | Acc: (88.93%) (24019/27008)\n",
            "Epoch: 77 | Batch_idx: 220 |  Loss: (0.3499) | Acc: (88.89%) (25145/28288)\n",
            "Epoch: 77 | Batch_idx: 230 |  Loss: (0.3494) | Acc: (88.88%) (26280/29568)\n",
            "Epoch: 77 | Batch_idx: 240 |  Loss: (0.3492) | Acc: (88.91%) (27428/30848)\n",
            "Epoch: 77 | Batch_idx: 250 |  Loss: (0.3488) | Acc: (88.90%) (28562/32128)\n",
            "Epoch: 77 | Batch_idx: 260 |  Loss: (0.3475) | Acc: (88.93%) (29709/33408)\n",
            "Epoch: 77 | Batch_idx: 270 |  Loss: (0.3466) | Acc: (88.93%) (30849/34688)\n",
            "Epoch: 77 | Batch_idx: 280 |  Loss: (0.3461) | Acc: (88.90%) (31975/35968)\n",
            "Epoch: 77 | Batch_idx: 290 |  Loss: (0.3462) | Acc: (88.89%) (33111/37248)\n",
            "Epoch: 77 | Batch_idx: 300 |  Loss: (0.3456) | Acc: (88.88%) (34244/38528)\n",
            "Epoch: 77 | Batch_idx: 310 |  Loss: (0.3462) | Acc: (88.85%) (35369/39808)\n",
            "Epoch: 77 | Batch_idx: 320 |  Loss: (0.3464) | Acc: (88.83%) (36497/41088)\n",
            "Epoch: 77 | Batch_idx: 330 |  Loss: (0.3468) | Acc: (88.78%) (37616/42368)\n",
            "Epoch: 77 | Batch_idx: 340 |  Loss: (0.3476) | Acc: (88.77%) (38746/43648)\n",
            "Epoch: 77 | Batch_idx: 350 |  Loss: (0.3486) | Acc: (88.72%) (39861/44928)\n",
            "Epoch: 77 | Batch_idx: 360 |  Loss: (0.3484) | Acc: (88.72%) (40998/46208)\n",
            "Epoch: 77 | Batch_idx: 370 |  Loss: (0.3493) | Acc: (88.70%) (42120/47488)\n",
            "Epoch: 77 | Batch_idx: 380 |  Loss: (0.3495) | Acc: (88.69%) (43253/48768)\n",
            "Epoch: 77 | Batch_idx: 390 |  Loss: (0.3490) | Acc: (88.71%) (44356/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3918) | Acc: (87.25%) (8725/10000)\n",
            "Epoch: 78 | Batch_idx: 0 |  Loss: (0.1805) | Acc: (92.97%) (119/128)\n",
            "Epoch: 78 | Batch_idx: 10 |  Loss: (0.2700) | Acc: (90.77%) (1278/1408)\n",
            "Epoch: 78 | Batch_idx: 20 |  Loss: (0.3130) | Acc: (88.99%) (2392/2688)\n",
            "Epoch: 78 | Batch_idx: 30 |  Loss: (0.3251) | Acc: (88.66%) (3518/3968)\n",
            "Epoch: 78 | Batch_idx: 40 |  Loss: (0.3341) | Acc: (88.80%) (4660/5248)\n",
            "Epoch: 78 | Batch_idx: 50 |  Loss: (0.3256) | Acc: (89.14%) (5819/6528)\n",
            "Epoch: 78 | Batch_idx: 60 |  Loss: (0.3272) | Acc: (89.06%) (6954/7808)\n",
            "Epoch: 78 | Batch_idx: 70 |  Loss: (0.3241) | Acc: (89.25%) (8111/9088)\n",
            "Epoch: 78 | Batch_idx: 80 |  Loss: (0.3302) | Acc: (89.15%) (9243/10368)\n",
            "Epoch: 78 | Batch_idx: 90 |  Loss: (0.3342) | Acc: (89.01%) (10368/11648)\n",
            "Epoch: 78 | Batch_idx: 100 |  Loss: (0.3324) | Acc: (89.15%) (11525/12928)\n",
            "Epoch: 78 | Batch_idx: 110 |  Loss: (0.3328) | Acc: (89.17%) (12669/14208)\n",
            "Epoch: 78 | Batch_idx: 120 |  Loss: (0.3382) | Acc: (89.05%) (13792/15488)\n",
            "Epoch: 78 | Batch_idx: 130 |  Loss: (0.3396) | Acc: (89.03%) (14928/16768)\n",
            "Epoch: 78 | Batch_idx: 140 |  Loss: (0.3377) | Acc: (89.11%) (16083/18048)\n",
            "Epoch: 78 | Batch_idx: 150 |  Loss: (0.3369) | Acc: (89.14%) (17229/19328)\n",
            "Epoch: 78 | Batch_idx: 160 |  Loss: (0.3360) | Acc: (89.12%) (18366/20608)\n",
            "Epoch: 78 | Batch_idx: 170 |  Loss: (0.3392) | Acc: (89.04%) (19490/21888)\n",
            "Epoch: 78 | Batch_idx: 180 |  Loss: (0.3426) | Acc: (88.91%) (20599/23168)\n",
            "Epoch: 78 | Batch_idx: 190 |  Loss: (0.3451) | Acc: (88.83%) (21716/24448)\n",
            "Epoch: 78 | Batch_idx: 200 |  Loss: (0.3441) | Acc: (88.83%) (22853/25728)\n",
            "Epoch: 78 | Batch_idx: 210 |  Loss: (0.3446) | Acc: (88.80%) (23984/27008)\n",
            "Epoch: 78 | Batch_idx: 220 |  Loss: (0.3452) | Acc: (88.80%) (25121/28288)\n",
            "Epoch: 78 | Batch_idx: 230 |  Loss: (0.3466) | Acc: (88.76%) (26244/29568)\n",
            "Epoch: 78 | Batch_idx: 240 |  Loss: (0.3490) | Acc: (88.73%) (27370/30848)\n",
            "Epoch: 78 | Batch_idx: 250 |  Loss: (0.3488) | Acc: (88.72%) (28505/32128)\n",
            "Epoch: 78 | Batch_idx: 260 |  Loss: (0.3483) | Acc: (88.73%) (29644/33408)\n",
            "Epoch: 78 | Batch_idx: 270 |  Loss: (0.3468) | Acc: (88.79%) (30800/34688)\n",
            "Epoch: 78 | Batch_idx: 280 |  Loss: (0.3457) | Acc: (88.83%) (31950/35968)\n",
            "Epoch: 78 | Batch_idx: 290 |  Loss: (0.3465) | Acc: (88.78%) (33070/37248)\n",
            "Epoch: 78 | Batch_idx: 300 |  Loss: (0.3473) | Acc: (88.72%) (34182/38528)\n",
            "Epoch: 78 | Batch_idx: 310 |  Loss: (0.3483) | Acc: (88.69%) (35304/39808)\n",
            "Epoch: 78 | Batch_idx: 320 |  Loss: (0.3490) | Acc: (88.65%) (36425/41088)\n",
            "Epoch: 78 | Batch_idx: 330 |  Loss: (0.3498) | Acc: (88.64%) (37553/42368)\n",
            "Epoch: 78 | Batch_idx: 340 |  Loss: (0.3511) | Acc: (88.59%) (38669/43648)\n",
            "Epoch: 78 | Batch_idx: 350 |  Loss: (0.3513) | Acc: (88.61%) (39812/44928)\n",
            "Epoch: 78 | Batch_idx: 360 |  Loss: (0.3519) | Acc: (88.60%) (40941/46208)\n",
            "Epoch: 78 | Batch_idx: 370 |  Loss: (0.3521) | Acc: (88.58%) (42067/47488)\n",
            "Epoch: 78 | Batch_idx: 380 |  Loss: (0.3528) | Acc: (88.53%) (43175/48768)\n",
            "Epoch: 78 | Batch_idx: 390 |  Loss: (0.3534) | Acc: (88.52%) (44262/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3944) | Acc: (87.89%) (8789/10000)\n",
            "Epoch: 79 | Batch_idx: 0 |  Loss: (0.2679) | Acc: (92.19%) (118/128)\n",
            "Epoch: 79 | Batch_idx: 10 |  Loss: (0.3122) | Acc: (89.42%) (1259/1408)\n",
            "Epoch: 79 | Batch_idx: 20 |  Loss: (0.3336) | Acc: (88.84%) (2388/2688)\n",
            "Epoch: 79 | Batch_idx: 30 |  Loss: (0.3431) | Acc: (88.84%) (3525/3968)\n",
            "Epoch: 79 | Batch_idx: 40 |  Loss: (0.3384) | Acc: (88.85%) (4663/5248)\n",
            "Epoch: 79 | Batch_idx: 50 |  Loss: (0.3347) | Acc: (88.92%) (5805/6528)\n",
            "Epoch: 79 | Batch_idx: 60 |  Loss: (0.3381) | Acc: (88.65%) (6922/7808)\n",
            "Epoch: 79 | Batch_idx: 70 |  Loss: (0.3387) | Acc: (88.71%) (8062/9088)\n",
            "Epoch: 79 | Batch_idx: 80 |  Loss: (0.3403) | Acc: (88.63%) (9189/10368)\n",
            "Epoch: 79 | Batch_idx: 90 |  Loss: (0.3437) | Acc: (88.62%) (10323/11648)\n",
            "Epoch: 79 | Batch_idx: 100 |  Loss: (0.3453) | Acc: (88.52%) (11444/12928)\n",
            "Epoch: 79 | Batch_idx: 110 |  Loss: (0.3455) | Acc: (88.52%) (12577/14208)\n",
            "Epoch: 79 | Batch_idx: 120 |  Loss: (0.3479) | Acc: (88.49%) (13706/15488)\n",
            "Epoch: 79 | Batch_idx: 130 |  Loss: (0.3496) | Acc: (88.47%) (14834/16768)\n",
            "Epoch: 79 | Batch_idx: 140 |  Loss: (0.3508) | Acc: (88.41%) (15957/18048)\n",
            "Epoch: 79 | Batch_idx: 150 |  Loss: (0.3521) | Acc: (88.33%) (17073/19328)\n",
            "Epoch: 79 | Batch_idx: 160 |  Loss: (0.3507) | Acc: (88.38%) (18214/20608)\n",
            "Epoch: 79 | Batch_idx: 170 |  Loss: (0.3533) | Acc: (88.37%) (19343/21888)\n",
            "Epoch: 79 | Batch_idx: 180 |  Loss: (0.3519) | Acc: (88.49%) (20502/23168)\n",
            "Epoch: 79 | Batch_idx: 190 |  Loss: (0.3511) | Acc: (88.53%) (21645/24448)\n",
            "Epoch: 79 | Batch_idx: 200 |  Loss: (0.3510) | Acc: (88.55%) (22781/25728)\n",
            "Epoch: 79 | Batch_idx: 210 |  Loss: (0.3512) | Acc: (88.54%) (23914/27008)\n",
            "Epoch: 79 | Batch_idx: 220 |  Loss: (0.3507) | Acc: (88.55%) (25049/28288)\n",
            "Epoch: 79 | Batch_idx: 230 |  Loss: (0.3508) | Acc: (88.52%) (26175/29568)\n",
            "Epoch: 79 | Batch_idx: 240 |  Loss: (0.3509) | Acc: (88.56%) (27320/30848)\n",
            "Epoch: 79 | Batch_idx: 250 |  Loss: (0.3527) | Acc: (88.50%) (28432/32128)\n",
            "Epoch: 79 | Batch_idx: 260 |  Loss: (0.3536) | Acc: (88.46%) (29552/33408)\n",
            "Epoch: 79 | Batch_idx: 270 |  Loss: (0.3545) | Acc: (88.43%) (30675/34688)\n",
            "Epoch: 79 | Batch_idx: 280 |  Loss: (0.3546) | Acc: (88.44%) (31809/35968)\n",
            "Epoch: 79 | Batch_idx: 290 |  Loss: (0.3542) | Acc: (88.44%) (32941/37248)\n",
            "Epoch: 79 | Batch_idx: 300 |  Loss: (0.3551) | Acc: (88.43%) (34069/38528)\n",
            "Epoch: 79 | Batch_idx: 310 |  Loss: (0.3553) | Acc: (88.42%) (35197/39808)\n",
            "Epoch: 79 | Batch_idx: 320 |  Loss: (0.3560) | Acc: (88.39%) (36318/41088)\n",
            "Epoch: 79 | Batch_idx: 330 |  Loss: (0.3564) | Acc: (88.38%) (37444/42368)\n",
            "Epoch: 79 | Batch_idx: 340 |  Loss: (0.3558) | Acc: (88.40%) (38585/43648)\n",
            "Epoch: 79 | Batch_idx: 350 |  Loss: (0.3560) | Acc: (88.39%) (39711/44928)\n",
            "Epoch: 79 | Batch_idx: 360 |  Loss: (0.3565) | Acc: (88.38%) (40839/46208)\n",
            "Epoch: 79 | Batch_idx: 370 |  Loss: (0.3563) | Acc: (88.37%) (41963/47488)\n",
            "Epoch: 79 | Batch_idx: 380 |  Loss: (0.3555) | Acc: (88.38%) (43099/48768)\n",
            "Epoch: 79 | Batch_idx: 390 |  Loss: (0.3541) | Acc: (88.41%) (44204/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4661) | Acc: (86.09%) (8609/10000)\n",
            "Epoch: 80 | Batch_idx: 0 |  Loss: (0.3206) | Acc: (89.84%) (115/128)\n",
            "Epoch: 80 | Batch_idx: 10 |  Loss: (0.3297) | Acc: (89.84%) (1265/1408)\n",
            "Epoch: 80 | Batch_idx: 20 |  Loss: (0.3233) | Acc: (89.77%) (2413/2688)\n",
            "Epoch: 80 | Batch_idx: 30 |  Loss: (0.3096) | Acc: (90.10%) (3575/3968)\n",
            "Epoch: 80 | Batch_idx: 40 |  Loss: (0.2983) | Acc: (90.36%) (4742/5248)\n",
            "Epoch: 80 | Batch_idx: 50 |  Loss: (0.2930) | Acc: (90.46%) (5905/6528)\n",
            "Epoch: 80 | Batch_idx: 60 |  Loss: (0.2890) | Acc: (90.41%) (7059/7808)\n",
            "Epoch: 80 | Batch_idx: 70 |  Loss: (0.2825) | Acc: (90.71%) (8244/9088)\n",
            "Epoch: 80 | Batch_idx: 80 |  Loss: (0.2775) | Acc: (90.93%) (9428/10368)\n",
            "Epoch: 80 | Batch_idx: 90 |  Loss: (0.2774) | Acc: (90.88%) (10586/11648)\n",
            "Epoch: 80 | Batch_idx: 100 |  Loss: (0.2742) | Acc: (91.00%) (11765/12928)\n",
            "Epoch: 80 | Batch_idx: 110 |  Loss: (0.2722) | Acc: (91.05%) (12936/14208)\n",
            "Epoch: 80 | Batch_idx: 120 |  Loss: (0.2702) | Acc: (91.05%) (14102/15488)\n",
            "Epoch: 80 | Batch_idx: 130 |  Loss: (0.2680) | Acc: (91.14%) (15283/16768)\n",
            "Epoch: 80 | Batch_idx: 140 |  Loss: (0.2660) | Acc: (91.21%) (16461/18048)\n",
            "Epoch: 80 | Batch_idx: 150 |  Loss: (0.2634) | Acc: (91.26%) (17639/19328)\n",
            "Epoch: 80 | Batch_idx: 160 |  Loss: (0.2614) | Acc: (91.36%) (18828/20608)\n",
            "Epoch: 80 | Batch_idx: 170 |  Loss: (0.2595) | Acc: (91.43%) (20012/21888)\n",
            "Epoch: 80 | Batch_idx: 180 |  Loss: (0.2590) | Acc: (91.43%) (21182/23168)\n",
            "Epoch: 80 | Batch_idx: 190 |  Loss: (0.2575) | Acc: (91.47%) (22362/24448)\n",
            "Epoch: 80 | Batch_idx: 200 |  Loss: (0.2569) | Acc: (91.49%) (23539/25728)\n",
            "Epoch: 80 | Batch_idx: 210 |  Loss: (0.2563) | Acc: (91.51%) (24714/27008)\n",
            "Epoch: 80 | Batch_idx: 220 |  Loss: (0.2563) | Acc: (91.50%) (25883/28288)\n",
            "Epoch: 80 | Batch_idx: 230 |  Loss: (0.2554) | Acc: (91.52%) (27060/29568)\n",
            "Epoch: 80 | Batch_idx: 240 |  Loss: (0.2544) | Acc: (91.57%) (28248/30848)\n",
            "Epoch: 80 | Batch_idx: 250 |  Loss: (0.2540) | Acc: (91.60%) (29428/32128)\n",
            "Epoch: 80 | Batch_idx: 260 |  Loss: (0.2539) | Acc: (91.62%) (30610/33408)\n",
            "Epoch: 80 | Batch_idx: 270 |  Loss: (0.2523) | Acc: (91.65%) (31792/34688)\n",
            "Epoch: 80 | Batch_idx: 280 |  Loss: (0.2512) | Acc: (91.69%) (32979/35968)\n",
            "Epoch: 80 | Batch_idx: 290 |  Loss: (0.2504) | Acc: (91.72%) (34164/37248)\n",
            "Epoch: 80 | Batch_idx: 300 |  Loss: (0.2501) | Acc: (91.72%) (35337/38528)\n",
            "Epoch: 80 | Batch_idx: 310 |  Loss: (0.2500) | Acc: (91.73%) (36516/39808)\n",
            "Epoch: 80 | Batch_idx: 320 |  Loss: (0.2502) | Acc: (91.72%) (37684/41088)\n",
            "Epoch: 80 | Batch_idx: 330 |  Loss: (0.2491) | Acc: (91.74%) (38869/42368)\n",
            "Epoch: 80 | Batch_idx: 340 |  Loss: (0.2488) | Acc: (91.76%) (40050/43648)\n",
            "Epoch: 80 | Batch_idx: 350 |  Loss: (0.2479) | Acc: (91.77%) (41231/44928)\n",
            "Epoch: 80 | Batch_idx: 360 |  Loss: (0.2461) | Acc: (91.84%) (42439/46208)\n",
            "Epoch: 80 | Batch_idx: 370 |  Loss: (0.2453) | Acc: (91.87%) (43626/47488)\n",
            "Epoch: 80 | Batch_idx: 380 |  Loss: (0.2446) | Acc: (91.88%) (44806/48768)\n",
            "Epoch: 80 | Batch_idx: 390 |  Loss: (0.2426) | Acc: (91.94%) (45968/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3084) | Acc: (90.43%) (9043/10000)\n",
            "Epoch: 81 | Batch_idx: 0 |  Loss: (0.3147) | Acc: (89.06%) (114/128)\n",
            "Epoch: 81 | Batch_idx: 10 |  Loss: (0.2017) | Acc: (93.25%) (1313/1408)\n",
            "Epoch: 81 | Batch_idx: 20 |  Loss: (0.2117) | Acc: (93.38%) (2510/2688)\n",
            "Epoch: 81 | Batch_idx: 30 |  Loss: (0.2109) | Acc: (93.35%) (3704/3968)\n",
            "Epoch: 81 | Batch_idx: 40 |  Loss: (0.2137) | Acc: (93.12%) (4887/5248)\n",
            "Epoch: 81 | Batch_idx: 50 |  Loss: (0.2123) | Acc: (93.11%) (6078/6528)\n",
            "Epoch: 81 | Batch_idx: 60 |  Loss: (0.2080) | Acc: (93.11%) (7270/7808)\n",
            "Epoch: 81 | Batch_idx: 70 |  Loss: (0.2072) | Acc: (93.07%) (8458/9088)\n",
            "Epoch: 81 | Batch_idx: 80 |  Loss: (0.2080) | Acc: (93.11%) (9654/10368)\n",
            "Epoch: 81 | Batch_idx: 90 |  Loss: (0.2128) | Acc: (92.93%) (10824/11648)\n",
            "Epoch: 81 | Batch_idx: 100 |  Loss: (0.2114) | Acc: (93.02%) (12026/12928)\n",
            "Epoch: 81 | Batch_idx: 110 |  Loss: (0.2105) | Acc: (93.04%) (13219/14208)\n",
            "Epoch: 81 | Batch_idx: 120 |  Loss: (0.2115) | Acc: (93.03%) (14409/15488)\n",
            "Epoch: 81 | Batch_idx: 130 |  Loss: (0.2106) | Acc: (93.09%) (15609/16768)\n",
            "Epoch: 81 | Batch_idx: 140 |  Loss: (0.2099) | Acc: (93.11%) (16805/18048)\n",
            "Epoch: 81 | Batch_idx: 150 |  Loss: (0.2095) | Acc: (93.11%) (17996/19328)\n",
            "Epoch: 81 | Batch_idx: 160 |  Loss: (0.2085) | Acc: (93.09%) (19183/20608)\n",
            "Epoch: 81 | Batch_idx: 170 |  Loss: (0.2103) | Acc: (93.06%) (20370/21888)\n",
            "Epoch: 81 | Batch_idx: 180 |  Loss: (0.2099) | Acc: (93.10%) (21570/23168)\n",
            "Epoch: 81 | Batch_idx: 190 |  Loss: (0.2099) | Acc: (93.08%) (22755/24448)\n",
            "Epoch: 81 | Batch_idx: 200 |  Loss: (0.2084) | Acc: (93.10%) (23953/25728)\n",
            "Epoch: 81 | Batch_idx: 210 |  Loss: (0.2078) | Acc: (93.10%) (25144/27008)\n",
            "Epoch: 81 | Batch_idx: 220 |  Loss: (0.2086) | Acc: (93.11%) (26340/28288)\n",
            "Epoch: 81 | Batch_idx: 230 |  Loss: (0.2091) | Acc: (93.09%) (27526/29568)\n",
            "Epoch: 81 | Batch_idx: 240 |  Loss: (0.2083) | Acc: (93.12%) (28726/30848)\n",
            "Epoch: 81 | Batch_idx: 250 |  Loss: (0.2071) | Acc: (93.15%) (29926/32128)\n",
            "Epoch: 81 | Batch_idx: 260 |  Loss: (0.2069) | Acc: (93.13%) (31113/33408)\n",
            "Epoch: 81 | Batch_idx: 270 |  Loss: (0.2065) | Acc: (93.14%) (32307/34688)\n",
            "Epoch: 81 | Batch_idx: 280 |  Loss: (0.2063) | Acc: (93.15%) (33505/35968)\n",
            "Epoch: 81 | Batch_idx: 290 |  Loss: (0.2061) | Acc: (93.18%) (34707/37248)\n",
            "Epoch: 81 | Batch_idx: 300 |  Loss: (0.2053) | Acc: (93.19%) (35904/38528)\n",
            "Epoch: 81 | Batch_idx: 310 |  Loss: (0.2054) | Acc: (93.21%) (37107/39808)\n",
            "Epoch: 81 | Batch_idx: 320 |  Loss: (0.2053) | Acc: (93.22%) (38302/41088)\n",
            "Epoch: 81 | Batch_idx: 330 |  Loss: (0.2057) | Acc: (93.22%) (39495/42368)\n",
            "Epoch: 81 | Batch_idx: 340 |  Loss: (0.2054) | Acc: (93.22%) (40689/43648)\n",
            "Epoch: 81 | Batch_idx: 350 |  Loss: (0.2047) | Acc: (93.26%) (41898/44928)\n",
            "Epoch: 81 | Batch_idx: 360 |  Loss: (0.2051) | Acc: (93.24%) (43084/46208)\n",
            "Epoch: 81 | Batch_idx: 370 |  Loss: (0.2045) | Acc: (93.25%) (44283/47488)\n",
            "Epoch: 81 | Batch_idx: 380 |  Loss: (0.2038) | Acc: (93.28%) (45493/48768)\n",
            "Epoch: 81 | Batch_idx: 390 |  Loss: (0.2037) | Acc: (93.30%) (46651/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3079) | Acc: (90.52%) (9052/10000)\n",
            "Epoch: 82 | Batch_idx: 0 |  Loss: (0.0957) | Acc: (96.09%) (123/128)\n",
            "Epoch: 82 | Batch_idx: 10 |  Loss: (0.1802) | Acc: (94.03%) (1324/1408)\n",
            "Epoch: 82 | Batch_idx: 20 |  Loss: (0.1785) | Acc: (93.94%) (2525/2688)\n",
            "Epoch: 82 | Batch_idx: 30 |  Loss: (0.1840) | Acc: (93.93%) (3727/3968)\n",
            "Epoch: 82 | Batch_idx: 40 |  Loss: (0.1859) | Acc: (93.86%) (4926/5248)\n",
            "Epoch: 82 | Batch_idx: 50 |  Loss: (0.1854) | Acc: (93.90%) (6130/6528)\n",
            "Epoch: 82 | Batch_idx: 60 |  Loss: (0.1819) | Acc: (93.98%) (7338/7808)\n",
            "Epoch: 82 | Batch_idx: 70 |  Loss: (0.1852) | Acc: (93.83%) (8527/9088)\n",
            "Epoch: 82 | Batch_idx: 80 |  Loss: (0.1857) | Acc: (93.77%) (9722/10368)\n",
            "Epoch: 82 | Batch_idx: 90 |  Loss: (0.1860) | Acc: (93.75%) (10920/11648)\n",
            "Epoch: 82 | Batch_idx: 100 |  Loss: (0.1863) | Acc: (93.72%) (12116/12928)\n",
            "Epoch: 82 | Batch_idx: 110 |  Loss: (0.1833) | Acc: (93.76%) (13322/14208)\n",
            "Epoch: 82 | Batch_idx: 120 |  Loss: (0.1806) | Acc: (93.89%) (14541/15488)\n",
            "Epoch: 82 | Batch_idx: 130 |  Loss: (0.1859) | Acc: (93.70%) (15712/16768)\n",
            "Epoch: 82 | Batch_idx: 140 |  Loss: (0.1868) | Acc: (93.74%) (16918/18048)\n",
            "Epoch: 82 | Batch_idx: 150 |  Loss: (0.1868) | Acc: (93.71%) (18112/19328)\n",
            "Epoch: 82 | Batch_idx: 160 |  Loss: (0.1842) | Acc: (93.86%) (19342/20608)\n",
            "Epoch: 82 | Batch_idx: 170 |  Loss: (0.1844) | Acc: (93.90%) (20553/21888)\n",
            "Epoch: 82 | Batch_idx: 180 |  Loss: (0.1836) | Acc: (93.93%) (21761/23168)\n",
            "Epoch: 82 | Batch_idx: 190 |  Loss: (0.1842) | Acc: (93.90%) (22956/24448)\n",
            "Epoch: 82 | Batch_idx: 200 |  Loss: (0.1838) | Acc: (93.90%) (24159/25728)\n",
            "Epoch: 82 | Batch_idx: 210 |  Loss: (0.1829) | Acc: (93.95%) (25373/27008)\n",
            "Epoch: 82 | Batch_idx: 220 |  Loss: (0.1841) | Acc: (93.88%) (26558/28288)\n",
            "Epoch: 82 | Batch_idx: 230 |  Loss: (0.1841) | Acc: (93.90%) (27765/29568)\n",
            "Epoch: 82 | Batch_idx: 240 |  Loss: (0.1837) | Acc: (93.89%) (28964/30848)\n",
            "Epoch: 82 | Batch_idx: 250 |  Loss: (0.1853) | Acc: (93.85%) (30153/32128)\n",
            "Epoch: 82 | Batch_idx: 260 |  Loss: (0.1859) | Acc: (93.82%) (31344/33408)\n",
            "Epoch: 82 | Batch_idx: 270 |  Loss: (0.1852) | Acc: (93.84%) (32552/34688)\n",
            "Epoch: 82 | Batch_idx: 280 |  Loss: (0.1845) | Acc: (93.89%) (33770/35968)\n",
            "Epoch: 82 | Batch_idx: 290 |  Loss: (0.1837) | Acc: (93.93%) (34986/37248)\n",
            "Epoch: 82 | Batch_idx: 300 |  Loss: (0.1836) | Acc: (93.92%) (36187/38528)\n",
            "Epoch: 82 | Batch_idx: 310 |  Loss: (0.1835) | Acc: (93.93%) (37392/39808)\n",
            "Epoch: 82 | Batch_idx: 320 |  Loss: (0.1838) | Acc: (93.89%) (38579/41088)\n",
            "Epoch: 82 | Batch_idx: 330 |  Loss: (0.1848) | Acc: (93.86%) (39767/42368)\n",
            "Epoch: 82 | Batch_idx: 340 |  Loss: (0.1851) | Acc: (93.85%) (40964/43648)\n",
            "Epoch: 82 | Batch_idx: 350 |  Loss: (0.1847) | Acc: (93.86%) (42170/44928)\n",
            "Epoch: 82 | Batch_idx: 360 |  Loss: (0.1852) | Acc: (93.85%) (43368/46208)\n",
            "Epoch: 82 | Batch_idx: 370 |  Loss: (0.1845) | Acc: (93.86%) (44570/47488)\n",
            "Epoch: 82 | Batch_idx: 380 |  Loss: (0.1844) | Acc: (93.85%) (45769/48768)\n",
            "Epoch: 82 | Batch_idx: 390 |  Loss: (0.1837) | Acc: (93.86%) (46931/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2944) | Acc: (90.86%) (9086/10000)\n",
            "Epoch: 83 | Batch_idx: 0 |  Loss: (0.2179) | Acc: (91.41%) (117/128)\n",
            "Epoch: 83 | Batch_idx: 10 |  Loss: (0.2139) | Acc: (92.40%) (1301/1408)\n",
            "Epoch: 83 | Batch_idx: 20 |  Loss: (0.1955) | Acc: (93.19%) (2505/2688)\n",
            "Epoch: 83 | Batch_idx: 30 |  Loss: (0.1909) | Acc: (93.32%) (3703/3968)\n",
            "Epoch: 83 | Batch_idx: 40 |  Loss: (0.1873) | Acc: (93.65%) (4915/5248)\n",
            "Epoch: 83 | Batch_idx: 50 |  Loss: (0.1887) | Acc: (93.67%) (6115/6528)\n",
            "Epoch: 83 | Batch_idx: 60 |  Loss: (0.1860) | Acc: (93.81%) (7325/7808)\n",
            "Epoch: 83 | Batch_idx: 70 |  Loss: (0.1858) | Acc: (93.85%) (8529/9088)\n",
            "Epoch: 83 | Batch_idx: 80 |  Loss: (0.1853) | Acc: (93.86%) (9731/10368)\n",
            "Epoch: 83 | Batch_idx: 90 |  Loss: (0.1841) | Acc: (93.93%) (10941/11648)\n",
            "Epoch: 83 | Batch_idx: 100 |  Loss: (0.1793) | Acc: (94.04%) (12157/12928)\n",
            "Epoch: 83 | Batch_idx: 110 |  Loss: (0.1804) | Acc: (94.04%) (13361/14208)\n",
            "Epoch: 83 | Batch_idx: 120 |  Loss: (0.1804) | Acc: (94.02%) (14562/15488)\n",
            "Epoch: 83 | Batch_idx: 130 |  Loss: (0.1831) | Acc: (93.98%) (15759/16768)\n",
            "Epoch: 83 | Batch_idx: 140 |  Loss: (0.1807) | Acc: (94.07%) (16978/18048)\n",
            "Epoch: 83 | Batch_idx: 150 |  Loss: (0.1797) | Acc: (94.09%) (18185/19328)\n",
            "Epoch: 83 | Batch_idx: 160 |  Loss: (0.1798) | Acc: (94.12%) (19397/20608)\n",
            "Epoch: 83 | Batch_idx: 170 |  Loss: (0.1803) | Acc: (94.13%) (20603/21888)\n",
            "Epoch: 83 | Batch_idx: 180 |  Loss: (0.1810) | Acc: (94.13%) (21808/23168)\n",
            "Epoch: 83 | Batch_idx: 190 |  Loss: (0.1804) | Acc: (94.11%) (23007/24448)\n",
            "Epoch: 83 | Batch_idx: 200 |  Loss: (0.1810) | Acc: (94.12%) (24214/25728)\n",
            "Epoch: 83 | Batch_idx: 210 |  Loss: (0.1802) | Acc: (94.12%) (25421/27008)\n",
            "Epoch: 83 | Batch_idx: 220 |  Loss: (0.1799) | Acc: (94.10%) (26620/28288)\n",
            "Epoch: 83 | Batch_idx: 230 |  Loss: (0.1791) | Acc: (94.12%) (27828/29568)\n",
            "Epoch: 83 | Batch_idx: 240 |  Loss: (0.1781) | Acc: (94.13%) (29037/30848)\n",
            "Epoch: 83 | Batch_idx: 250 |  Loss: (0.1774) | Acc: (94.15%) (30248/32128)\n",
            "Epoch: 83 | Batch_idx: 260 |  Loss: (0.1769) | Acc: (94.15%) (31452/33408)\n",
            "Epoch: 83 | Batch_idx: 270 |  Loss: (0.1767) | Acc: (94.16%) (32663/34688)\n",
            "Epoch: 83 | Batch_idx: 280 |  Loss: (0.1764) | Acc: (94.19%) (33877/35968)\n",
            "Epoch: 83 | Batch_idx: 290 |  Loss: (0.1763) | Acc: (94.20%) (35087/37248)\n",
            "Epoch: 83 | Batch_idx: 300 |  Loss: (0.1755) | Acc: (94.23%) (36306/38528)\n",
            "Epoch: 83 | Batch_idx: 310 |  Loss: (0.1756) | Acc: (94.22%) (37509/39808)\n",
            "Epoch: 83 | Batch_idx: 320 |  Loss: (0.1749) | Acc: (94.23%) (38716/41088)\n",
            "Epoch: 83 | Batch_idx: 330 |  Loss: (0.1755) | Acc: (94.22%) (39918/42368)\n",
            "Epoch: 83 | Batch_idx: 340 |  Loss: (0.1750) | Acc: (94.21%) (41121/43648)\n",
            "Epoch: 83 | Batch_idx: 350 |  Loss: (0.1755) | Acc: (94.20%) (42323/44928)\n",
            "Epoch: 83 | Batch_idx: 360 |  Loss: (0.1762) | Acc: (94.18%) (43517/46208)\n",
            "Epoch: 83 | Batch_idx: 370 |  Loss: (0.1757) | Acc: (94.19%) (44731/47488)\n",
            "Epoch: 83 | Batch_idx: 380 |  Loss: (0.1758) | Acc: (94.17%) (45925/48768)\n",
            "Epoch: 83 | Batch_idx: 390 |  Loss: (0.1765) | Acc: (94.14%) (47072/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3113) | Acc: (90.69%) (9069/10000)\n",
            "Epoch: 84 | Batch_idx: 0 |  Loss: (0.2291) | Acc: (89.84%) (115/128)\n",
            "Epoch: 84 | Batch_idx: 10 |  Loss: (0.1806) | Acc: (93.82%) (1321/1408)\n",
            "Epoch: 84 | Batch_idx: 20 |  Loss: (0.1700) | Acc: (93.97%) (2526/2688)\n",
            "Epoch: 84 | Batch_idx: 30 |  Loss: (0.1665) | Acc: (94.28%) (3741/3968)\n",
            "Epoch: 84 | Batch_idx: 40 |  Loss: (0.1650) | Acc: (94.25%) (4946/5248)\n",
            "Epoch: 84 | Batch_idx: 50 |  Loss: (0.1618) | Acc: (94.39%) (6162/6528)\n",
            "Epoch: 84 | Batch_idx: 60 |  Loss: (0.1586) | Acc: (94.47%) (7376/7808)\n",
            "Epoch: 84 | Batch_idx: 70 |  Loss: (0.1604) | Acc: (94.45%) (8584/9088)\n",
            "Epoch: 84 | Batch_idx: 80 |  Loss: (0.1607) | Acc: (94.46%) (9794/10368)\n",
            "Epoch: 84 | Batch_idx: 90 |  Loss: (0.1587) | Acc: (94.57%) (11015/11648)\n",
            "Epoch: 84 | Batch_idx: 100 |  Loss: (0.1589) | Acc: (94.54%) (12222/12928)\n",
            "Epoch: 84 | Batch_idx: 110 |  Loss: (0.1598) | Acc: (94.55%) (13433/14208)\n",
            "Epoch: 84 | Batch_idx: 120 |  Loss: (0.1613) | Acc: (94.54%) (14642/15488)\n",
            "Epoch: 84 | Batch_idx: 130 |  Loss: (0.1626) | Acc: (94.52%) (15849/16768)\n",
            "Epoch: 84 | Batch_idx: 140 |  Loss: (0.1611) | Acc: (94.58%) (17070/18048)\n",
            "Epoch: 84 | Batch_idx: 150 |  Loss: (0.1626) | Acc: (94.51%) (18267/19328)\n",
            "Epoch: 84 | Batch_idx: 160 |  Loss: (0.1638) | Acc: (94.51%) (19476/20608)\n",
            "Epoch: 84 | Batch_idx: 170 |  Loss: (0.1637) | Acc: (94.52%) (20688/21888)\n",
            "Epoch: 84 | Batch_idx: 180 |  Loss: (0.1645) | Acc: (94.46%) (21885/23168)\n",
            "Epoch: 84 | Batch_idx: 190 |  Loss: (0.1653) | Acc: (94.41%) (23082/24448)\n",
            "Epoch: 84 | Batch_idx: 200 |  Loss: (0.1652) | Acc: (94.45%) (24299/25728)\n",
            "Epoch: 84 | Batch_idx: 210 |  Loss: (0.1655) | Acc: (94.43%) (25504/27008)\n",
            "Epoch: 84 | Batch_idx: 220 |  Loss: (0.1668) | Acc: (94.39%) (26702/28288)\n",
            "Epoch: 84 | Batch_idx: 230 |  Loss: (0.1674) | Acc: (94.39%) (27910/29568)\n",
            "Epoch: 84 | Batch_idx: 240 |  Loss: (0.1663) | Acc: (94.41%) (29124/30848)\n",
            "Epoch: 84 | Batch_idx: 250 |  Loss: (0.1661) | Acc: (94.42%) (30335/32128)\n",
            "Epoch: 84 | Batch_idx: 260 |  Loss: (0.1659) | Acc: (94.43%) (31547/33408)\n",
            "Epoch: 84 | Batch_idx: 270 |  Loss: (0.1663) | Acc: (94.42%) (32754/34688)\n",
            "Epoch: 84 | Batch_idx: 280 |  Loss: (0.1669) | Acc: (94.40%) (33955/35968)\n",
            "Epoch: 84 | Batch_idx: 290 |  Loss: (0.1662) | Acc: (94.43%) (35173/37248)\n",
            "Epoch: 84 | Batch_idx: 300 |  Loss: (0.1677) | Acc: (94.37%) (36359/38528)\n",
            "Epoch: 84 | Batch_idx: 310 |  Loss: (0.1689) | Acc: (94.34%) (37553/39808)\n",
            "Epoch: 84 | Batch_idx: 320 |  Loss: (0.1686) | Acc: (94.34%) (38763/41088)\n",
            "Epoch: 84 | Batch_idx: 330 |  Loss: (0.1697) | Acc: (94.30%) (39955/42368)\n",
            "Epoch: 84 | Batch_idx: 340 |  Loss: (0.1689) | Acc: (94.31%) (41166/43648)\n",
            "Epoch: 84 | Batch_idx: 350 |  Loss: (0.1684) | Acc: (94.33%) (42382/44928)\n",
            "Epoch: 84 | Batch_idx: 360 |  Loss: (0.1676) | Acc: (94.36%) (43601/46208)\n",
            "Epoch: 84 | Batch_idx: 370 |  Loss: (0.1685) | Acc: (94.33%) (44797/47488)\n",
            "Epoch: 84 | Batch_idx: 380 |  Loss: (0.1693) | Acc: (94.31%) (45992/48768)\n",
            "Epoch: 84 | Batch_idx: 390 |  Loss: (0.1690) | Acc: (94.33%) (47163/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2995) | Acc: (91.05%) (9105/10000)\n",
            "Epoch: 85 | Batch_idx: 0 |  Loss: (0.1088) | Acc: (94.53%) (121/128)\n",
            "Epoch: 85 | Batch_idx: 10 |  Loss: (0.1566) | Acc: (94.46%) (1330/1408)\n",
            "Epoch: 85 | Batch_idx: 20 |  Loss: (0.1767) | Acc: (94.01%) (2527/2688)\n",
            "Epoch: 85 | Batch_idx: 30 |  Loss: (0.1784) | Acc: (94.10%) (3734/3968)\n",
            "Epoch: 85 | Batch_idx: 40 |  Loss: (0.1726) | Acc: (94.26%) (4947/5248)\n",
            "Epoch: 85 | Batch_idx: 50 |  Loss: (0.1648) | Acc: (94.53%) (6171/6528)\n",
            "Epoch: 85 | Batch_idx: 60 |  Loss: (0.1614) | Acc: (94.63%) (7389/7808)\n",
            "Epoch: 85 | Batch_idx: 70 |  Loss: (0.1617) | Acc: (94.53%) (8591/9088)\n",
            "Epoch: 85 | Batch_idx: 80 |  Loss: (0.1600) | Acc: (94.52%) (9800/10368)\n",
            "Epoch: 85 | Batch_idx: 90 |  Loss: (0.1581) | Acc: (94.53%) (11011/11648)\n",
            "Epoch: 85 | Batch_idx: 100 |  Loss: (0.1574) | Acc: (94.54%) (12222/12928)\n",
            "Epoch: 85 | Batch_idx: 110 |  Loss: (0.1611) | Acc: (94.43%) (13416/14208)\n",
            "Epoch: 85 | Batch_idx: 120 |  Loss: (0.1630) | Acc: (94.39%) (14619/15488)\n",
            "Epoch: 85 | Batch_idx: 130 |  Loss: (0.1649) | Acc: (94.32%) (15815/16768)\n",
            "Epoch: 85 | Batch_idx: 140 |  Loss: (0.1628) | Acc: (94.35%) (17029/18048)\n",
            "Epoch: 85 | Batch_idx: 150 |  Loss: (0.1643) | Acc: (94.35%) (18236/19328)\n",
            "Epoch: 85 | Batch_idx: 160 |  Loss: (0.1647) | Acc: (94.38%) (19449/20608)\n",
            "Epoch: 85 | Batch_idx: 170 |  Loss: (0.1630) | Acc: (94.47%) (20677/21888)\n",
            "Epoch: 85 | Batch_idx: 180 |  Loss: (0.1633) | Acc: (94.47%) (21887/23168)\n",
            "Epoch: 85 | Batch_idx: 190 |  Loss: (0.1639) | Acc: (94.47%) (23095/24448)\n",
            "Epoch: 85 | Batch_idx: 200 |  Loss: (0.1629) | Acc: (94.52%) (24319/25728)\n",
            "Epoch: 85 | Batch_idx: 210 |  Loss: (0.1630) | Acc: (94.51%) (25525/27008)\n",
            "Epoch: 85 | Batch_idx: 220 |  Loss: (0.1632) | Acc: (94.52%) (26739/28288)\n",
            "Epoch: 85 | Batch_idx: 230 |  Loss: (0.1636) | Acc: (94.52%) (27947/29568)\n",
            "Epoch: 85 | Batch_idx: 240 |  Loss: (0.1638) | Acc: (94.50%) (29152/30848)\n",
            "Epoch: 85 | Batch_idx: 250 |  Loss: (0.1647) | Acc: (94.48%) (30355/32128)\n",
            "Epoch: 85 | Batch_idx: 260 |  Loss: (0.1645) | Acc: (94.50%) (31570/33408)\n",
            "Epoch: 85 | Batch_idx: 270 |  Loss: (0.1650) | Acc: (94.46%) (32767/34688)\n",
            "Epoch: 85 | Batch_idx: 280 |  Loss: (0.1651) | Acc: (94.46%) (33975/35968)\n",
            "Epoch: 85 | Batch_idx: 290 |  Loss: (0.1640) | Acc: (94.49%) (35196/37248)\n",
            "Epoch: 85 | Batch_idx: 300 |  Loss: (0.1631) | Acc: (94.52%) (36418/38528)\n",
            "Epoch: 85 | Batch_idx: 310 |  Loss: (0.1620) | Acc: (94.57%) (37647/39808)\n",
            "Epoch: 85 | Batch_idx: 320 |  Loss: (0.1616) | Acc: (94.58%) (38859/41088)\n",
            "Epoch: 85 | Batch_idx: 330 |  Loss: (0.1621) | Acc: (94.59%) (40077/42368)\n",
            "Epoch: 85 | Batch_idx: 340 |  Loss: (0.1618) | Acc: (94.62%) (41299/43648)\n",
            "Epoch: 85 | Batch_idx: 350 |  Loss: (0.1611) | Acc: (94.64%) (42518/44928)\n",
            "Epoch: 85 | Batch_idx: 360 |  Loss: (0.1607) | Acc: (94.66%) (43740/46208)\n",
            "Epoch: 85 | Batch_idx: 370 |  Loss: (0.1607) | Acc: (94.66%) (44953/47488)\n",
            "Epoch: 85 | Batch_idx: 380 |  Loss: (0.1609) | Acc: (94.65%) (46158/48768)\n",
            "Epoch: 85 | Batch_idx: 390 |  Loss: (0.1613) | Acc: (94.64%) (47318/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3008) | Acc: (90.69%) (9069/10000)\n",
            "Epoch: 86 | Batch_idx: 0 |  Loss: (0.3044) | Acc: (89.84%) (115/128)\n",
            "Epoch: 86 | Batch_idx: 10 |  Loss: (0.1456) | Acc: (95.45%) (1344/1408)\n",
            "Epoch: 86 | Batch_idx: 20 |  Loss: (0.1707) | Acc: (94.68%) (2545/2688)\n",
            "Epoch: 86 | Batch_idx: 30 |  Loss: (0.1643) | Acc: (94.71%) (3758/3968)\n",
            "Epoch: 86 | Batch_idx: 40 |  Loss: (0.1609) | Acc: (94.80%) (4975/5248)\n",
            "Epoch: 86 | Batch_idx: 50 |  Loss: (0.1583) | Acc: (94.82%) (6190/6528)\n",
            "Epoch: 86 | Batch_idx: 60 |  Loss: (0.1579) | Acc: (94.76%) (7399/7808)\n",
            "Epoch: 86 | Batch_idx: 70 |  Loss: (0.1599) | Acc: (94.72%) (8608/9088)\n",
            "Epoch: 86 | Batch_idx: 80 |  Loss: (0.1585) | Acc: (94.65%) (9813/10368)\n",
            "Epoch: 86 | Batch_idx: 90 |  Loss: (0.1581) | Acc: (94.62%) (11021/11648)\n",
            "Epoch: 86 | Batch_idx: 100 |  Loss: (0.1601) | Acc: (94.56%) (12225/12928)\n",
            "Epoch: 86 | Batch_idx: 110 |  Loss: (0.1612) | Acc: (94.54%) (13432/14208)\n",
            "Epoch: 86 | Batch_idx: 120 |  Loss: (0.1601) | Acc: (94.54%) (14642/15488)\n",
            "Epoch: 86 | Batch_idx: 130 |  Loss: (0.1593) | Acc: (94.53%) (15851/16768)\n",
            "Epoch: 86 | Batch_idx: 140 |  Loss: (0.1590) | Acc: (94.59%) (17071/18048)\n",
            "Epoch: 86 | Batch_idx: 150 |  Loss: (0.1594) | Acc: (94.59%) (18283/19328)\n",
            "Epoch: 86 | Batch_idx: 160 |  Loss: (0.1602) | Acc: (94.55%) (19485/20608)\n",
            "Epoch: 86 | Batch_idx: 170 |  Loss: (0.1606) | Acc: (94.54%) (20692/21888)\n",
            "Epoch: 86 | Batch_idx: 180 |  Loss: (0.1590) | Acc: (94.57%) (21911/23168)\n",
            "Epoch: 86 | Batch_idx: 190 |  Loss: (0.1584) | Acc: (94.57%) (23121/24448)\n",
            "Epoch: 86 | Batch_idx: 200 |  Loss: (0.1582) | Acc: (94.61%) (24340/25728)\n",
            "Epoch: 86 | Batch_idx: 210 |  Loss: (0.1589) | Acc: (94.59%) (25547/27008)\n",
            "Epoch: 86 | Batch_idx: 220 |  Loss: (0.1589) | Acc: (94.58%) (26756/28288)\n",
            "Epoch: 86 | Batch_idx: 230 |  Loss: (0.1595) | Acc: (94.57%) (27961/29568)\n",
            "Epoch: 86 | Batch_idx: 240 |  Loss: (0.1589) | Acc: (94.59%) (29180/30848)\n",
            "Epoch: 86 | Batch_idx: 250 |  Loss: (0.1591) | Acc: (94.62%) (30398/32128)\n",
            "Epoch: 86 | Batch_idx: 260 |  Loss: (0.1589) | Acc: (94.62%) (31611/33408)\n",
            "Epoch: 86 | Batch_idx: 270 |  Loss: (0.1593) | Acc: (94.58%) (32808/34688)\n",
            "Epoch: 86 | Batch_idx: 280 |  Loss: (0.1587) | Acc: (94.60%) (34026/35968)\n",
            "Epoch: 86 | Batch_idx: 290 |  Loss: (0.1590) | Acc: (94.58%) (35231/37248)\n",
            "Epoch: 86 | Batch_idx: 300 |  Loss: (0.1598) | Acc: (94.55%) (36428/38528)\n",
            "Epoch: 86 | Batch_idx: 310 |  Loss: (0.1604) | Acc: (94.53%) (37632/39808)\n",
            "Epoch: 86 | Batch_idx: 320 |  Loss: (0.1603) | Acc: (94.54%) (38844/41088)\n",
            "Epoch: 86 | Batch_idx: 330 |  Loss: (0.1610) | Acc: (94.52%) (40045/42368)\n",
            "Epoch: 86 | Batch_idx: 340 |  Loss: (0.1611) | Acc: (94.51%) (41251/43648)\n",
            "Epoch: 86 | Batch_idx: 350 |  Loss: (0.1606) | Acc: (94.52%) (42464/44928)\n",
            "Epoch: 86 | Batch_idx: 360 |  Loss: (0.1600) | Acc: (94.54%) (43685/46208)\n",
            "Epoch: 86 | Batch_idx: 370 |  Loss: (0.1605) | Acc: (94.53%) (44889/47488)\n",
            "Epoch: 86 | Batch_idx: 380 |  Loss: (0.1601) | Acc: (94.55%) (46109/48768)\n",
            "Epoch: 86 | Batch_idx: 390 |  Loss: (0.1599) | Acc: (94.55%) (47277/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2964) | Acc: (91.11%) (9111/10000)\n",
            "Epoch: 87 | Batch_idx: 0 |  Loss: (0.2270) | Acc: (92.97%) (119/128)\n",
            "Epoch: 87 | Batch_idx: 10 |  Loss: (0.1609) | Acc: (95.03%) (1338/1408)\n",
            "Epoch: 87 | Batch_idx: 20 |  Loss: (0.1438) | Acc: (95.28%) (2561/2688)\n",
            "Epoch: 87 | Batch_idx: 30 |  Loss: (0.1406) | Acc: (95.34%) (3783/3968)\n",
            "Epoch: 87 | Batch_idx: 40 |  Loss: (0.1432) | Acc: (95.24%) (4998/5248)\n",
            "Epoch: 87 | Batch_idx: 50 |  Loss: (0.1480) | Acc: (95.04%) (6204/6528)\n",
            "Epoch: 87 | Batch_idx: 60 |  Loss: (0.1445) | Acc: (95.26%) (7438/7808)\n",
            "Epoch: 87 | Batch_idx: 70 |  Loss: (0.1489) | Acc: (95.14%) (8646/9088)\n",
            "Epoch: 87 | Batch_idx: 80 |  Loss: (0.1492) | Acc: (95.09%) (9859/10368)\n",
            "Epoch: 87 | Batch_idx: 90 |  Loss: (0.1512) | Acc: (95.05%) (11071/11648)\n",
            "Epoch: 87 | Batch_idx: 100 |  Loss: (0.1515) | Acc: (95.08%) (12292/12928)\n",
            "Epoch: 87 | Batch_idx: 110 |  Loss: (0.1540) | Acc: (94.94%) (13489/14208)\n",
            "Epoch: 87 | Batch_idx: 120 |  Loss: (0.1536) | Acc: (94.98%) (14710/15488)\n",
            "Epoch: 87 | Batch_idx: 130 |  Loss: (0.1512) | Acc: (95.03%) (15934/16768)\n",
            "Epoch: 87 | Batch_idx: 140 |  Loss: (0.1518) | Acc: (95.01%) (17148/18048)\n",
            "Epoch: 87 | Batch_idx: 150 |  Loss: (0.1518) | Acc: (94.99%) (18360/19328)\n",
            "Epoch: 87 | Batch_idx: 160 |  Loss: (0.1526) | Acc: (95.01%) (19579/20608)\n",
            "Epoch: 87 | Batch_idx: 170 |  Loss: (0.1526) | Acc: (94.99%) (20791/21888)\n",
            "Epoch: 87 | Batch_idx: 180 |  Loss: (0.1551) | Acc: (94.90%) (21986/23168)\n",
            "Epoch: 87 | Batch_idx: 190 |  Loss: (0.1565) | Acc: (94.85%) (23188/24448)\n",
            "Epoch: 87 | Batch_idx: 200 |  Loss: (0.1562) | Acc: (94.86%) (24405/25728)\n",
            "Epoch: 87 | Batch_idx: 210 |  Loss: (0.1552) | Acc: (94.88%) (25626/27008)\n",
            "Epoch: 87 | Batch_idx: 220 |  Loss: (0.1549) | Acc: (94.88%) (26840/28288)\n",
            "Epoch: 87 | Batch_idx: 230 |  Loss: (0.1554) | Acc: (94.85%) (28045/29568)\n",
            "Epoch: 87 | Batch_idx: 240 |  Loss: (0.1549) | Acc: (94.87%) (29264/30848)\n",
            "Epoch: 87 | Batch_idx: 250 |  Loss: (0.1548) | Acc: (94.85%) (30475/32128)\n",
            "Epoch: 87 | Batch_idx: 260 |  Loss: (0.1552) | Acc: (94.83%) (31682/33408)\n",
            "Epoch: 87 | Batch_idx: 270 |  Loss: (0.1559) | Acc: (94.81%) (32887/34688)\n",
            "Epoch: 87 | Batch_idx: 280 |  Loss: (0.1555) | Acc: (94.81%) (34103/35968)\n",
            "Epoch: 87 | Batch_idx: 290 |  Loss: (0.1563) | Acc: (94.79%) (35306/37248)\n",
            "Epoch: 87 | Batch_idx: 300 |  Loss: (0.1568) | Acc: (94.76%) (36511/38528)\n",
            "Epoch: 87 | Batch_idx: 310 |  Loss: (0.1566) | Acc: (94.76%) (37724/39808)\n",
            "Epoch: 87 | Batch_idx: 320 |  Loss: (0.1564) | Acc: (94.76%) (38937/41088)\n",
            "Epoch: 87 | Batch_idx: 330 |  Loss: (0.1568) | Acc: (94.77%) (40152/42368)\n",
            "Epoch: 87 | Batch_idx: 340 |  Loss: (0.1566) | Acc: (94.76%) (41361/43648)\n",
            "Epoch: 87 | Batch_idx: 350 |  Loss: (0.1559) | Acc: (94.78%) (42585/44928)\n",
            "Epoch: 87 | Batch_idx: 360 |  Loss: (0.1553) | Acc: (94.81%) (43808/46208)\n",
            "Epoch: 87 | Batch_idx: 370 |  Loss: (0.1554) | Acc: (94.81%) (45021/47488)\n",
            "Epoch: 87 | Batch_idx: 380 |  Loss: (0.1547) | Acc: (94.82%) (46242/48768)\n",
            "Epoch: 87 | Batch_idx: 390 |  Loss: (0.1555) | Acc: (94.80%) (47399/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3003) | Acc: (90.93%) (9093/10000)\n",
            "Epoch: 88 | Batch_idx: 0 |  Loss: (0.3120) | Acc: (89.84%) (115/128)\n",
            "Epoch: 88 | Batch_idx: 10 |  Loss: (0.1756) | Acc: (93.75%) (1320/1408)\n",
            "Epoch: 88 | Batch_idx: 20 |  Loss: (0.1632) | Acc: (94.35%) (2536/2688)\n",
            "Epoch: 88 | Batch_idx: 30 |  Loss: (0.1647) | Acc: (94.23%) (3739/3968)\n",
            "Epoch: 88 | Batch_idx: 40 |  Loss: (0.1600) | Acc: (94.49%) (4959/5248)\n",
            "Epoch: 88 | Batch_idx: 50 |  Loss: (0.1593) | Acc: (94.59%) (6175/6528)\n",
            "Epoch: 88 | Batch_idx: 60 |  Loss: (0.1596) | Acc: (94.70%) (7394/7808)\n",
            "Epoch: 88 | Batch_idx: 70 |  Loss: (0.1596) | Acc: (94.77%) (8613/9088)\n",
            "Epoch: 88 | Batch_idx: 80 |  Loss: (0.1548) | Acc: (94.93%) (9842/10368)\n",
            "Epoch: 88 | Batch_idx: 90 |  Loss: (0.1535) | Acc: (94.94%) (11059/11648)\n",
            "Epoch: 88 | Batch_idx: 100 |  Loss: (0.1520) | Acc: (94.99%) (12280/12928)\n",
            "Epoch: 88 | Batch_idx: 110 |  Loss: (0.1523) | Acc: (94.97%) (13494/14208)\n",
            "Epoch: 88 | Batch_idx: 120 |  Loss: (0.1504) | Acc: (95.06%) (14723/15488)\n",
            "Epoch: 88 | Batch_idx: 130 |  Loss: (0.1519) | Acc: (94.92%) (15917/16768)\n",
            "Epoch: 88 | Batch_idx: 140 |  Loss: (0.1507) | Acc: (94.96%) (17139/18048)\n",
            "Epoch: 88 | Batch_idx: 150 |  Loss: (0.1517) | Acc: (94.96%) (18354/19328)\n",
            "Epoch: 88 | Batch_idx: 160 |  Loss: (0.1520) | Acc: (94.93%) (19563/20608)\n",
            "Epoch: 88 | Batch_idx: 170 |  Loss: (0.1511) | Acc: (94.93%) (20778/21888)\n",
            "Epoch: 88 | Batch_idx: 180 |  Loss: (0.1524) | Acc: (94.87%) (21979/23168)\n",
            "Epoch: 88 | Batch_idx: 190 |  Loss: (0.1518) | Acc: (94.89%) (23198/24448)\n",
            "Epoch: 88 | Batch_idx: 200 |  Loss: (0.1506) | Acc: (94.92%) (24422/25728)\n",
            "Epoch: 88 | Batch_idx: 210 |  Loss: (0.1497) | Acc: (94.96%) (25647/27008)\n",
            "Epoch: 88 | Batch_idx: 220 |  Loss: (0.1491) | Acc: (94.98%) (26867/28288)\n",
            "Epoch: 88 | Batch_idx: 230 |  Loss: (0.1496) | Acc: (94.93%) (28069/29568)\n",
            "Epoch: 88 | Batch_idx: 240 |  Loss: (0.1502) | Acc: (94.89%) (29271/30848)\n",
            "Epoch: 88 | Batch_idx: 250 |  Loss: (0.1498) | Acc: (94.91%) (30492/32128)\n",
            "Epoch: 88 | Batch_idx: 260 |  Loss: (0.1515) | Acc: (94.88%) (31697/33408)\n",
            "Epoch: 88 | Batch_idx: 270 |  Loss: (0.1508) | Acc: (94.87%) (32910/34688)\n",
            "Epoch: 88 | Batch_idx: 280 |  Loss: (0.1510) | Acc: (94.87%) (34122/35968)\n",
            "Epoch: 88 | Batch_idx: 290 |  Loss: (0.1514) | Acc: (94.86%) (35333/37248)\n",
            "Epoch: 88 | Batch_idx: 300 |  Loss: (0.1520) | Acc: (94.82%) (36534/38528)\n",
            "Epoch: 88 | Batch_idx: 310 |  Loss: (0.1521) | Acc: (94.83%) (37749/39808)\n",
            "Epoch: 88 | Batch_idx: 320 |  Loss: (0.1524) | Acc: (94.81%) (38954/41088)\n",
            "Epoch: 88 | Batch_idx: 330 |  Loss: (0.1522) | Acc: (94.82%) (40174/42368)\n",
            "Epoch: 88 | Batch_idx: 340 |  Loss: (0.1522) | Acc: (94.82%) (41386/43648)\n",
            "Epoch: 88 | Batch_idx: 350 |  Loss: (0.1518) | Acc: (94.83%) (42604/44928)\n",
            "Epoch: 88 | Batch_idx: 360 |  Loss: (0.1520) | Acc: (94.81%) (43811/46208)\n",
            "Epoch: 88 | Batch_idx: 370 |  Loss: (0.1520) | Acc: (94.81%) (45024/47488)\n",
            "Epoch: 88 | Batch_idx: 380 |  Loss: (0.1517) | Acc: (94.81%) (46235/48768)\n",
            "Epoch: 88 | Batch_idx: 390 |  Loss: (0.1514) | Acc: (94.82%) (47409/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2951) | Acc: (91.04%) (9104/10000)\n",
            "Epoch: 89 | Batch_idx: 0 |  Loss: (0.0307) | Acc: (99.22%) (127/128)\n",
            "Epoch: 89 | Batch_idx: 10 |  Loss: (0.1399) | Acc: (95.03%) (1338/1408)\n",
            "Epoch: 89 | Batch_idx: 20 |  Loss: (0.1392) | Acc: (94.94%) (2552/2688)\n",
            "Epoch: 89 | Batch_idx: 30 |  Loss: (0.1422) | Acc: (95.14%) (3775/3968)\n",
            "Epoch: 89 | Batch_idx: 40 |  Loss: (0.1444) | Acc: (94.95%) (4983/5248)\n",
            "Epoch: 89 | Batch_idx: 50 |  Loss: (0.1417) | Acc: (94.98%) (6200/6528)\n",
            "Epoch: 89 | Batch_idx: 60 |  Loss: (0.1433) | Acc: (94.92%) (7411/7808)\n",
            "Epoch: 89 | Batch_idx: 70 |  Loss: (0.1447) | Acc: (94.82%) (8617/9088)\n",
            "Epoch: 89 | Batch_idx: 80 |  Loss: (0.1455) | Acc: (94.78%) (9827/10368)\n",
            "Epoch: 89 | Batch_idx: 90 |  Loss: (0.1442) | Acc: (94.79%) (11041/11648)\n",
            "Epoch: 89 | Batch_idx: 100 |  Loss: (0.1436) | Acc: (94.86%) (12264/12928)\n",
            "Epoch: 89 | Batch_idx: 110 |  Loss: (0.1417) | Acc: (94.98%) (13495/14208)\n",
            "Epoch: 89 | Batch_idx: 120 |  Loss: (0.1397) | Acc: (95.09%) (14728/15488)\n",
            "Epoch: 89 | Batch_idx: 130 |  Loss: (0.1406) | Acc: (95.08%) (15943/16768)\n",
            "Epoch: 89 | Batch_idx: 140 |  Loss: (0.1416) | Acc: (95.07%) (17159/18048)\n",
            "Epoch: 89 | Batch_idx: 150 |  Loss: (0.1414) | Acc: (95.08%) (18377/19328)\n",
            "Epoch: 89 | Batch_idx: 160 |  Loss: (0.1419) | Acc: (95.07%) (19593/20608)\n",
            "Epoch: 89 | Batch_idx: 170 |  Loss: (0.1420) | Acc: (95.12%) (20820/21888)\n",
            "Epoch: 89 | Batch_idx: 180 |  Loss: (0.1412) | Acc: (95.16%) (22046/23168)\n",
            "Epoch: 89 | Batch_idx: 190 |  Loss: (0.1415) | Acc: (95.17%) (23266/24448)\n",
            "Epoch: 89 | Batch_idx: 200 |  Loss: (0.1404) | Acc: (95.16%) (24483/25728)\n",
            "Epoch: 89 | Batch_idx: 210 |  Loss: (0.1395) | Acc: (95.20%) (25712/27008)\n",
            "Epoch: 89 | Batch_idx: 220 |  Loss: (0.1400) | Acc: (95.15%) (26915/28288)\n",
            "Epoch: 89 | Batch_idx: 230 |  Loss: (0.1401) | Acc: (95.16%) (28136/29568)\n",
            "Epoch: 89 | Batch_idx: 240 |  Loss: (0.1394) | Acc: (95.17%) (29357/30848)\n",
            "Epoch: 89 | Batch_idx: 250 |  Loss: (0.1397) | Acc: (95.16%) (30574/32128)\n",
            "Epoch: 89 | Batch_idx: 260 |  Loss: (0.1402) | Acc: (95.16%) (31792/33408)\n",
            "Epoch: 89 | Batch_idx: 270 |  Loss: (0.1400) | Acc: (95.17%) (33013/34688)\n",
            "Epoch: 89 | Batch_idx: 280 |  Loss: (0.1400) | Acc: (95.18%) (34233/35968)\n",
            "Epoch: 89 | Batch_idx: 290 |  Loss: (0.1411) | Acc: (95.14%) (35436/37248)\n",
            "Epoch: 89 | Batch_idx: 300 |  Loss: (0.1411) | Acc: (95.14%) (36654/38528)\n",
            "Epoch: 89 | Batch_idx: 310 |  Loss: (0.1402) | Acc: (95.17%) (37885/39808)\n",
            "Epoch: 89 | Batch_idx: 320 |  Loss: (0.1409) | Acc: (95.15%) (39096/41088)\n",
            "Epoch: 89 | Batch_idx: 330 |  Loss: (0.1412) | Acc: (95.15%) (40315/42368)\n",
            "Epoch: 89 | Batch_idx: 340 |  Loss: (0.1411) | Acc: (95.16%) (41536/43648)\n",
            "Epoch: 89 | Batch_idx: 350 |  Loss: (0.1413) | Acc: (95.15%) (42747/44928)\n",
            "Epoch: 89 | Batch_idx: 360 |  Loss: (0.1418) | Acc: (95.13%) (43958/46208)\n",
            "Epoch: 89 | Batch_idx: 370 |  Loss: (0.1424) | Acc: (95.11%) (45168/47488)\n",
            "Epoch: 89 | Batch_idx: 380 |  Loss: (0.1423) | Acc: (95.13%) (46393/48768)\n",
            "Epoch: 89 | Batch_idx: 390 |  Loss: (0.1420) | Acc: (95.13%) (47565/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3086) | Acc: (90.79%) (9079/10000)\n",
            "Epoch: 90 | Batch_idx: 0 |  Loss: (0.1060) | Acc: (97.66%) (125/128)\n",
            "Epoch: 90 | Batch_idx: 10 |  Loss: (0.1592) | Acc: (95.17%) (1340/1408)\n",
            "Epoch: 90 | Batch_idx: 20 |  Loss: (0.1517) | Acc: (95.39%) (2564/2688)\n",
            "Epoch: 90 | Batch_idx: 30 |  Loss: (0.1436) | Acc: (95.72%) (3798/3968)\n",
            "Epoch: 90 | Batch_idx: 40 |  Loss: (0.1361) | Acc: (95.73%) (5024/5248)\n",
            "Epoch: 90 | Batch_idx: 50 |  Loss: (0.1314) | Acc: (95.62%) (6242/6528)\n",
            "Epoch: 90 | Batch_idx: 60 |  Loss: (0.1314) | Acc: (95.61%) (7465/7808)\n",
            "Epoch: 90 | Batch_idx: 70 |  Loss: (0.1380) | Acc: (95.31%) (8662/9088)\n",
            "Epoch: 90 | Batch_idx: 80 |  Loss: (0.1350) | Acc: (95.39%) (9890/10368)\n",
            "Epoch: 90 | Batch_idx: 90 |  Loss: (0.1362) | Acc: (95.31%) (11102/11648)\n",
            "Epoch: 90 | Batch_idx: 100 |  Loss: (0.1386) | Acc: (95.17%) (12303/12928)\n",
            "Epoch: 90 | Batch_idx: 110 |  Loss: (0.1393) | Acc: (95.15%) (13519/14208)\n",
            "Epoch: 90 | Batch_idx: 120 |  Loss: (0.1403) | Acc: (95.16%) (14738/15488)\n",
            "Epoch: 90 | Batch_idx: 130 |  Loss: (0.1422) | Acc: (95.13%) (15951/16768)\n",
            "Epoch: 90 | Batch_idx: 140 |  Loss: (0.1423) | Acc: (95.13%) (17169/18048)\n",
            "Epoch: 90 | Batch_idx: 150 |  Loss: (0.1434) | Acc: (95.11%) (18382/19328)\n",
            "Epoch: 90 | Batch_idx: 160 |  Loss: (0.1439) | Acc: (95.05%) (19587/20608)\n",
            "Epoch: 90 | Batch_idx: 170 |  Loss: (0.1435) | Acc: (95.07%) (20809/21888)\n",
            "Epoch: 90 | Batch_idx: 180 |  Loss: (0.1435) | Acc: (95.08%) (22028/23168)\n",
            "Epoch: 90 | Batch_idx: 190 |  Loss: (0.1436) | Acc: (95.08%) (23246/24448)\n",
            "Epoch: 90 | Batch_idx: 200 |  Loss: (0.1439) | Acc: (95.06%) (24457/25728)\n",
            "Epoch: 90 | Batch_idx: 210 |  Loss: (0.1441) | Acc: (95.06%) (25673/27008)\n",
            "Epoch: 90 | Batch_idx: 220 |  Loss: (0.1443) | Acc: (95.07%) (26892/28288)\n",
            "Epoch: 90 | Batch_idx: 230 |  Loss: (0.1433) | Acc: (95.10%) (28120/29568)\n",
            "Epoch: 90 | Batch_idx: 240 |  Loss: (0.1433) | Acc: (95.11%) (29338/30848)\n",
            "Epoch: 90 | Batch_idx: 250 |  Loss: (0.1426) | Acc: (95.14%) (30565/32128)\n",
            "Epoch: 90 | Batch_idx: 260 |  Loss: (0.1423) | Acc: (95.17%) (31796/33408)\n",
            "Epoch: 90 | Batch_idx: 270 |  Loss: (0.1416) | Acc: (95.20%) (33023/34688)\n",
            "Epoch: 90 | Batch_idx: 280 |  Loss: (0.1414) | Acc: (95.19%) (34239/35968)\n",
            "Epoch: 90 | Batch_idx: 290 |  Loss: (0.1419) | Acc: (95.17%) (35448/37248)\n",
            "Epoch: 90 | Batch_idx: 300 |  Loss: (0.1414) | Acc: (95.18%) (36672/38528)\n",
            "Epoch: 90 | Batch_idx: 310 |  Loss: (0.1411) | Acc: (95.18%) (37889/39808)\n",
            "Epoch: 90 | Batch_idx: 320 |  Loss: (0.1406) | Acc: (95.18%) (39108/41088)\n",
            "Epoch: 90 | Batch_idx: 330 |  Loss: (0.1403) | Acc: (95.19%) (40329/42368)\n",
            "Epoch: 90 | Batch_idx: 340 |  Loss: (0.1405) | Acc: (95.20%) (41553/43648)\n",
            "Epoch: 90 | Batch_idx: 350 |  Loss: (0.1400) | Acc: (95.21%) (42778/44928)\n",
            "Epoch: 90 | Batch_idx: 360 |  Loss: (0.1400) | Acc: (95.22%) (43997/46208)\n",
            "Epoch: 90 | Batch_idx: 370 |  Loss: (0.1396) | Acc: (95.24%) (45227/47488)\n",
            "Epoch: 90 | Batch_idx: 380 |  Loss: (0.1392) | Acc: (95.26%) (46455/48768)\n",
            "Epoch: 90 | Batch_idx: 390 |  Loss: (0.1387) | Acc: (95.26%) (47630/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3042) | Acc: (91.05%) (9105/10000)\n",
            "Epoch: 91 | Batch_idx: 0 |  Loss: (0.0747) | Acc: (97.66%) (125/128)\n",
            "Epoch: 91 | Batch_idx: 10 |  Loss: (0.1092) | Acc: (96.16%) (1354/1408)\n",
            "Epoch: 91 | Batch_idx: 20 |  Loss: (0.1187) | Acc: (95.68%) (2572/2688)\n",
            "Epoch: 91 | Batch_idx: 30 |  Loss: (0.1209) | Acc: (95.67%) (3796/3968)\n",
            "Epoch: 91 | Batch_idx: 40 |  Loss: (0.1254) | Acc: (95.64%) (5019/5248)\n",
            "Epoch: 91 | Batch_idx: 50 |  Loss: (0.1372) | Acc: (95.31%) (6222/6528)\n",
            "Epoch: 91 | Batch_idx: 60 |  Loss: (0.1377) | Acc: (95.33%) (7443/7808)\n",
            "Epoch: 91 | Batch_idx: 70 |  Loss: (0.1344) | Acc: (95.47%) (8676/9088)\n",
            "Epoch: 91 | Batch_idx: 80 |  Loss: (0.1333) | Acc: (95.54%) (9906/10368)\n",
            "Epoch: 91 | Batch_idx: 90 |  Loss: (0.1342) | Acc: (95.50%) (11124/11648)\n",
            "Epoch: 91 | Batch_idx: 100 |  Loss: (0.1354) | Acc: (95.43%) (12337/12928)\n",
            "Epoch: 91 | Batch_idx: 110 |  Loss: (0.1332) | Acc: (95.50%) (13569/14208)\n",
            "Epoch: 91 | Batch_idx: 120 |  Loss: (0.1355) | Acc: (95.40%) (14776/15488)\n",
            "Epoch: 91 | Batch_idx: 130 |  Loss: (0.1349) | Acc: (95.38%) (15994/16768)\n",
            "Epoch: 91 | Batch_idx: 140 |  Loss: (0.1371) | Acc: (95.36%) (17210/18048)\n",
            "Epoch: 91 | Batch_idx: 150 |  Loss: (0.1366) | Acc: (95.37%) (18434/19328)\n",
            "Epoch: 91 | Batch_idx: 160 |  Loss: (0.1371) | Acc: (95.37%) (19654/20608)\n",
            "Epoch: 91 | Batch_idx: 170 |  Loss: (0.1364) | Acc: (95.39%) (20880/21888)\n",
            "Epoch: 91 | Batch_idx: 180 |  Loss: (0.1386) | Acc: (95.33%) (22086/23168)\n",
            "Epoch: 91 | Batch_idx: 190 |  Loss: (0.1376) | Acc: (95.37%) (23316/24448)\n",
            "Epoch: 91 | Batch_idx: 200 |  Loss: (0.1369) | Acc: (95.35%) (24532/25728)\n",
            "Epoch: 91 | Batch_idx: 210 |  Loss: (0.1366) | Acc: (95.35%) (25752/27008)\n",
            "Epoch: 91 | Batch_idx: 220 |  Loss: (0.1361) | Acc: (95.37%) (26979/28288)\n",
            "Epoch: 91 | Batch_idx: 230 |  Loss: (0.1369) | Acc: (95.37%) (28200/29568)\n",
            "Epoch: 91 | Batch_idx: 240 |  Loss: (0.1375) | Acc: (95.35%) (29413/30848)\n",
            "Epoch: 91 | Batch_idx: 250 |  Loss: (0.1372) | Acc: (95.36%) (30638/32128)\n",
            "Epoch: 91 | Batch_idx: 260 |  Loss: (0.1379) | Acc: (95.33%) (31848/33408)\n",
            "Epoch: 91 | Batch_idx: 270 |  Loss: (0.1379) | Acc: (95.33%) (33068/34688)\n",
            "Epoch: 91 | Batch_idx: 280 |  Loss: (0.1371) | Acc: (95.37%) (34301/35968)\n",
            "Epoch: 91 | Batch_idx: 290 |  Loss: (0.1377) | Acc: (95.34%) (35514/37248)\n",
            "Epoch: 91 | Batch_idx: 300 |  Loss: (0.1388) | Acc: (95.32%) (36724/38528)\n",
            "Epoch: 91 | Batch_idx: 310 |  Loss: (0.1389) | Acc: (95.33%) (37948/39808)\n",
            "Epoch: 91 | Batch_idx: 320 |  Loss: (0.1390) | Acc: (95.31%) (39162/41088)\n",
            "Epoch: 91 | Batch_idx: 330 |  Loss: (0.1388) | Acc: (95.32%) (40385/42368)\n",
            "Epoch: 91 | Batch_idx: 340 |  Loss: (0.1395) | Acc: (95.29%) (41593/43648)\n",
            "Epoch: 91 | Batch_idx: 350 |  Loss: (0.1390) | Acc: (95.32%) (42826/44928)\n",
            "Epoch: 91 | Batch_idx: 360 |  Loss: (0.1390) | Acc: (95.33%) (44050/46208)\n",
            "Epoch: 91 | Batch_idx: 370 |  Loss: (0.1390) | Acc: (95.32%) (45266/47488)\n",
            "Epoch: 91 | Batch_idx: 380 |  Loss: (0.1394) | Acc: (95.30%) (46477/48768)\n",
            "Epoch: 91 | Batch_idx: 390 |  Loss: (0.1399) | Acc: (95.29%) (47647/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3064) | Acc: (90.93%) (9093/10000)\n",
            "Epoch: 92 | Batch_idx: 0 |  Loss: (0.2209) | Acc: (93.75%) (120/128)\n",
            "Epoch: 92 | Batch_idx: 10 |  Loss: (0.1453) | Acc: (95.24%) (1341/1408)\n",
            "Epoch: 92 | Batch_idx: 20 |  Loss: (0.1471) | Acc: (94.98%) (2553/2688)\n",
            "Epoch: 92 | Batch_idx: 30 |  Loss: (0.1429) | Acc: (95.06%) (3772/3968)\n",
            "Epoch: 92 | Batch_idx: 40 |  Loss: (0.1354) | Acc: (95.33%) (5003/5248)\n",
            "Epoch: 92 | Batch_idx: 50 |  Loss: (0.1327) | Acc: (95.45%) (6231/6528)\n",
            "Epoch: 92 | Batch_idx: 60 |  Loss: (0.1334) | Acc: (95.35%) (7445/7808)\n",
            "Epoch: 92 | Batch_idx: 70 |  Loss: (0.1309) | Acc: (95.44%) (8674/9088)\n",
            "Epoch: 92 | Batch_idx: 80 |  Loss: (0.1311) | Acc: (95.45%) (9896/10368)\n",
            "Epoch: 92 | Batch_idx: 90 |  Loss: (0.1323) | Acc: (95.39%) (11111/11648)\n",
            "Epoch: 92 | Batch_idx: 100 |  Loss: (0.1325) | Acc: (95.41%) (12335/12928)\n",
            "Epoch: 92 | Batch_idx: 110 |  Loss: (0.1310) | Acc: (95.52%) (13571/14208)\n",
            "Epoch: 92 | Batch_idx: 120 |  Loss: (0.1298) | Acc: (95.56%) (14800/15488)\n",
            "Epoch: 92 | Batch_idx: 130 |  Loss: (0.1306) | Acc: (95.54%) (16020/16768)\n",
            "Epoch: 92 | Batch_idx: 140 |  Loss: (0.1316) | Acc: (95.50%) (17235/18048)\n",
            "Epoch: 92 | Batch_idx: 150 |  Loss: (0.1335) | Acc: (95.46%) (18451/19328)\n",
            "Epoch: 92 | Batch_idx: 160 |  Loss: (0.1320) | Acc: (95.52%) (19684/20608)\n",
            "Epoch: 92 | Batch_idx: 170 |  Loss: (0.1317) | Acc: (95.55%) (20913/21888)\n",
            "Epoch: 92 | Batch_idx: 180 |  Loss: (0.1320) | Acc: (95.51%) (22128/23168)\n",
            "Epoch: 92 | Batch_idx: 190 |  Loss: (0.1318) | Acc: (95.50%) (23348/24448)\n",
            "Epoch: 92 | Batch_idx: 200 |  Loss: (0.1315) | Acc: (95.53%) (24577/25728)\n",
            "Epoch: 92 | Batch_idx: 210 |  Loss: (0.1312) | Acc: (95.52%) (25799/27008)\n",
            "Epoch: 92 | Batch_idx: 220 |  Loss: (0.1322) | Acc: (95.48%) (27010/28288)\n",
            "Epoch: 92 | Batch_idx: 230 |  Loss: (0.1325) | Acc: (95.47%) (28230/29568)\n",
            "Epoch: 92 | Batch_idx: 240 |  Loss: (0.1329) | Acc: (95.48%) (29453/30848)\n",
            "Epoch: 92 | Batch_idx: 250 |  Loss: (0.1336) | Acc: (95.47%) (30673/32128)\n",
            "Epoch: 92 | Batch_idx: 260 |  Loss: (0.1336) | Acc: (95.47%) (31895/33408)\n",
            "Epoch: 92 | Batch_idx: 270 |  Loss: (0.1337) | Acc: (95.46%) (33114/34688)\n",
            "Epoch: 92 | Batch_idx: 280 |  Loss: (0.1338) | Acc: (95.45%) (34332/35968)\n",
            "Epoch: 92 | Batch_idx: 290 |  Loss: (0.1344) | Acc: (95.44%) (35548/37248)\n",
            "Epoch: 92 | Batch_idx: 300 |  Loss: (0.1339) | Acc: (95.45%) (36774/38528)\n",
            "Epoch: 92 | Batch_idx: 310 |  Loss: (0.1339) | Acc: (95.46%) (38001/39808)\n",
            "Epoch: 92 | Batch_idx: 320 |  Loss: (0.1340) | Acc: (95.46%) (39221/41088)\n",
            "Epoch: 92 | Batch_idx: 330 |  Loss: (0.1340) | Acc: (95.46%) (40444/42368)\n",
            "Epoch: 92 | Batch_idx: 340 |  Loss: (0.1334) | Acc: (95.47%) (41671/43648)\n",
            "Epoch: 92 | Batch_idx: 350 |  Loss: (0.1335) | Acc: (95.47%) (42895/44928)\n",
            "Epoch: 92 | Batch_idx: 360 |  Loss: (0.1338) | Acc: (95.46%) (44109/46208)\n",
            "Epoch: 92 | Batch_idx: 370 |  Loss: (0.1339) | Acc: (95.47%) (45336/47488)\n",
            "Epoch: 92 | Batch_idx: 380 |  Loss: (0.1341) | Acc: (95.46%) (46555/48768)\n",
            "Epoch: 92 | Batch_idx: 390 |  Loss: (0.1350) | Acc: (95.44%) (47721/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3030) | Acc: (91.21%) (9121/10000)\n",
            "Epoch: 93 | Batch_idx: 0 |  Loss: (0.0929) | Acc: (95.31%) (122/128)\n",
            "Epoch: 93 | Batch_idx: 10 |  Loss: (0.1266) | Acc: (95.17%) (1340/1408)\n",
            "Epoch: 93 | Batch_idx: 20 |  Loss: (0.1344) | Acc: (95.46%) (2566/2688)\n",
            "Epoch: 93 | Batch_idx: 30 |  Loss: (0.1296) | Acc: (95.61%) (3794/3968)\n",
            "Epoch: 93 | Batch_idx: 40 |  Loss: (0.1294) | Acc: (95.66%) (5020/5248)\n",
            "Epoch: 93 | Batch_idx: 50 |  Loss: (0.1297) | Acc: (95.71%) (6248/6528)\n",
            "Epoch: 93 | Batch_idx: 60 |  Loss: (0.1255) | Acc: (95.84%) (7483/7808)\n",
            "Epoch: 93 | Batch_idx: 70 |  Loss: (0.1255) | Acc: (95.72%) (8699/9088)\n",
            "Epoch: 93 | Batch_idx: 80 |  Loss: (0.1290) | Acc: (95.59%) (9911/10368)\n",
            "Epoch: 93 | Batch_idx: 90 |  Loss: (0.1285) | Acc: (95.63%) (11139/11648)\n",
            "Epoch: 93 | Batch_idx: 100 |  Loss: (0.1324) | Acc: (95.52%) (12349/12928)\n",
            "Epoch: 93 | Batch_idx: 110 |  Loss: (0.1342) | Acc: (95.50%) (13569/14208)\n",
            "Epoch: 93 | Batch_idx: 120 |  Loss: (0.1342) | Acc: (95.50%) (14791/15488)\n",
            "Epoch: 93 | Batch_idx: 130 |  Loss: (0.1319) | Acc: (95.60%) (16031/16768)\n",
            "Epoch: 93 | Batch_idx: 140 |  Loss: (0.1302) | Acc: (95.63%) (17259/18048)\n",
            "Epoch: 93 | Batch_idx: 150 |  Loss: (0.1329) | Acc: (95.53%) (18465/19328)\n",
            "Epoch: 93 | Batch_idx: 160 |  Loss: (0.1332) | Acc: (95.52%) (19684/20608)\n",
            "Epoch: 93 | Batch_idx: 170 |  Loss: (0.1346) | Acc: (95.50%) (20904/21888)\n",
            "Epoch: 93 | Batch_idx: 180 |  Loss: (0.1347) | Acc: (95.49%) (22122/23168)\n",
            "Epoch: 93 | Batch_idx: 190 |  Loss: (0.1353) | Acc: (95.44%) (23332/24448)\n",
            "Epoch: 93 | Batch_idx: 200 |  Loss: (0.1353) | Acc: (95.41%) (24548/25728)\n",
            "Epoch: 93 | Batch_idx: 210 |  Loss: (0.1350) | Acc: (95.42%) (25771/27008)\n",
            "Epoch: 93 | Batch_idx: 220 |  Loss: (0.1349) | Acc: (95.44%) (26997/28288)\n",
            "Epoch: 93 | Batch_idx: 230 |  Loss: (0.1337) | Acc: (95.50%) (28236/29568)\n",
            "Epoch: 93 | Batch_idx: 240 |  Loss: (0.1334) | Acc: (95.49%) (29457/30848)\n",
            "Epoch: 93 | Batch_idx: 250 |  Loss: (0.1328) | Acc: (95.51%) (30685/32128)\n",
            "Epoch: 93 | Batch_idx: 260 |  Loss: (0.1336) | Acc: (95.50%) (31905/33408)\n",
            "Epoch: 93 | Batch_idx: 270 |  Loss: (0.1332) | Acc: (95.51%) (33130/34688)\n",
            "Epoch: 93 | Batch_idx: 280 |  Loss: (0.1336) | Acc: (95.52%) (34355/35968)\n",
            "Epoch: 93 | Batch_idx: 290 |  Loss: (0.1343) | Acc: (95.51%) (35574/37248)\n",
            "Epoch: 93 | Batch_idx: 300 |  Loss: (0.1339) | Acc: (95.52%) (36803/38528)\n",
            "Epoch: 93 | Batch_idx: 310 |  Loss: (0.1339) | Acc: (95.55%) (38036/39808)\n",
            "Epoch: 93 | Batch_idx: 320 |  Loss: (0.1341) | Acc: (95.54%) (39257/41088)\n",
            "Epoch: 93 | Batch_idx: 330 |  Loss: (0.1337) | Acc: (95.56%) (40486/42368)\n",
            "Epoch: 93 | Batch_idx: 340 |  Loss: (0.1334) | Acc: (95.58%) (41718/43648)\n",
            "Epoch: 93 | Batch_idx: 350 |  Loss: (0.1336) | Acc: (95.56%) (42932/44928)\n",
            "Epoch: 93 | Batch_idx: 360 |  Loss: (0.1340) | Acc: (95.55%) (44154/46208)\n",
            "Epoch: 93 | Batch_idx: 370 |  Loss: (0.1334) | Acc: (95.58%) (45389/47488)\n",
            "Epoch: 93 | Batch_idx: 380 |  Loss: (0.1331) | Acc: (95.59%) (46619/48768)\n",
            "Epoch: 93 | Batch_idx: 390 |  Loss: (0.1334) | Acc: (95.58%) (47792/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3033) | Acc: (91.40%) (9140/10000)\n",
            "Epoch: 94 | Batch_idx: 0 |  Loss: (0.1917) | Acc: (93.75%) (120/128)\n",
            "Epoch: 94 | Batch_idx: 10 |  Loss: (0.1238) | Acc: (95.95%) (1351/1408)\n",
            "Epoch: 94 | Batch_idx: 20 |  Loss: (0.1183) | Acc: (95.87%) (2577/2688)\n",
            "Epoch: 94 | Batch_idx: 30 |  Loss: (0.1176) | Acc: (95.92%) (3806/3968)\n",
            "Epoch: 94 | Batch_idx: 40 |  Loss: (0.1236) | Acc: (95.75%) (5025/5248)\n",
            "Epoch: 94 | Batch_idx: 50 |  Loss: (0.1290) | Acc: (95.51%) (6235/6528)\n",
            "Epoch: 94 | Batch_idx: 60 |  Loss: (0.1313) | Acc: (95.61%) (7465/7808)\n",
            "Epoch: 94 | Batch_idx: 70 |  Loss: (0.1276) | Acc: (95.74%) (8701/9088)\n",
            "Epoch: 94 | Batch_idx: 80 |  Loss: (0.1274) | Acc: (95.77%) (9929/10368)\n",
            "Epoch: 94 | Batch_idx: 90 |  Loss: (0.1274) | Acc: (95.74%) (11152/11648)\n",
            "Epoch: 94 | Batch_idx: 100 |  Loss: (0.1269) | Acc: (95.77%) (12381/12928)\n",
            "Epoch: 94 | Batch_idx: 110 |  Loss: (0.1267) | Acc: (95.76%) (13605/14208)\n",
            "Epoch: 94 | Batch_idx: 120 |  Loss: (0.1267) | Acc: (95.75%) (14830/15488)\n",
            "Epoch: 94 | Batch_idx: 130 |  Loss: (0.1267) | Acc: (95.72%) (16050/16768)\n",
            "Epoch: 94 | Batch_idx: 140 |  Loss: (0.1274) | Acc: (95.67%) (17266/18048)\n",
            "Epoch: 94 | Batch_idx: 150 |  Loss: (0.1269) | Acc: (95.66%) (18489/19328)\n",
            "Epoch: 94 | Batch_idx: 160 |  Loss: (0.1267) | Acc: (95.68%) (19718/20608)\n",
            "Epoch: 94 | Batch_idx: 170 |  Loss: (0.1272) | Acc: (95.69%) (20945/21888)\n",
            "Epoch: 94 | Batch_idx: 180 |  Loss: (0.1267) | Acc: (95.71%) (22175/23168)\n",
            "Epoch: 94 | Batch_idx: 190 |  Loss: (0.1258) | Acc: (95.73%) (23404/24448)\n",
            "Epoch: 94 | Batch_idx: 200 |  Loss: (0.1266) | Acc: (95.68%) (24616/25728)\n",
            "Epoch: 94 | Batch_idx: 210 |  Loss: (0.1253) | Acc: (95.72%) (25853/27008)\n",
            "Epoch: 94 | Batch_idx: 220 |  Loss: (0.1259) | Acc: (95.71%) (27074/28288)\n",
            "Epoch: 94 | Batch_idx: 230 |  Loss: (0.1257) | Acc: (95.73%) (28306/29568)\n",
            "Epoch: 94 | Batch_idx: 240 |  Loss: (0.1255) | Acc: (95.73%) (29531/30848)\n",
            "Epoch: 94 | Batch_idx: 250 |  Loss: (0.1245) | Acc: (95.76%) (30765/32128)\n",
            "Epoch: 94 | Batch_idx: 260 |  Loss: (0.1249) | Acc: (95.75%) (31987/33408)\n",
            "Epoch: 94 | Batch_idx: 270 |  Loss: (0.1247) | Acc: (95.77%) (33221/34688)\n",
            "Epoch: 94 | Batch_idx: 280 |  Loss: (0.1247) | Acc: (95.78%) (34450/35968)\n",
            "Epoch: 94 | Batch_idx: 290 |  Loss: (0.1248) | Acc: (95.77%) (35674/37248)\n",
            "Epoch: 94 | Batch_idx: 300 |  Loss: (0.1249) | Acc: (95.76%) (36896/38528)\n",
            "Epoch: 94 | Batch_idx: 310 |  Loss: (0.1248) | Acc: (95.76%) (38122/39808)\n",
            "Epoch: 94 | Batch_idx: 320 |  Loss: (0.1249) | Acc: (95.75%) (39342/41088)\n",
            "Epoch: 94 | Batch_idx: 330 |  Loss: (0.1258) | Acc: (95.74%) (40562/42368)\n",
            "Epoch: 94 | Batch_idx: 340 |  Loss: (0.1265) | Acc: (95.71%) (41774/43648)\n",
            "Epoch: 94 | Batch_idx: 350 |  Loss: (0.1277) | Acc: (95.67%) (42984/44928)\n",
            "Epoch: 94 | Batch_idx: 360 |  Loss: (0.1271) | Acc: (95.69%) (44218/46208)\n",
            "Epoch: 94 | Batch_idx: 370 |  Loss: (0.1273) | Acc: (95.68%) (45437/47488)\n",
            "Epoch: 94 | Batch_idx: 380 |  Loss: (0.1280) | Acc: (95.66%) (46651/48768)\n",
            "Epoch: 94 | Batch_idx: 390 |  Loss: (0.1281) | Acc: (95.65%) (47823/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2976) | Acc: (91.25%) (9125/10000)\n",
            "Epoch: 95 | Batch_idx: 0 |  Loss: (0.1121) | Acc: (97.66%) (125/128)\n",
            "Epoch: 95 | Batch_idx: 10 |  Loss: (0.1171) | Acc: (96.31%) (1356/1408)\n",
            "Epoch: 95 | Batch_idx: 20 |  Loss: (0.1282) | Acc: (95.98%) (2580/2688)\n",
            "Epoch: 95 | Batch_idx: 30 |  Loss: (0.1205) | Acc: (96.14%) (3815/3968)\n",
            "Epoch: 95 | Batch_idx: 40 |  Loss: (0.1167) | Acc: (96.13%) (5045/5248)\n",
            "Epoch: 95 | Batch_idx: 50 |  Loss: (0.1150) | Acc: (96.12%) (6275/6528)\n",
            "Epoch: 95 | Batch_idx: 60 |  Loss: (0.1178) | Acc: (95.91%) (7489/7808)\n",
            "Epoch: 95 | Batch_idx: 70 |  Loss: (0.1185) | Acc: (95.91%) (8716/9088)\n",
            "Epoch: 95 | Batch_idx: 80 |  Loss: (0.1159) | Acc: (95.98%) (9951/10368)\n",
            "Epoch: 95 | Batch_idx: 90 |  Loss: (0.1162) | Acc: (95.97%) (11179/11648)\n",
            "Epoch: 95 | Batch_idx: 100 |  Loss: (0.1174) | Acc: (95.95%) (12405/12928)\n",
            "Epoch: 95 | Batch_idx: 110 |  Loss: (0.1179) | Acc: (95.95%) (13632/14208)\n",
            "Epoch: 95 | Batch_idx: 120 |  Loss: (0.1182) | Acc: (95.97%) (14864/15488)\n",
            "Epoch: 95 | Batch_idx: 130 |  Loss: (0.1203) | Acc: (95.91%) (16083/16768)\n",
            "Epoch: 95 | Batch_idx: 140 |  Loss: (0.1213) | Acc: (95.84%) (17298/18048)\n",
            "Epoch: 95 | Batch_idx: 150 |  Loss: (0.1218) | Acc: (95.83%) (18522/19328)\n",
            "Epoch: 95 | Batch_idx: 160 |  Loss: (0.1221) | Acc: (95.83%) (19748/20608)\n",
            "Epoch: 95 | Batch_idx: 170 |  Loss: (0.1219) | Acc: (95.83%) (20975/21888)\n",
            "Epoch: 95 | Batch_idx: 180 |  Loss: (0.1221) | Acc: (95.83%) (22202/23168)\n",
            "Epoch: 95 | Batch_idx: 190 |  Loss: (0.1224) | Acc: (95.80%) (23422/24448)\n",
            "Epoch: 95 | Batch_idx: 200 |  Loss: (0.1216) | Acc: (95.83%) (24655/25728)\n",
            "Epoch: 95 | Batch_idx: 210 |  Loss: (0.1213) | Acc: (95.85%) (25886/27008)\n",
            "Epoch: 95 | Batch_idx: 220 |  Loss: (0.1222) | Acc: (95.80%) (27100/28288)\n",
            "Epoch: 95 | Batch_idx: 230 |  Loss: (0.1233) | Acc: (95.75%) (28310/29568)\n",
            "Epoch: 95 | Batch_idx: 240 |  Loss: (0.1228) | Acc: (95.75%) (29537/30848)\n",
            "Epoch: 95 | Batch_idx: 250 |  Loss: (0.1221) | Acc: (95.79%) (30777/32128)\n",
            "Epoch: 95 | Batch_idx: 260 |  Loss: (0.1226) | Acc: (95.79%) (32002/33408)\n",
            "Epoch: 95 | Batch_idx: 270 |  Loss: (0.1230) | Acc: (95.78%) (33224/34688)\n",
            "Epoch: 95 | Batch_idx: 280 |  Loss: (0.1240) | Acc: (95.75%) (34440/35968)\n",
            "Epoch: 95 | Batch_idx: 290 |  Loss: (0.1235) | Acc: (95.79%) (35678/37248)\n",
            "Epoch: 95 | Batch_idx: 300 |  Loss: (0.1237) | Acc: (95.78%) (36902/38528)\n",
            "Epoch: 95 | Batch_idx: 310 |  Loss: (0.1241) | Acc: (95.77%) (38123/39808)\n",
            "Epoch: 95 | Batch_idx: 320 |  Loss: (0.1244) | Acc: (95.74%) (39337/41088)\n",
            "Epoch: 95 | Batch_idx: 330 |  Loss: (0.1248) | Acc: (95.72%) (40555/42368)\n",
            "Epoch: 95 | Batch_idx: 340 |  Loss: (0.1244) | Acc: (95.72%) (41778/43648)\n",
            "Epoch: 95 | Batch_idx: 350 |  Loss: (0.1248) | Acc: (95.70%) (42998/44928)\n",
            "Epoch: 95 | Batch_idx: 360 |  Loss: (0.1250) | Acc: (95.69%) (44218/46208)\n",
            "Epoch: 95 | Batch_idx: 370 |  Loss: (0.1251) | Acc: (95.70%) (45445/47488)\n",
            "Epoch: 95 | Batch_idx: 380 |  Loss: (0.1255) | Acc: (95.69%) (46665/48768)\n",
            "Epoch: 95 | Batch_idx: 390 |  Loss: (0.1260) | Acc: (95.67%) (47835/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3058) | Acc: (91.08%) (9108/10000)\n",
            "Epoch: 96 | Batch_idx: 0 |  Loss: (0.0665) | Acc: (98.44%) (126/128)\n",
            "Epoch: 96 | Batch_idx: 10 |  Loss: (0.1127) | Acc: (96.31%) (1356/1408)\n",
            "Epoch: 96 | Batch_idx: 20 |  Loss: (0.1146) | Acc: (96.17%) (2585/2688)\n",
            "Epoch: 96 | Batch_idx: 30 |  Loss: (0.1255) | Acc: (95.84%) (3803/3968)\n",
            "Epoch: 96 | Batch_idx: 40 |  Loss: (0.1265) | Acc: (95.96%) (5036/5248)\n",
            "Epoch: 96 | Batch_idx: 50 |  Loss: (0.1276) | Acc: (95.89%) (6260/6528)\n",
            "Epoch: 96 | Batch_idx: 60 |  Loss: (0.1255) | Acc: (95.98%) (7494/7808)\n",
            "Epoch: 96 | Batch_idx: 70 |  Loss: (0.1230) | Acc: (95.98%) (8723/9088)\n",
            "Epoch: 96 | Batch_idx: 80 |  Loss: (0.1227) | Acc: (95.98%) (9951/10368)\n",
            "Epoch: 96 | Batch_idx: 90 |  Loss: (0.1222) | Acc: (96.00%) (11182/11648)\n",
            "Epoch: 96 | Batch_idx: 100 |  Loss: (0.1233) | Acc: (96.00%) (12411/12928)\n",
            "Epoch: 96 | Batch_idx: 110 |  Loss: (0.1238) | Acc: (95.95%) (13633/14208)\n",
            "Epoch: 96 | Batch_idx: 120 |  Loss: (0.1239) | Acc: (95.96%) (14863/15488)\n",
            "Epoch: 96 | Batch_idx: 130 |  Loss: (0.1219) | Acc: (96.02%) (16101/16768)\n",
            "Epoch: 96 | Batch_idx: 140 |  Loss: (0.1204) | Acc: (96.07%) (17339/18048)\n",
            "Epoch: 96 | Batch_idx: 150 |  Loss: (0.1197) | Acc: (96.09%) (18572/19328)\n",
            "Epoch: 96 | Batch_idx: 160 |  Loss: (0.1196) | Acc: (96.07%) (19799/20608)\n",
            "Epoch: 96 | Batch_idx: 170 |  Loss: (0.1191) | Acc: (96.06%) (21025/21888)\n",
            "Epoch: 96 | Batch_idx: 180 |  Loss: (0.1192) | Acc: (96.07%) (22258/23168)\n",
            "Epoch: 96 | Batch_idx: 190 |  Loss: (0.1183) | Acc: (96.11%) (23496/24448)\n",
            "Epoch: 96 | Batch_idx: 200 |  Loss: (0.1187) | Acc: (96.09%) (24722/25728)\n",
            "Epoch: 96 | Batch_idx: 210 |  Loss: (0.1194) | Acc: (96.08%) (25950/27008)\n",
            "Epoch: 96 | Batch_idx: 220 |  Loss: (0.1196) | Acc: (96.07%) (27176/28288)\n",
            "Epoch: 96 | Batch_idx: 230 |  Loss: (0.1199) | Acc: (96.04%) (28396/29568)\n",
            "Epoch: 96 | Batch_idx: 240 |  Loss: (0.1205) | Acc: (96.02%) (29620/30848)\n",
            "Epoch: 96 | Batch_idx: 250 |  Loss: (0.1218) | Acc: (95.97%) (30832/32128)\n",
            "Epoch: 96 | Batch_idx: 260 |  Loss: (0.1214) | Acc: (95.97%) (32060/33408)\n",
            "Epoch: 96 | Batch_idx: 270 |  Loss: (0.1208) | Acc: (95.98%) (33292/34688)\n",
            "Epoch: 96 | Batch_idx: 280 |  Loss: (0.1213) | Acc: (95.94%) (34509/35968)\n",
            "Epoch: 96 | Batch_idx: 290 |  Loss: (0.1207) | Acc: (95.96%) (35742/37248)\n",
            "Epoch: 96 | Batch_idx: 300 |  Loss: (0.1206) | Acc: (95.95%) (36969/38528)\n",
            "Epoch: 96 | Batch_idx: 310 |  Loss: (0.1201) | Acc: (95.97%) (38202/39808)\n",
            "Epoch: 96 | Batch_idx: 320 |  Loss: (0.1203) | Acc: (95.95%) (39422/41088)\n",
            "Epoch: 96 | Batch_idx: 330 |  Loss: (0.1200) | Acc: (95.95%) (40652/42368)\n",
            "Epoch: 96 | Batch_idx: 340 |  Loss: (0.1204) | Acc: (95.93%) (41873/43648)\n",
            "Epoch: 96 | Batch_idx: 350 |  Loss: (0.1206) | Acc: (95.92%) (43097/44928)\n",
            "Epoch: 96 | Batch_idx: 360 |  Loss: (0.1205) | Acc: (95.92%) (44321/46208)\n",
            "Epoch: 96 | Batch_idx: 370 |  Loss: (0.1210) | Acc: (95.90%) (45540/47488)\n",
            "Epoch: 96 | Batch_idx: 380 |  Loss: (0.1212) | Acc: (95.88%) (46761/48768)\n",
            "Epoch: 96 | Batch_idx: 390 |  Loss: (0.1211) | Acc: (95.90%) (47948/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3251) | Acc: (90.85%) (9085/10000)\n",
            "Epoch: 97 | Batch_idx: 0 |  Loss: (0.1465) | Acc: (94.53%) (121/128)\n",
            "Epoch: 97 | Batch_idx: 10 |  Loss: (0.1095) | Acc: (96.88%) (1364/1408)\n",
            "Epoch: 97 | Batch_idx: 20 |  Loss: (0.1158) | Acc: (96.09%) (2583/2688)\n",
            "Epoch: 97 | Batch_idx: 30 |  Loss: (0.1153) | Acc: (96.02%) (3810/3968)\n",
            "Epoch: 97 | Batch_idx: 40 |  Loss: (0.1090) | Acc: (96.30%) (5054/5248)\n",
            "Epoch: 97 | Batch_idx: 50 |  Loss: (0.1130) | Acc: (96.17%) (6278/6528)\n",
            "Epoch: 97 | Batch_idx: 60 |  Loss: (0.1119) | Acc: (96.18%) (7510/7808)\n",
            "Epoch: 97 | Batch_idx: 70 |  Loss: (0.1109) | Acc: (96.23%) (8745/9088)\n",
            "Epoch: 97 | Batch_idx: 80 |  Loss: (0.1111) | Acc: (96.23%) (9977/10368)\n",
            "Epoch: 97 | Batch_idx: 90 |  Loss: (0.1105) | Acc: (96.29%) (11216/11648)\n",
            "Epoch: 97 | Batch_idx: 100 |  Loss: (0.1130) | Acc: (96.24%) (12442/12928)\n",
            "Epoch: 97 | Batch_idx: 110 |  Loss: (0.1124) | Acc: (96.24%) (13674/14208)\n",
            "Epoch: 97 | Batch_idx: 120 |  Loss: (0.1143) | Acc: (96.20%) (14900/15488)\n",
            "Epoch: 97 | Batch_idx: 130 |  Loss: (0.1139) | Acc: (96.19%) (16129/16768)\n",
            "Epoch: 97 | Batch_idx: 140 |  Loss: (0.1125) | Acc: (96.22%) (17366/18048)\n",
            "Epoch: 97 | Batch_idx: 150 |  Loss: (0.1124) | Acc: (96.21%) (18595/19328)\n",
            "Epoch: 97 | Batch_idx: 160 |  Loss: (0.1114) | Acc: (96.24%) (19834/20608)\n",
            "Epoch: 97 | Batch_idx: 170 |  Loss: (0.1113) | Acc: (96.25%) (21068/21888)\n",
            "Epoch: 97 | Batch_idx: 180 |  Loss: (0.1120) | Acc: (96.20%) (22287/23168)\n",
            "Epoch: 97 | Batch_idx: 190 |  Loss: (0.1119) | Acc: (96.18%) (23515/24448)\n",
            "Epoch: 97 | Batch_idx: 200 |  Loss: (0.1124) | Acc: (96.16%) (24740/25728)\n",
            "Epoch: 97 | Batch_idx: 210 |  Loss: (0.1123) | Acc: (96.17%) (25974/27008)\n",
            "Epoch: 97 | Batch_idx: 220 |  Loss: (0.1124) | Acc: (96.19%) (27209/28288)\n",
            "Epoch: 97 | Batch_idx: 230 |  Loss: (0.1123) | Acc: (96.20%) (28445/29568)\n",
            "Epoch: 97 | Batch_idx: 240 |  Loss: (0.1128) | Acc: (96.17%) (29666/30848)\n",
            "Epoch: 97 | Batch_idx: 250 |  Loss: (0.1126) | Acc: (96.17%) (30897/32128)\n",
            "Epoch: 97 | Batch_idx: 260 |  Loss: (0.1118) | Acc: (96.18%) (32132/33408)\n",
            "Epoch: 97 | Batch_idx: 270 |  Loss: (0.1117) | Acc: (96.20%) (33369/34688)\n",
            "Epoch: 97 | Batch_idx: 280 |  Loss: (0.1119) | Acc: (96.18%) (34593/35968)\n",
            "Epoch: 97 | Batch_idx: 290 |  Loss: (0.1123) | Acc: (96.16%) (35816/37248)\n",
            "Epoch: 97 | Batch_idx: 300 |  Loss: (0.1120) | Acc: (96.17%) (37052/38528)\n",
            "Epoch: 97 | Batch_idx: 310 |  Loss: (0.1123) | Acc: (96.16%) (38280/39808)\n",
            "Epoch: 97 | Batch_idx: 320 |  Loss: (0.1123) | Acc: (96.16%) (39511/41088)\n",
            "Epoch: 97 | Batch_idx: 330 |  Loss: (0.1118) | Acc: (96.16%) (40743/42368)\n",
            "Epoch: 97 | Batch_idx: 340 |  Loss: (0.1127) | Acc: (96.15%) (41967/43648)\n",
            "Epoch: 97 | Batch_idx: 350 |  Loss: (0.1132) | Acc: (96.13%) (43190/44928)\n",
            "Epoch: 97 | Batch_idx: 360 |  Loss: (0.1135) | Acc: (96.13%) (44421/46208)\n",
            "Epoch: 97 | Batch_idx: 370 |  Loss: (0.1132) | Acc: (96.13%) (45652/47488)\n",
            "Epoch: 97 | Batch_idx: 380 |  Loss: (0.1129) | Acc: (96.14%) (46886/48768)\n",
            "Epoch: 97 | Batch_idx: 390 |  Loss: (0.1133) | Acc: (96.12%) (48062/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3163) | Acc: (91.12%) (9112/10000)\n",
            "Epoch: 98 | Batch_idx: 0 |  Loss: (0.0756) | Acc: (97.66%) (125/128)\n",
            "Epoch: 98 | Batch_idx: 10 |  Loss: (0.1183) | Acc: (96.02%) (1352/1408)\n",
            "Epoch: 98 | Batch_idx: 20 |  Loss: (0.1275) | Acc: (95.72%) (2573/2688)\n",
            "Epoch: 98 | Batch_idx: 30 |  Loss: (0.1214) | Acc: (95.82%) (3802/3968)\n",
            "Epoch: 98 | Batch_idx: 40 |  Loss: (0.1255) | Acc: (95.75%) (5025/5248)\n",
            "Epoch: 98 | Batch_idx: 50 |  Loss: (0.1226) | Acc: (95.88%) (6259/6528)\n",
            "Epoch: 98 | Batch_idx: 60 |  Loss: (0.1197) | Acc: (95.95%) (7492/7808)\n",
            "Epoch: 98 | Batch_idx: 70 |  Loss: (0.1173) | Acc: (96.06%) (8730/9088)\n",
            "Epoch: 98 | Batch_idx: 80 |  Loss: (0.1171) | Acc: (95.97%) (9950/10368)\n",
            "Epoch: 98 | Batch_idx: 90 |  Loss: (0.1177) | Acc: (96.03%) (11186/11648)\n",
            "Epoch: 98 | Batch_idx: 100 |  Loss: (0.1176) | Acc: (96.04%) (12416/12928)\n",
            "Epoch: 98 | Batch_idx: 110 |  Loss: (0.1186) | Acc: (96.04%) (13645/14208)\n",
            "Epoch: 98 | Batch_idx: 120 |  Loss: (0.1178) | Acc: (96.08%) (14881/15488)\n",
            "Epoch: 98 | Batch_idx: 130 |  Loss: (0.1175) | Acc: (96.12%) (16118/16768)\n",
            "Epoch: 98 | Batch_idx: 140 |  Loss: (0.1160) | Acc: (96.18%) (17358/18048)\n",
            "Epoch: 98 | Batch_idx: 150 |  Loss: (0.1160) | Acc: (96.19%) (18592/19328)\n",
            "Epoch: 98 | Batch_idx: 160 |  Loss: (0.1152) | Acc: (96.23%) (19831/20608)\n",
            "Epoch: 98 | Batch_idx: 170 |  Loss: (0.1145) | Acc: (96.23%) (21062/21888)\n",
            "Epoch: 98 | Batch_idx: 180 |  Loss: (0.1145) | Acc: (96.22%) (22293/23168)\n",
            "Epoch: 98 | Batch_idx: 190 |  Loss: (0.1146) | Acc: (96.20%) (23519/24448)\n",
            "Epoch: 98 | Batch_idx: 200 |  Loss: (0.1140) | Acc: (96.21%) (24752/25728)\n",
            "Epoch: 98 | Batch_idx: 210 |  Loss: (0.1143) | Acc: (96.17%) (25973/27008)\n",
            "Epoch: 98 | Batch_idx: 220 |  Loss: (0.1143) | Acc: (96.16%) (27202/28288)\n",
            "Epoch: 98 | Batch_idx: 230 |  Loss: (0.1152) | Acc: (96.13%) (28424/29568)\n",
            "Epoch: 98 | Batch_idx: 240 |  Loss: (0.1157) | Acc: (96.12%) (29650/30848)\n",
            "Epoch: 98 | Batch_idx: 250 |  Loss: (0.1153) | Acc: (96.13%) (30886/32128)\n",
            "Epoch: 98 | Batch_idx: 260 |  Loss: (0.1148) | Acc: (96.15%) (32121/33408)\n",
            "Epoch: 98 | Batch_idx: 270 |  Loss: (0.1150) | Acc: (96.13%) (33347/34688)\n",
            "Epoch: 98 | Batch_idx: 280 |  Loss: (0.1154) | Acc: (96.13%) (34576/35968)\n",
            "Epoch: 98 | Batch_idx: 290 |  Loss: (0.1151) | Acc: (96.15%) (35815/37248)\n",
            "Epoch: 98 | Batch_idx: 300 |  Loss: (0.1154) | Acc: (96.12%) (37034/38528)\n",
            "Epoch: 98 | Batch_idx: 310 |  Loss: (0.1147) | Acc: (96.15%) (38274/39808)\n",
            "Epoch: 98 | Batch_idx: 320 |  Loss: (0.1151) | Acc: (96.14%) (39502/41088)\n",
            "Epoch: 98 | Batch_idx: 330 |  Loss: (0.1156) | Acc: (96.12%) (40724/42368)\n",
            "Epoch: 98 | Batch_idx: 340 |  Loss: (0.1156) | Acc: (96.10%) (41945/43648)\n",
            "Epoch: 98 | Batch_idx: 350 |  Loss: (0.1154) | Acc: (96.10%) (43175/44928)\n",
            "Epoch: 98 | Batch_idx: 360 |  Loss: (0.1149) | Acc: (96.11%) (44411/46208)\n",
            "Epoch: 98 | Batch_idx: 370 |  Loss: (0.1149) | Acc: (96.11%) (45639/47488)\n",
            "Epoch: 98 | Batch_idx: 380 |  Loss: (0.1149) | Acc: (96.11%) (46870/48768)\n",
            "Epoch: 98 | Batch_idx: 390 |  Loss: (0.1150) | Acc: (96.10%) (48050/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3245) | Acc: (90.85%) (9085/10000)\n",
            "Epoch: 99 | Batch_idx: 0 |  Loss: (0.1136) | Acc: (94.53%) (121/128)\n",
            "Epoch: 99 | Batch_idx: 10 |  Loss: (0.1306) | Acc: (95.74%) (1348/1408)\n",
            "Epoch: 99 | Batch_idx: 20 |  Loss: (0.1212) | Acc: (96.21%) (2586/2688)\n",
            "Epoch: 99 | Batch_idx: 30 |  Loss: (0.1239) | Acc: (95.97%) (3808/3968)\n",
            "Epoch: 99 | Batch_idx: 40 |  Loss: (0.1234) | Acc: (96.00%) (5038/5248)\n",
            "Epoch: 99 | Batch_idx: 50 |  Loss: (0.1235) | Acc: (96.02%) (6268/6528)\n",
            "Epoch: 99 | Batch_idx: 60 |  Loss: (0.1219) | Acc: (96.06%) (7500/7808)\n",
            "Epoch: 99 | Batch_idx: 70 |  Loss: (0.1191) | Acc: (96.12%) (8735/9088)\n",
            "Epoch: 99 | Batch_idx: 80 |  Loss: (0.1201) | Acc: (96.09%) (9963/10368)\n",
            "Epoch: 99 | Batch_idx: 90 |  Loss: (0.1204) | Acc: (96.05%) (11188/11648)\n",
            "Epoch: 99 | Batch_idx: 100 |  Loss: (0.1184) | Acc: (96.04%) (12416/12928)\n",
            "Epoch: 99 | Batch_idx: 110 |  Loss: (0.1188) | Acc: (96.06%) (13648/14208)\n",
            "Epoch: 99 | Batch_idx: 120 |  Loss: (0.1162) | Acc: (96.11%) (14886/15488)\n",
            "Epoch: 99 | Batch_idx: 130 |  Loss: (0.1157) | Acc: (96.17%) (16126/16768)\n",
            "Epoch: 99 | Batch_idx: 140 |  Loss: (0.1160) | Acc: (96.18%) (17359/18048)\n",
            "Epoch: 99 | Batch_idx: 150 |  Loss: (0.1141) | Acc: (96.25%) (18603/19328)\n",
            "Epoch: 99 | Batch_idx: 160 |  Loss: (0.1143) | Acc: (96.22%) (19830/20608)\n",
            "Epoch: 99 | Batch_idx: 170 |  Loss: (0.1147) | Acc: (96.25%) (21067/21888)\n",
            "Epoch: 99 | Batch_idx: 180 |  Loss: (0.1133) | Acc: (96.30%) (22310/23168)\n",
            "Epoch: 99 | Batch_idx: 190 |  Loss: (0.1138) | Acc: (96.27%) (23537/24448)\n",
            "Epoch: 99 | Batch_idx: 200 |  Loss: (0.1137) | Acc: (96.26%) (24765/25728)\n",
            "Epoch: 99 | Batch_idx: 210 |  Loss: (0.1142) | Acc: (96.23%) (25989/27008)\n",
            "Epoch: 99 | Batch_idx: 220 |  Loss: (0.1148) | Acc: (96.21%) (27217/28288)\n",
            "Epoch: 99 | Batch_idx: 230 |  Loss: (0.1140) | Acc: (96.24%) (28457/29568)\n",
            "Epoch: 99 | Batch_idx: 240 |  Loss: (0.1143) | Acc: (96.22%) (29682/30848)\n",
            "Epoch: 99 | Batch_idx: 250 |  Loss: (0.1141) | Acc: (96.22%) (30912/32128)\n",
            "Epoch: 99 | Batch_idx: 260 |  Loss: (0.1136) | Acc: (96.22%) (32146/33408)\n",
            "Epoch: 99 | Batch_idx: 270 |  Loss: (0.1144) | Acc: (96.19%) (33367/34688)\n",
            "Epoch: 99 | Batch_idx: 280 |  Loss: (0.1138) | Acc: (96.20%) (34601/35968)\n",
            "Epoch: 99 | Batch_idx: 290 |  Loss: (0.1137) | Acc: (96.21%) (35837/37248)\n",
            "Epoch: 99 | Batch_idx: 300 |  Loss: (0.1143) | Acc: (96.21%) (37066/38528)\n",
            "Epoch: 99 | Batch_idx: 310 |  Loss: (0.1147) | Acc: (96.21%) (38298/39808)\n",
            "Epoch: 99 | Batch_idx: 320 |  Loss: (0.1136) | Acc: (96.23%) (39540/41088)\n",
            "Epoch: 99 | Batch_idx: 330 |  Loss: (0.1133) | Acc: (96.22%) (40768/42368)\n",
            "Epoch: 99 | Batch_idx: 340 |  Loss: (0.1134) | Acc: (96.22%) (41998/43648)\n",
            "Epoch: 99 | Batch_idx: 350 |  Loss: (0.1136) | Acc: (96.21%) (43223/44928)\n",
            "Epoch: 99 | Batch_idx: 360 |  Loss: (0.1135) | Acc: (96.20%) (44451/46208)\n",
            "Epoch: 99 | Batch_idx: 370 |  Loss: (0.1138) | Acc: (96.19%) (45680/47488)\n",
            "Epoch: 99 | Batch_idx: 380 |  Loss: (0.1138) | Acc: (96.19%) (46911/48768)\n",
            "Epoch: 99 | Batch_idx: 390 |  Loss: (0.1142) | Acc: (96.19%) (48095/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3206) | Acc: (91.14%) (9114/10000)\n",
            "Epoch: 100 | Batch_idx: 0 |  Loss: (0.1549) | Acc: (94.53%) (121/128)\n",
            "Epoch: 100 | Batch_idx: 10 |  Loss: (0.1223) | Acc: (95.60%) (1346/1408)\n",
            "Epoch: 100 | Batch_idx: 20 |  Loss: (0.1168) | Acc: (96.02%) (2581/2688)\n",
            "Epoch: 100 | Batch_idx: 30 |  Loss: (0.1240) | Acc: (95.69%) (3797/3968)\n",
            "Epoch: 100 | Batch_idx: 40 |  Loss: (0.1214) | Acc: (95.87%) (5031/5248)\n",
            "Epoch: 100 | Batch_idx: 50 |  Loss: (0.1212) | Acc: (95.91%) (6261/6528)\n",
            "Epoch: 100 | Batch_idx: 60 |  Loss: (0.1193) | Acc: (95.98%) (7494/7808)\n",
            "Epoch: 100 | Batch_idx: 70 |  Loss: (0.1161) | Acc: (96.07%) (8731/9088)\n",
            "Epoch: 100 | Batch_idx: 80 |  Loss: (0.1166) | Acc: (95.94%) (9947/10368)\n",
            "Epoch: 100 | Batch_idx: 90 |  Loss: (0.1156) | Acc: (95.96%) (11177/11648)\n",
            "Epoch: 100 | Batch_idx: 100 |  Loss: (0.1176) | Acc: (95.87%) (12394/12928)\n",
            "Epoch: 100 | Batch_idx: 110 |  Loss: (0.1177) | Acc: (95.89%) (13624/14208)\n",
            "Epoch: 100 | Batch_idx: 120 |  Loss: (0.1169) | Acc: (95.91%) (14855/15488)\n",
            "Epoch: 100 | Batch_idx: 130 |  Loss: (0.1155) | Acc: (95.98%) (16094/16768)\n",
            "Epoch: 100 | Batch_idx: 140 |  Loss: (0.1168) | Acc: (95.97%) (17321/18048)\n",
            "Epoch: 100 | Batch_idx: 150 |  Loss: (0.1157) | Acc: (95.99%) (18553/19328)\n",
            "Epoch: 100 | Batch_idx: 160 |  Loss: (0.1166) | Acc: (95.96%) (19776/20608)\n",
            "Epoch: 100 | Batch_idx: 170 |  Loss: (0.1158) | Acc: (96.03%) (21020/21888)\n",
            "Epoch: 100 | Batch_idx: 180 |  Loss: (0.1155) | Acc: (96.05%) (22254/23168)\n",
            "Epoch: 100 | Batch_idx: 190 |  Loss: (0.1147) | Acc: (96.10%) (23494/24448)\n",
            "Epoch: 100 | Batch_idx: 200 |  Loss: (0.1153) | Acc: (96.07%) (24717/25728)\n",
            "Epoch: 100 | Batch_idx: 210 |  Loss: (0.1157) | Acc: (96.05%) (25942/27008)\n",
            "Epoch: 100 | Batch_idx: 220 |  Loss: (0.1156) | Acc: (96.07%) (27176/28288)\n",
            "Epoch: 100 | Batch_idx: 230 |  Loss: (0.1156) | Acc: (96.06%) (28403/29568)\n",
            "Epoch: 100 | Batch_idx: 240 |  Loss: (0.1146) | Acc: (96.10%) (29645/30848)\n",
            "Epoch: 100 | Batch_idx: 250 |  Loss: (0.1154) | Acc: (96.08%) (30870/32128)\n",
            "Epoch: 100 | Batch_idx: 260 |  Loss: (0.1155) | Acc: (96.07%) (32094/33408)\n",
            "Epoch: 100 | Batch_idx: 270 |  Loss: (0.1147) | Acc: (96.09%) (33331/34688)\n",
            "Epoch: 100 | Batch_idx: 280 |  Loss: (0.1147) | Acc: (96.09%) (34563/35968)\n",
            "Epoch: 100 | Batch_idx: 290 |  Loss: (0.1145) | Acc: (96.11%) (35798/37248)\n",
            "Epoch: 100 | Batch_idx: 300 |  Loss: (0.1143) | Acc: (96.11%) (37030/38528)\n",
            "Epoch: 100 | Batch_idx: 310 |  Loss: (0.1147) | Acc: (96.09%) (38252/39808)\n",
            "Epoch: 100 | Batch_idx: 320 |  Loss: (0.1144) | Acc: (96.11%) (39490/41088)\n",
            "Epoch: 100 | Batch_idx: 330 |  Loss: (0.1145) | Acc: (96.10%) (40716/42368)\n",
            "Epoch: 100 | Batch_idx: 340 |  Loss: (0.1142) | Acc: (96.09%) (41941/43648)\n",
            "Epoch: 100 | Batch_idx: 350 |  Loss: (0.1140) | Acc: (96.10%) (43176/44928)\n",
            "Epoch: 100 | Batch_idx: 360 |  Loss: (0.1133) | Acc: (96.12%) (44415/46208)\n",
            "Epoch: 100 | Batch_idx: 370 |  Loss: (0.1136) | Acc: (96.11%) (45639/47488)\n",
            "Epoch: 100 | Batch_idx: 380 |  Loss: (0.1147) | Acc: (96.08%) (46855/48768)\n",
            "Epoch: 100 | Batch_idx: 390 |  Loss: (0.1148) | Acc: (96.07%) (48033/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3105) | Acc: (91.32%) (9132/10000)\n",
            "Epoch: 101 | Batch_idx: 0 |  Loss: (0.0729) | Acc: (97.66%) (125/128)\n",
            "Epoch: 101 | Batch_idx: 10 |  Loss: (0.1136) | Acc: (96.02%) (1352/1408)\n",
            "Epoch: 101 | Batch_idx: 20 |  Loss: (0.1173) | Acc: (95.83%) (2576/2688)\n",
            "Epoch: 101 | Batch_idx: 30 |  Loss: (0.1196) | Acc: (95.82%) (3802/3968)\n",
            "Epoch: 101 | Batch_idx: 40 |  Loss: (0.1130) | Acc: (96.06%) (5041/5248)\n",
            "Epoch: 101 | Batch_idx: 50 |  Loss: (0.1142) | Acc: (96.11%) (6274/6528)\n",
            "Epoch: 101 | Batch_idx: 60 |  Loss: (0.1136) | Acc: (96.08%) (7502/7808)\n",
            "Epoch: 101 | Batch_idx: 70 |  Loss: (0.1153) | Acc: (96.09%) (8733/9088)\n",
            "Epoch: 101 | Batch_idx: 80 |  Loss: (0.1151) | Acc: (96.10%) (9964/10368)\n",
            "Epoch: 101 | Batch_idx: 90 |  Loss: (0.1155) | Acc: (96.12%) (11196/11648)\n",
            "Epoch: 101 | Batch_idx: 100 |  Loss: (0.1141) | Acc: (96.14%) (12429/12928)\n",
            "Epoch: 101 | Batch_idx: 110 |  Loss: (0.1126) | Acc: (96.19%) (13667/14208)\n",
            "Epoch: 101 | Batch_idx: 120 |  Loss: (0.1135) | Acc: (96.13%) (14889/15488)\n",
            "Epoch: 101 | Batch_idx: 130 |  Loss: (0.1141) | Acc: (96.08%) (16110/16768)\n",
            "Epoch: 101 | Batch_idx: 140 |  Loss: (0.1129) | Acc: (96.12%) (17348/18048)\n",
            "Epoch: 101 | Batch_idx: 150 |  Loss: (0.1122) | Acc: (96.12%) (18578/19328)\n",
            "Epoch: 101 | Batch_idx: 160 |  Loss: (0.1115) | Acc: (96.13%) (19811/20608)\n",
            "Epoch: 101 | Batch_idx: 170 |  Loss: (0.1116) | Acc: (96.18%) (21051/21888)\n",
            "Epoch: 101 | Batch_idx: 180 |  Loss: (0.1118) | Acc: (96.17%) (22281/23168)\n",
            "Epoch: 101 | Batch_idx: 190 |  Loss: (0.1105) | Acc: (96.22%) (23524/24448)\n",
            "Epoch: 101 | Batch_idx: 200 |  Loss: (0.1109) | Acc: (96.22%) (24755/25728)\n",
            "Epoch: 101 | Batch_idx: 210 |  Loss: (0.1103) | Acc: (96.22%) (25987/27008)\n",
            "Epoch: 101 | Batch_idx: 220 |  Loss: (0.1096) | Acc: (96.27%) (27232/28288)\n",
            "Epoch: 101 | Batch_idx: 230 |  Loss: (0.1084) | Acc: (96.30%) (28474/29568)\n",
            "Epoch: 101 | Batch_idx: 240 |  Loss: (0.1080) | Acc: (96.30%) (29708/30848)\n",
            "Epoch: 101 | Batch_idx: 250 |  Loss: (0.1083) | Acc: (96.30%) (30938/32128)\n",
            "Epoch: 101 | Batch_idx: 260 |  Loss: (0.1082) | Acc: (96.29%) (32169/33408)\n",
            "Epoch: 101 | Batch_idx: 270 |  Loss: (0.1080) | Acc: (96.31%) (33407/34688)\n",
            "Epoch: 101 | Batch_idx: 280 |  Loss: (0.1082) | Acc: (96.29%) (34634/35968)\n",
            "Epoch: 101 | Batch_idx: 290 |  Loss: (0.1080) | Acc: (96.30%) (35870/37248)\n",
            "Epoch: 101 | Batch_idx: 300 |  Loss: (0.1074) | Acc: (96.34%) (37116/38528)\n",
            "Epoch: 101 | Batch_idx: 310 |  Loss: (0.1076) | Acc: (96.32%) (38344/39808)\n",
            "Epoch: 101 | Batch_idx: 320 |  Loss: (0.1074) | Acc: (96.34%) (39583/41088)\n",
            "Epoch: 101 | Batch_idx: 330 |  Loss: (0.1068) | Acc: (96.36%) (40825/42368)\n",
            "Epoch: 101 | Batch_idx: 340 |  Loss: (0.1070) | Acc: (96.36%) (42058/43648)\n",
            "Epoch: 101 | Batch_idx: 350 |  Loss: (0.1068) | Acc: (96.35%) (43290/44928)\n",
            "Epoch: 101 | Batch_idx: 360 |  Loss: (0.1069) | Acc: (96.35%) (44523/46208)\n",
            "Epoch: 101 | Batch_idx: 370 |  Loss: (0.1079) | Acc: (96.33%) (45744/47488)\n",
            "Epoch: 101 | Batch_idx: 380 |  Loss: (0.1080) | Acc: (96.32%) (46974/48768)\n",
            "Epoch: 101 | Batch_idx: 390 |  Loss: (0.1077) | Acc: (96.33%) (48164/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3225) | Acc: (91.36%) (9136/10000)\n",
            "Epoch: 102 | Batch_idx: 0 |  Loss: (0.1120) | Acc: (95.31%) (122/128)\n",
            "Epoch: 102 | Batch_idx: 10 |  Loss: (0.1138) | Acc: (95.45%) (1344/1408)\n",
            "Epoch: 102 | Batch_idx: 20 |  Loss: (0.1124) | Acc: (95.76%) (2574/2688)\n",
            "Epoch: 102 | Batch_idx: 30 |  Loss: (0.1112) | Acc: (95.97%) (3808/3968)\n",
            "Epoch: 102 | Batch_idx: 40 |  Loss: (0.1123) | Acc: (95.87%) (5031/5248)\n",
            "Epoch: 102 | Batch_idx: 50 |  Loss: (0.1104) | Acc: (95.99%) (6266/6528)\n",
            "Epoch: 102 | Batch_idx: 60 |  Loss: (0.1074) | Acc: (96.08%) (7502/7808)\n",
            "Epoch: 102 | Batch_idx: 70 |  Loss: (0.1060) | Acc: (96.24%) (8746/9088)\n",
            "Epoch: 102 | Batch_idx: 80 |  Loss: (0.1069) | Acc: (96.27%) (9981/10368)\n",
            "Epoch: 102 | Batch_idx: 90 |  Loss: (0.1075) | Acc: (96.28%) (11215/11648)\n",
            "Epoch: 102 | Batch_idx: 100 |  Loss: (0.1089) | Acc: (96.24%) (12442/12928)\n",
            "Epoch: 102 | Batch_idx: 110 |  Loss: (0.1069) | Acc: (96.34%) (13688/14208)\n",
            "Epoch: 102 | Batch_idx: 120 |  Loss: (0.1056) | Acc: (96.38%) (14928/15488)\n",
            "Epoch: 102 | Batch_idx: 130 |  Loss: (0.1064) | Acc: (96.40%) (16164/16768)\n",
            "Epoch: 102 | Batch_idx: 140 |  Loss: (0.1056) | Acc: (96.42%) (17402/18048)\n",
            "Epoch: 102 | Batch_idx: 150 |  Loss: (0.1056) | Acc: (96.41%) (18634/19328)\n",
            "Epoch: 102 | Batch_idx: 160 |  Loss: (0.1061) | Acc: (96.39%) (19864/20608)\n",
            "Epoch: 102 | Batch_idx: 170 |  Loss: (0.1064) | Acc: (96.37%) (21094/21888)\n",
            "Epoch: 102 | Batch_idx: 180 |  Loss: (0.1062) | Acc: (96.37%) (22327/23168)\n",
            "Epoch: 102 | Batch_idx: 190 |  Loss: (0.1047) | Acc: (96.41%) (23571/24448)\n",
            "Epoch: 102 | Batch_idx: 200 |  Loss: (0.1045) | Acc: (96.41%) (24805/25728)\n",
            "Epoch: 102 | Batch_idx: 210 |  Loss: (0.1050) | Acc: (96.39%) (26034/27008)\n",
            "Epoch: 102 | Batch_idx: 220 |  Loss: (0.1061) | Acc: (96.37%) (27262/28288)\n",
            "Epoch: 102 | Batch_idx: 230 |  Loss: (0.1061) | Acc: (96.37%) (28494/29568)\n",
            "Epoch: 102 | Batch_idx: 240 |  Loss: (0.1055) | Acc: (96.40%) (29737/30848)\n",
            "Epoch: 102 | Batch_idx: 250 |  Loss: (0.1055) | Acc: (96.40%) (30970/32128)\n",
            "Epoch: 102 | Batch_idx: 260 |  Loss: (0.1050) | Acc: (96.41%) (32210/33408)\n",
            "Epoch: 102 | Batch_idx: 270 |  Loss: (0.1044) | Acc: (96.44%) (33452/34688)\n",
            "Epoch: 102 | Batch_idx: 280 |  Loss: (0.1045) | Acc: (96.42%) (34682/35968)\n",
            "Epoch: 102 | Batch_idx: 290 |  Loss: (0.1054) | Acc: (96.40%) (35908/37248)\n",
            "Epoch: 102 | Batch_idx: 300 |  Loss: (0.1057) | Acc: (96.38%) (37135/38528)\n",
            "Epoch: 102 | Batch_idx: 310 |  Loss: (0.1055) | Acc: (96.38%) (38367/39808)\n",
            "Epoch: 102 | Batch_idx: 320 |  Loss: (0.1055) | Acc: (96.39%) (39603/41088)\n",
            "Epoch: 102 | Batch_idx: 330 |  Loss: (0.1053) | Acc: (96.38%) (40835/42368)\n",
            "Epoch: 102 | Batch_idx: 340 |  Loss: (0.1052) | Acc: (96.38%) (42070/43648)\n",
            "Epoch: 102 | Batch_idx: 350 |  Loss: (0.1052) | Acc: (96.39%) (43307/44928)\n",
            "Epoch: 102 | Batch_idx: 360 |  Loss: (0.1049) | Acc: (96.40%) (44546/46208)\n",
            "Epoch: 102 | Batch_idx: 370 |  Loss: (0.1045) | Acc: (96.41%) (45784/47488)\n",
            "Epoch: 102 | Batch_idx: 380 |  Loss: (0.1044) | Acc: (96.41%) (47017/48768)\n",
            "Epoch: 102 | Batch_idx: 390 |  Loss: (0.1045) | Acc: (96.39%) (48196/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3126) | Acc: (91.19%) (9119/10000)\n",
            "Epoch: 103 | Batch_idx: 0 |  Loss: (0.0641) | Acc: (98.44%) (126/128)\n",
            "Epoch: 103 | Batch_idx: 10 |  Loss: (0.0771) | Acc: (97.30%) (1370/1408)\n",
            "Epoch: 103 | Batch_idx: 20 |  Loss: (0.0824) | Acc: (97.14%) (2611/2688)\n",
            "Epoch: 103 | Batch_idx: 30 |  Loss: (0.0978) | Acc: (96.72%) (3838/3968)\n",
            "Epoch: 103 | Batch_idx: 40 |  Loss: (0.1020) | Acc: (96.40%) (5059/5248)\n",
            "Epoch: 103 | Batch_idx: 50 |  Loss: (0.1005) | Acc: (96.31%) (6287/6528)\n",
            "Epoch: 103 | Batch_idx: 60 |  Loss: (0.1018) | Acc: (96.32%) (7521/7808)\n",
            "Epoch: 103 | Batch_idx: 70 |  Loss: (0.1033) | Acc: (96.36%) (8757/9088)\n",
            "Epoch: 103 | Batch_idx: 80 |  Loss: (0.1036) | Acc: (96.26%) (9980/10368)\n",
            "Epoch: 103 | Batch_idx: 90 |  Loss: (0.1040) | Acc: (96.27%) (11213/11648)\n",
            "Epoch: 103 | Batch_idx: 100 |  Loss: (0.1026) | Acc: (96.29%) (12448/12928)\n",
            "Epoch: 103 | Batch_idx: 110 |  Loss: (0.1039) | Acc: (96.26%) (13677/14208)\n",
            "Epoch: 103 | Batch_idx: 120 |  Loss: (0.1023) | Acc: (96.31%) (14916/15488)\n",
            "Epoch: 103 | Batch_idx: 130 |  Loss: (0.1009) | Acc: (96.36%) (16157/16768)\n",
            "Epoch: 103 | Batch_idx: 140 |  Loss: (0.1014) | Acc: (96.37%) (17392/18048)\n",
            "Epoch: 103 | Batch_idx: 150 |  Loss: (0.1028) | Acc: (96.32%) (18616/19328)\n",
            "Epoch: 103 | Batch_idx: 160 |  Loss: (0.1044) | Acc: (96.28%) (19841/20608)\n",
            "Epoch: 103 | Batch_idx: 170 |  Loss: (0.1043) | Acc: (96.33%) (21084/21888)\n",
            "Epoch: 103 | Batch_idx: 180 |  Loss: (0.1052) | Acc: (96.32%) (22315/23168)\n",
            "Epoch: 103 | Batch_idx: 190 |  Loss: (0.1053) | Acc: (96.31%) (23547/24448)\n",
            "Epoch: 103 | Batch_idx: 200 |  Loss: (0.1054) | Acc: (96.30%) (24777/25728)\n",
            "Epoch: 103 | Batch_idx: 210 |  Loss: (0.1058) | Acc: (96.29%) (26005/27008)\n",
            "Epoch: 103 | Batch_idx: 220 |  Loss: (0.1058) | Acc: (96.27%) (27232/28288)\n",
            "Epoch: 103 | Batch_idx: 230 |  Loss: (0.1069) | Acc: (96.25%) (28460/29568)\n",
            "Epoch: 103 | Batch_idx: 240 |  Loss: (0.1066) | Acc: (96.29%) (29704/30848)\n",
            "Epoch: 103 | Batch_idx: 250 |  Loss: (0.1065) | Acc: (96.29%) (30936/32128)\n",
            "Epoch: 103 | Batch_idx: 260 |  Loss: (0.1055) | Acc: (96.32%) (32179/33408)\n",
            "Epoch: 103 | Batch_idx: 270 |  Loss: (0.1052) | Acc: (96.34%) (33419/34688)\n",
            "Epoch: 103 | Batch_idx: 280 |  Loss: (0.1048) | Acc: (96.37%) (34661/35968)\n",
            "Epoch: 103 | Batch_idx: 290 |  Loss: (0.1043) | Acc: (96.36%) (35894/37248)\n",
            "Epoch: 103 | Batch_idx: 300 |  Loss: (0.1048) | Acc: (96.34%) (37119/38528)\n",
            "Epoch: 103 | Batch_idx: 310 |  Loss: (0.1043) | Acc: (96.36%) (38359/39808)\n",
            "Epoch: 103 | Batch_idx: 320 |  Loss: (0.1048) | Acc: (96.36%) (39594/41088)\n",
            "Epoch: 103 | Batch_idx: 330 |  Loss: (0.1047) | Acc: (96.37%) (40830/42368)\n",
            "Epoch: 103 | Batch_idx: 340 |  Loss: (0.1044) | Acc: (96.39%) (42072/43648)\n",
            "Epoch: 103 | Batch_idx: 350 |  Loss: (0.1038) | Acc: (96.42%) (43318/44928)\n",
            "Epoch: 103 | Batch_idx: 360 |  Loss: (0.1041) | Acc: (96.42%) (44554/46208)\n",
            "Epoch: 103 | Batch_idx: 370 |  Loss: (0.1039) | Acc: (96.43%) (45793/47488)\n",
            "Epoch: 103 | Batch_idx: 380 |  Loss: (0.1037) | Acc: (96.43%) (47028/48768)\n",
            "Epoch: 103 | Batch_idx: 390 |  Loss: (0.1035) | Acc: (96.43%) (48216/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3266) | Acc: (91.03%) (9103/10000)\n",
            "Epoch: 104 | Batch_idx: 0 |  Loss: (0.0508) | Acc: (97.66%) (125/128)\n",
            "Epoch: 104 | Batch_idx: 10 |  Loss: (0.0968) | Acc: (96.66%) (1361/1408)\n",
            "Epoch: 104 | Batch_idx: 20 |  Loss: (0.0981) | Acc: (96.28%) (2588/2688)\n",
            "Epoch: 104 | Batch_idx: 30 |  Loss: (0.1016) | Acc: (96.27%) (3820/3968)\n",
            "Epoch: 104 | Batch_idx: 40 |  Loss: (0.1038) | Acc: (96.27%) (5052/5248)\n",
            "Epoch: 104 | Batch_idx: 50 |  Loss: (0.1077) | Acc: (96.25%) (6283/6528)\n",
            "Epoch: 104 | Batch_idx: 60 |  Loss: (0.1079) | Acc: (96.23%) (7514/7808)\n",
            "Epoch: 104 | Batch_idx: 70 |  Loss: (0.1098) | Acc: (96.21%) (8744/9088)\n",
            "Epoch: 104 | Batch_idx: 80 |  Loss: (0.1094) | Acc: (96.25%) (9979/10368)\n",
            "Epoch: 104 | Batch_idx: 90 |  Loss: (0.1071) | Acc: (96.34%) (11222/11648)\n",
            "Epoch: 104 | Batch_idx: 100 |  Loss: (0.1048) | Acc: (96.40%) (12463/12928)\n",
            "Epoch: 104 | Batch_idx: 110 |  Loss: (0.1075) | Acc: (96.32%) (13685/14208)\n",
            "Epoch: 104 | Batch_idx: 120 |  Loss: (0.1069) | Acc: (96.29%) (14914/15488)\n",
            "Epoch: 104 | Batch_idx: 130 |  Loss: (0.1067) | Acc: (96.32%) (16151/16768)\n",
            "Epoch: 104 | Batch_idx: 140 |  Loss: (0.1065) | Acc: (96.32%) (17383/18048)\n",
            "Epoch: 104 | Batch_idx: 150 |  Loss: (0.1075) | Acc: (96.30%) (18613/19328)\n",
            "Epoch: 104 | Batch_idx: 160 |  Loss: (0.1073) | Acc: (96.33%) (19852/20608)\n",
            "Epoch: 104 | Batch_idx: 170 |  Loss: (0.1078) | Acc: (96.33%) (21085/21888)\n",
            "Epoch: 104 | Batch_idx: 180 |  Loss: (0.1075) | Acc: (96.32%) (22316/23168)\n",
            "Epoch: 104 | Batch_idx: 190 |  Loss: (0.1078) | Acc: (96.28%) (23539/24448)\n",
            "Epoch: 104 | Batch_idx: 200 |  Loss: (0.1078) | Acc: (96.29%) (24774/25728)\n",
            "Epoch: 104 | Batch_idx: 210 |  Loss: (0.1079) | Acc: (96.30%) (26008/27008)\n",
            "Epoch: 104 | Batch_idx: 220 |  Loss: (0.1073) | Acc: (96.30%) (27242/28288)\n",
            "Epoch: 104 | Batch_idx: 230 |  Loss: (0.1062) | Acc: (96.35%) (28489/29568)\n",
            "Epoch: 104 | Batch_idx: 240 |  Loss: (0.1065) | Acc: (96.33%) (29715/30848)\n",
            "Epoch: 104 | Batch_idx: 250 |  Loss: (0.1068) | Acc: (96.31%) (30942/32128)\n",
            "Epoch: 104 | Batch_idx: 260 |  Loss: (0.1062) | Acc: (96.33%) (32182/33408)\n",
            "Epoch: 104 | Batch_idx: 270 |  Loss: (0.1064) | Acc: (96.32%) (33413/34688)\n",
            "Epoch: 104 | Batch_idx: 280 |  Loss: (0.1062) | Acc: (96.34%) (34650/35968)\n",
            "Epoch: 104 | Batch_idx: 290 |  Loss: (0.1066) | Acc: (96.31%) (35873/37248)\n",
            "Epoch: 104 | Batch_idx: 300 |  Loss: (0.1066) | Acc: (96.32%) (37110/38528)\n",
            "Epoch: 104 | Batch_idx: 310 |  Loss: (0.1062) | Acc: (96.33%) (38348/39808)\n",
            "Epoch: 104 | Batch_idx: 320 |  Loss: (0.1055) | Acc: (96.34%) (39586/41088)\n",
            "Epoch: 104 | Batch_idx: 330 |  Loss: (0.1055) | Acc: (96.34%) (40819/42368)\n",
            "Epoch: 104 | Batch_idx: 340 |  Loss: (0.1060) | Acc: (96.34%) (42050/43648)\n",
            "Epoch: 104 | Batch_idx: 350 |  Loss: (0.1054) | Acc: (96.35%) (43289/44928)\n",
            "Epoch: 104 | Batch_idx: 360 |  Loss: (0.1059) | Acc: (96.35%) (44520/46208)\n",
            "Epoch: 104 | Batch_idx: 370 |  Loss: (0.1061) | Acc: (96.34%) (45752/47488)\n",
            "Epoch: 104 | Batch_idx: 380 |  Loss: (0.1059) | Acc: (96.36%) (46994/48768)\n",
            "Epoch: 104 | Batch_idx: 390 |  Loss: (0.1057) | Acc: (96.36%) (48180/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3189) | Acc: (91.09%) (9109/10000)\n",
            "Epoch: 105 | Batch_idx: 0 |  Loss: (0.0678) | Acc: (98.44%) (126/128)\n",
            "Epoch: 105 | Batch_idx: 10 |  Loss: (0.1150) | Acc: (95.88%) (1350/1408)\n",
            "Epoch: 105 | Batch_idx: 20 |  Loss: (0.1018) | Acc: (96.58%) (2596/2688)\n",
            "Epoch: 105 | Batch_idx: 30 |  Loss: (0.0938) | Acc: (96.77%) (3840/3968)\n",
            "Epoch: 105 | Batch_idx: 40 |  Loss: (0.0895) | Acc: (96.86%) (5083/5248)\n",
            "Epoch: 105 | Batch_idx: 50 |  Loss: (0.0938) | Acc: (96.75%) (6316/6528)\n",
            "Epoch: 105 | Batch_idx: 60 |  Loss: (0.0919) | Acc: (96.82%) (7560/7808)\n",
            "Epoch: 105 | Batch_idx: 70 |  Loss: (0.0934) | Acc: (96.74%) (8792/9088)\n",
            "Epoch: 105 | Batch_idx: 80 |  Loss: (0.0916) | Acc: (96.82%) (10038/10368)\n",
            "Epoch: 105 | Batch_idx: 90 |  Loss: (0.0938) | Acc: (96.72%) (11266/11648)\n",
            "Epoch: 105 | Batch_idx: 100 |  Loss: (0.0948) | Acc: (96.68%) (12499/12928)\n",
            "Epoch: 105 | Batch_idx: 110 |  Loss: (0.0947) | Acc: (96.66%) (13734/14208)\n",
            "Epoch: 105 | Batch_idx: 120 |  Loss: (0.0941) | Acc: (96.67%) (14973/15488)\n",
            "Epoch: 105 | Batch_idx: 130 |  Loss: (0.0959) | Acc: (96.64%) (16205/16768)\n",
            "Epoch: 105 | Batch_idx: 140 |  Loss: (0.0970) | Acc: (96.65%) (17443/18048)\n",
            "Epoch: 105 | Batch_idx: 150 |  Loss: (0.0989) | Acc: (96.60%) (18671/19328)\n",
            "Epoch: 105 | Batch_idx: 160 |  Loss: (0.0983) | Acc: (96.65%) (19917/20608)\n",
            "Epoch: 105 | Batch_idx: 170 |  Loss: (0.0998) | Acc: (96.61%) (21145/21888)\n",
            "Epoch: 105 | Batch_idx: 180 |  Loss: (0.0999) | Acc: (96.61%) (22382/23168)\n",
            "Epoch: 105 | Batch_idx: 190 |  Loss: (0.0986) | Acc: (96.63%) (23624/24448)\n",
            "Epoch: 105 | Batch_idx: 200 |  Loss: (0.0976) | Acc: (96.65%) (24865/25728)\n",
            "Epoch: 105 | Batch_idx: 210 |  Loss: (0.0966) | Acc: (96.69%) (26114/27008)\n",
            "Epoch: 105 | Batch_idx: 220 |  Loss: (0.0971) | Acc: (96.67%) (27346/28288)\n",
            "Epoch: 105 | Batch_idx: 230 |  Loss: (0.0977) | Acc: (96.65%) (28576/29568)\n",
            "Epoch: 105 | Batch_idx: 240 |  Loss: (0.0976) | Acc: (96.65%) (29816/30848)\n",
            "Epoch: 105 | Batch_idx: 250 |  Loss: (0.0978) | Acc: (96.66%) (31056/32128)\n",
            "Epoch: 105 | Batch_idx: 260 |  Loss: (0.0974) | Acc: (96.69%) (32302/33408)\n",
            "Epoch: 105 | Batch_idx: 270 |  Loss: (0.0974) | Acc: (96.69%) (33541/34688)\n",
            "Epoch: 105 | Batch_idx: 280 |  Loss: (0.0986) | Acc: (96.67%) (34772/35968)\n",
            "Epoch: 105 | Batch_idx: 290 |  Loss: (0.0990) | Acc: (96.66%) (36003/37248)\n",
            "Epoch: 105 | Batch_idx: 300 |  Loss: (0.0989) | Acc: (96.66%) (37242/38528)\n",
            "Epoch: 105 | Batch_idx: 310 |  Loss: (0.0989) | Acc: (96.65%) (38476/39808)\n",
            "Epoch: 105 | Batch_idx: 320 |  Loss: (0.0995) | Acc: (96.62%) (39698/41088)\n",
            "Epoch: 105 | Batch_idx: 330 |  Loss: (0.0995) | Acc: (96.62%) (40938/42368)\n",
            "Epoch: 105 | Batch_idx: 340 |  Loss: (0.0998) | Acc: (96.61%) (42167/43648)\n",
            "Epoch: 105 | Batch_idx: 350 |  Loss: (0.1001) | Acc: (96.60%) (43400/44928)\n",
            "Epoch: 105 | Batch_idx: 360 |  Loss: (0.0998) | Acc: (96.62%) (44645/46208)\n",
            "Epoch: 105 | Batch_idx: 370 |  Loss: (0.0995) | Acc: (96.64%) (45892/47488)\n",
            "Epoch: 105 | Batch_idx: 380 |  Loss: (0.0995) | Acc: (96.63%) (47126/48768)\n",
            "Epoch: 105 | Batch_idx: 390 |  Loss: (0.0993) | Acc: (96.64%) (48322/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3135) | Acc: (91.14%) (9114/10000)\n",
            "Epoch: 106 | Batch_idx: 0 |  Loss: (0.0955) | Acc: (96.09%) (123/128)\n",
            "Epoch: 106 | Batch_idx: 10 |  Loss: (0.0924) | Acc: (96.59%) (1360/1408)\n",
            "Epoch: 106 | Batch_idx: 20 |  Loss: (0.0840) | Acc: (96.95%) (2606/2688)\n",
            "Epoch: 106 | Batch_idx: 30 |  Loss: (0.0942) | Acc: (96.88%) (3844/3968)\n",
            "Epoch: 106 | Batch_idx: 40 |  Loss: (0.0884) | Acc: (97.16%) (5099/5248)\n",
            "Epoch: 106 | Batch_idx: 50 |  Loss: (0.0907) | Acc: (97.12%) (6340/6528)\n",
            "Epoch: 106 | Batch_idx: 60 |  Loss: (0.0925) | Acc: (97.02%) (7575/7808)\n",
            "Epoch: 106 | Batch_idx: 70 |  Loss: (0.0934) | Acc: (96.92%) (8808/9088)\n",
            "Epoch: 106 | Batch_idx: 80 |  Loss: (0.0934) | Acc: (96.92%) (10049/10368)\n",
            "Epoch: 106 | Batch_idx: 90 |  Loss: (0.0939) | Acc: (96.82%) (11278/11648)\n",
            "Epoch: 106 | Batch_idx: 100 |  Loss: (0.0968) | Acc: (96.72%) (12504/12928)\n",
            "Epoch: 106 | Batch_idx: 110 |  Loss: (0.0969) | Acc: (96.71%) (13741/14208)\n",
            "Epoch: 106 | Batch_idx: 120 |  Loss: (0.0953) | Acc: (96.76%) (14986/15488)\n",
            "Epoch: 106 | Batch_idx: 130 |  Loss: (0.0967) | Acc: (96.70%) (16215/16768)\n",
            "Epoch: 106 | Batch_idx: 140 |  Loss: (0.0958) | Acc: (96.70%) (17453/18048)\n",
            "Epoch: 106 | Batch_idx: 150 |  Loss: (0.0954) | Acc: (96.69%) (18689/19328)\n",
            "Epoch: 106 | Batch_idx: 160 |  Loss: (0.0958) | Acc: (96.69%) (19925/20608)\n",
            "Epoch: 106 | Batch_idx: 170 |  Loss: (0.0953) | Acc: (96.71%) (21167/21888)\n",
            "Epoch: 106 | Batch_idx: 180 |  Loss: (0.0961) | Acc: (96.69%) (22402/23168)\n",
            "Epoch: 106 | Batch_idx: 190 |  Loss: (0.0958) | Acc: (96.72%) (23647/24448)\n",
            "Epoch: 106 | Batch_idx: 200 |  Loss: (0.0962) | Acc: (96.69%) (24876/25728)\n",
            "Epoch: 106 | Batch_idx: 210 |  Loss: (0.0961) | Acc: (96.69%) (26113/27008)\n",
            "Epoch: 106 | Batch_idx: 220 |  Loss: (0.0966) | Acc: (96.68%) (27349/28288)\n",
            "Epoch: 106 | Batch_idx: 230 |  Loss: (0.0970) | Acc: (96.67%) (28583/29568)\n",
            "Epoch: 106 | Batch_idx: 240 |  Loss: (0.0966) | Acc: (96.68%) (29825/30848)\n",
            "Epoch: 106 | Batch_idx: 250 |  Loss: (0.0967) | Acc: (96.70%) (31067/32128)\n",
            "Epoch: 106 | Batch_idx: 260 |  Loss: (0.0964) | Acc: (96.71%) (32310/33408)\n",
            "Epoch: 106 | Batch_idx: 270 |  Loss: (0.0967) | Acc: (96.71%) (33548/34688)\n",
            "Epoch: 106 | Batch_idx: 280 |  Loss: (0.0968) | Acc: (96.69%) (34777/35968)\n",
            "Epoch: 106 | Batch_idx: 290 |  Loss: (0.0976) | Acc: (96.65%) (35999/37248)\n",
            "Epoch: 106 | Batch_idx: 300 |  Loss: (0.0982) | Acc: (96.62%) (37227/38528)\n",
            "Epoch: 106 | Batch_idx: 310 |  Loss: (0.0986) | Acc: (96.61%) (38457/39808)\n",
            "Epoch: 106 | Batch_idx: 320 |  Loss: (0.0988) | Acc: (96.60%) (39690/41088)\n",
            "Epoch: 106 | Batch_idx: 330 |  Loss: (0.0992) | Acc: (96.59%) (40925/42368)\n",
            "Epoch: 106 | Batch_idx: 340 |  Loss: (0.0988) | Acc: (96.60%) (42164/43648)\n",
            "Epoch: 106 | Batch_idx: 350 |  Loss: (0.0992) | Acc: (96.57%) (43388/44928)\n",
            "Epoch: 106 | Batch_idx: 360 |  Loss: (0.0993) | Acc: (96.56%) (44619/46208)\n",
            "Epoch: 106 | Batch_idx: 370 |  Loss: (0.0994) | Acc: (96.55%) (45851/47488)\n",
            "Epoch: 106 | Batch_idx: 380 |  Loss: (0.0993) | Acc: (96.56%) (47092/48768)\n",
            "Epoch: 106 | Batch_idx: 390 |  Loss: (0.0990) | Acc: (96.58%) (48289/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3214) | Acc: (91.23%) (9123/10000)\n",
            "Epoch: 107 | Batch_idx: 0 |  Loss: (0.0683) | Acc: (98.44%) (126/128)\n",
            "Epoch: 107 | Batch_idx: 10 |  Loss: (0.1014) | Acc: (96.52%) (1359/1408)\n",
            "Epoch: 107 | Batch_idx: 20 |  Loss: (0.0973) | Acc: (96.73%) (2600/2688)\n",
            "Epoch: 107 | Batch_idx: 30 |  Loss: (0.0994) | Acc: (96.60%) (3833/3968)\n",
            "Epoch: 107 | Batch_idx: 40 |  Loss: (0.0972) | Acc: (96.57%) (5068/5248)\n",
            "Epoch: 107 | Batch_idx: 50 |  Loss: (0.1010) | Acc: (96.38%) (6292/6528)\n",
            "Epoch: 107 | Batch_idx: 60 |  Loss: (0.1009) | Acc: (96.43%) (7529/7808)\n",
            "Epoch: 107 | Batch_idx: 70 |  Loss: (0.1004) | Acc: (96.51%) (8771/9088)\n",
            "Epoch: 107 | Batch_idx: 80 |  Loss: (0.1024) | Acc: (96.47%) (10002/10368)\n",
            "Epoch: 107 | Batch_idx: 90 |  Loss: (0.1020) | Acc: (96.49%) (11239/11648)\n",
            "Epoch: 107 | Batch_idx: 100 |  Loss: (0.1015) | Acc: (96.53%) (12479/12928)\n",
            "Epoch: 107 | Batch_idx: 110 |  Loss: (0.1019) | Acc: (96.47%) (13707/14208)\n",
            "Epoch: 107 | Batch_idx: 120 |  Loss: (0.1025) | Acc: (96.48%) (14943/15488)\n",
            "Epoch: 107 | Batch_idx: 130 |  Loss: (0.1020) | Acc: (96.49%) (16179/16768)\n",
            "Epoch: 107 | Batch_idx: 140 |  Loss: (0.1008) | Acc: (96.54%) (17424/18048)\n",
            "Epoch: 107 | Batch_idx: 150 |  Loss: (0.1004) | Acc: (96.55%) (18661/19328)\n",
            "Epoch: 107 | Batch_idx: 160 |  Loss: (0.1000) | Acc: (96.56%) (19900/20608)\n",
            "Epoch: 107 | Batch_idx: 170 |  Loss: (0.1005) | Acc: (96.57%) (21137/21888)\n",
            "Epoch: 107 | Batch_idx: 180 |  Loss: (0.1020) | Acc: (96.53%) (22364/23168)\n",
            "Epoch: 107 | Batch_idx: 190 |  Loss: (0.1023) | Acc: (96.51%) (23595/24448)\n",
            "Epoch: 107 | Batch_idx: 200 |  Loss: (0.1016) | Acc: (96.53%) (24836/25728)\n",
            "Epoch: 107 | Batch_idx: 210 |  Loss: (0.1011) | Acc: (96.56%) (26078/27008)\n",
            "Epoch: 107 | Batch_idx: 220 |  Loss: (0.1022) | Acc: (96.54%) (27309/28288)\n",
            "Epoch: 107 | Batch_idx: 230 |  Loss: (0.1021) | Acc: (96.53%) (28542/29568)\n",
            "Epoch: 107 | Batch_idx: 240 |  Loss: (0.1020) | Acc: (96.53%) (29779/30848)\n",
            "Epoch: 107 | Batch_idx: 250 |  Loss: (0.1011) | Acc: (96.57%) (31025/32128)\n",
            "Epoch: 107 | Batch_idx: 260 |  Loss: (0.1014) | Acc: (96.55%) (32256/33408)\n",
            "Epoch: 107 | Batch_idx: 270 |  Loss: (0.1018) | Acc: (96.55%) (33491/34688)\n",
            "Epoch: 107 | Batch_idx: 280 |  Loss: (0.1024) | Acc: (96.55%) (34726/35968)\n",
            "Epoch: 107 | Batch_idx: 290 |  Loss: (0.1019) | Acc: (96.56%) (35967/37248)\n",
            "Epoch: 107 | Batch_idx: 300 |  Loss: (0.1025) | Acc: (96.54%) (37196/38528)\n",
            "Epoch: 107 | Batch_idx: 310 |  Loss: (0.1025) | Acc: (96.56%) (38437/39808)\n",
            "Epoch: 107 | Batch_idx: 320 |  Loss: (0.1025) | Acc: (96.56%) (39676/41088)\n",
            "Epoch: 107 | Batch_idx: 330 |  Loss: (0.1017) | Acc: (96.58%) (40921/42368)\n",
            "Epoch: 107 | Batch_idx: 340 |  Loss: (0.1013) | Acc: (96.59%) (42158/43648)\n",
            "Epoch: 107 | Batch_idx: 350 |  Loss: (0.1015) | Acc: (96.57%) (43385/44928)\n",
            "Epoch: 107 | Batch_idx: 360 |  Loss: (0.1018) | Acc: (96.57%) (44622/46208)\n",
            "Epoch: 107 | Batch_idx: 370 |  Loss: (0.1021) | Acc: (96.54%) (45847/47488)\n",
            "Epoch: 107 | Batch_idx: 380 |  Loss: (0.1015) | Acc: (96.56%) (47092/48768)\n",
            "Epoch: 107 | Batch_idx: 390 |  Loss: (0.1009) | Acc: (96.58%) (48292/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3284) | Acc: (90.93%) (9093/10000)\n",
            "Epoch: 108 | Batch_idx: 0 |  Loss: (0.0894) | Acc: (97.66%) (125/128)\n",
            "Epoch: 108 | Batch_idx: 10 |  Loss: (0.1056) | Acc: (96.16%) (1354/1408)\n",
            "Epoch: 108 | Batch_idx: 20 |  Loss: (0.1002) | Acc: (96.32%) (2589/2688)\n",
            "Epoch: 108 | Batch_idx: 30 |  Loss: (0.1023) | Acc: (96.35%) (3823/3968)\n",
            "Epoch: 108 | Batch_idx: 40 |  Loss: (0.1018) | Acc: (96.46%) (5062/5248)\n",
            "Epoch: 108 | Batch_idx: 50 |  Loss: (0.1006) | Acc: (96.57%) (6304/6528)\n",
            "Epoch: 108 | Batch_idx: 60 |  Loss: (0.1000) | Acc: (96.59%) (7542/7808)\n",
            "Epoch: 108 | Batch_idx: 70 |  Loss: (0.1002) | Acc: (96.63%) (8782/9088)\n",
            "Epoch: 108 | Batch_idx: 80 |  Loss: (0.1002) | Acc: (96.66%) (10022/10368)\n",
            "Epoch: 108 | Batch_idx: 90 |  Loss: (0.1001) | Acc: (96.66%) (11259/11648)\n",
            "Epoch: 108 | Batch_idx: 100 |  Loss: (0.0998) | Acc: (96.64%) (12494/12928)\n",
            "Epoch: 108 | Batch_idx: 110 |  Loss: (0.1004) | Acc: (96.61%) (13727/14208)\n",
            "Epoch: 108 | Batch_idx: 120 |  Loss: (0.1009) | Acc: (96.60%) (14961/15488)\n",
            "Epoch: 108 | Batch_idx: 130 |  Loss: (0.0999) | Acc: (96.65%) (16206/16768)\n",
            "Epoch: 108 | Batch_idx: 140 |  Loss: (0.0984) | Acc: (96.67%) (17447/18048)\n",
            "Epoch: 108 | Batch_idx: 150 |  Loss: (0.0970) | Acc: (96.70%) (18690/19328)\n",
            "Epoch: 108 | Batch_idx: 160 |  Loss: (0.0972) | Acc: (96.72%) (19932/20608)\n",
            "Epoch: 108 | Batch_idx: 170 |  Loss: (0.0983) | Acc: (96.70%) (21166/21888)\n",
            "Epoch: 108 | Batch_idx: 180 |  Loss: (0.0985) | Acc: (96.69%) (22401/23168)\n",
            "Epoch: 108 | Batch_idx: 190 |  Loss: (0.0994) | Acc: (96.67%) (23635/24448)\n",
            "Epoch: 108 | Batch_idx: 200 |  Loss: (0.0997) | Acc: (96.67%) (24871/25728)\n",
            "Epoch: 108 | Batch_idx: 210 |  Loss: (0.0994) | Acc: (96.66%) (26105/27008)\n",
            "Epoch: 108 | Batch_idx: 220 |  Loss: (0.0993) | Acc: (96.64%) (27337/28288)\n",
            "Epoch: 108 | Batch_idx: 230 |  Loss: (0.0989) | Acc: (96.67%) (28582/29568)\n",
            "Epoch: 108 | Batch_idx: 240 |  Loss: (0.0984) | Acc: (96.67%) (29822/30848)\n",
            "Epoch: 108 | Batch_idx: 250 |  Loss: (0.0983) | Acc: (96.67%) (31059/32128)\n",
            "Epoch: 108 | Batch_idx: 260 |  Loss: (0.0981) | Acc: (96.69%) (32302/33408)\n",
            "Epoch: 108 | Batch_idx: 270 |  Loss: (0.0974) | Acc: (96.70%) (33544/34688)\n",
            "Epoch: 108 | Batch_idx: 280 |  Loss: (0.0961) | Acc: (96.76%) (34801/35968)\n",
            "Epoch: 108 | Batch_idx: 290 |  Loss: (0.0958) | Acc: (96.77%) (36046/37248)\n",
            "Epoch: 108 | Batch_idx: 300 |  Loss: (0.0956) | Acc: (96.77%) (37285/38528)\n",
            "Epoch: 108 | Batch_idx: 310 |  Loss: (0.0958) | Acc: (96.76%) (38519/39808)\n",
            "Epoch: 108 | Batch_idx: 320 |  Loss: (0.0959) | Acc: (96.77%) (39760/41088)\n",
            "Epoch: 108 | Batch_idx: 330 |  Loss: (0.0960) | Acc: (96.76%) (40996/42368)\n",
            "Epoch: 108 | Batch_idx: 340 |  Loss: (0.0964) | Acc: (96.74%) (42225/43648)\n",
            "Epoch: 108 | Batch_idx: 350 |  Loss: (0.0964) | Acc: (96.72%) (43456/44928)\n",
            "Epoch: 108 | Batch_idx: 360 |  Loss: (0.0964) | Acc: (96.71%) (44689/46208)\n",
            "Epoch: 108 | Batch_idx: 370 |  Loss: (0.0965) | Acc: (96.71%) (45926/47488)\n",
            "Epoch: 108 | Batch_idx: 380 |  Loss: (0.0963) | Acc: (96.72%) (47167/48768)\n",
            "Epoch: 108 | Batch_idx: 390 |  Loss: (0.0961) | Acc: (96.72%) (48361/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3269) | Acc: (91.23%) (9123/10000)\n",
            "Epoch: 109 | Batch_idx: 0 |  Loss: (0.0749) | Acc: (96.88%) (124/128)\n",
            "Epoch: 109 | Batch_idx: 10 |  Loss: (0.1009) | Acc: (96.66%) (1361/1408)\n",
            "Epoch: 109 | Batch_idx: 20 |  Loss: (0.0900) | Acc: (97.17%) (2612/2688)\n",
            "Epoch: 109 | Batch_idx: 30 |  Loss: (0.0898) | Acc: (97.18%) (3856/3968)\n",
            "Epoch: 109 | Batch_idx: 40 |  Loss: (0.0879) | Acc: (97.28%) (5105/5248)\n",
            "Epoch: 109 | Batch_idx: 50 |  Loss: (0.0903) | Acc: (97.23%) (6347/6528)\n",
            "Epoch: 109 | Batch_idx: 60 |  Loss: (0.0877) | Acc: (97.25%) (7593/7808)\n",
            "Epoch: 109 | Batch_idx: 70 |  Loss: (0.0904) | Acc: (97.10%) (8824/9088)\n",
            "Epoch: 109 | Batch_idx: 80 |  Loss: (0.0913) | Acc: (97.05%) (10062/10368)\n",
            "Epoch: 109 | Batch_idx: 90 |  Loss: (0.0907) | Acc: (97.06%) (11305/11648)\n",
            "Epoch: 109 | Batch_idx: 100 |  Loss: (0.0922) | Acc: (96.96%) (12535/12928)\n",
            "Epoch: 109 | Batch_idx: 110 |  Loss: (0.0921) | Acc: (96.94%) (13773/14208)\n",
            "Epoch: 109 | Batch_idx: 120 |  Loss: (0.0916) | Acc: (96.96%) (15017/15488)\n",
            "Epoch: 109 | Batch_idx: 130 |  Loss: (0.0914) | Acc: (96.95%) (16256/16768)\n",
            "Epoch: 109 | Batch_idx: 140 |  Loss: (0.0920) | Acc: (96.92%) (17492/18048)\n",
            "Epoch: 109 | Batch_idx: 150 |  Loss: (0.0910) | Acc: (96.96%) (18740/19328)\n",
            "Epoch: 109 | Batch_idx: 160 |  Loss: (0.0916) | Acc: (96.92%) (19974/20608)\n",
            "Epoch: 109 | Batch_idx: 170 |  Loss: (0.0913) | Acc: (96.95%) (21220/21888)\n",
            "Epoch: 109 | Batch_idx: 180 |  Loss: (0.0906) | Acc: (96.97%) (22467/23168)\n",
            "Epoch: 109 | Batch_idx: 190 |  Loss: (0.0910) | Acc: (96.94%) (23700/24448)\n",
            "Epoch: 109 | Batch_idx: 200 |  Loss: (0.0912) | Acc: (96.94%) (24940/25728)\n",
            "Epoch: 109 | Batch_idx: 210 |  Loss: (0.0922) | Acc: (96.90%) (26171/27008)\n",
            "Epoch: 109 | Batch_idx: 220 |  Loss: (0.0917) | Acc: (96.94%) (27422/28288)\n",
            "Epoch: 109 | Batch_idx: 230 |  Loss: (0.0912) | Acc: (96.95%) (28665/29568)\n",
            "Epoch: 109 | Batch_idx: 240 |  Loss: (0.0919) | Acc: (96.91%) (29894/30848)\n",
            "Epoch: 109 | Batch_idx: 250 |  Loss: (0.0927) | Acc: (96.87%) (31122/32128)\n",
            "Epoch: 109 | Batch_idx: 260 |  Loss: (0.0937) | Acc: (96.82%) (32344/33408)\n",
            "Epoch: 109 | Batch_idx: 270 |  Loss: (0.0939) | Acc: (96.82%) (33584/34688)\n",
            "Epoch: 109 | Batch_idx: 280 |  Loss: (0.0943) | Acc: (96.79%) (34814/35968)\n",
            "Epoch: 109 | Batch_idx: 290 |  Loss: (0.0950) | Acc: (96.79%) (36052/37248)\n",
            "Epoch: 109 | Batch_idx: 300 |  Loss: (0.0950) | Acc: (96.79%) (37293/38528)\n",
            "Epoch: 109 | Batch_idx: 310 |  Loss: (0.0951) | Acc: (96.77%) (38524/39808)\n",
            "Epoch: 109 | Batch_idx: 320 |  Loss: (0.0945) | Acc: (96.80%) (39775/41088)\n",
            "Epoch: 109 | Batch_idx: 330 |  Loss: (0.0945) | Acc: (96.81%) (41015/42368)\n",
            "Epoch: 109 | Batch_idx: 340 |  Loss: (0.0936) | Acc: (96.84%) (42269/43648)\n",
            "Epoch: 109 | Batch_idx: 350 |  Loss: (0.0937) | Acc: (96.84%) (43510/44928)\n",
            "Epoch: 109 | Batch_idx: 360 |  Loss: (0.0940) | Acc: (96.84%) (44747/46208)\n",
            "Epoch: 109 | Batch_idx: 370 |  Loss: (0.0934) | Acc: (96.85%) (45992/47488)\n",
            "Epoch: 109 | Batch_idx: 380 |  Loss: (0.0936) | Acc: (96.83%) (47222/48768)\n",
            "Epoch: 109 | Batch_idx: 390 |  Loss: (0.0935) | Acc: (96.82%) (48410/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3293) | Acc: (91.45%) (9145/10000)\n",
            "Epoch: 110 | Batch_idx: 0 |  Loss: (0.0481) | Acc: (97.66%) (125/128)\n",
            "Epoch: 110 | Batch_idx: 10 |  Loss: (0.0702) | Acc: (97.66%) (1375/1408)\n",
            "Epoch: 110 | Batch_idx: 20 |  Loss: (0.0853) | Acc: (97.40%) (2618/2688)\n",
            "Epoch: 110 | Batch_idx: 30 |  Loss: (0.0908) | Acc: (97.30%) (3861/3968)\n",
            "Epoch: 110 | Batch_idx: 40 |  Loss: (0.0900) | Acc: (97.29%) (5106/5248)\n",
            "Epoch: 110 | Batch_idx: 50 |  Loss: (0.0919) | Acc: (97.21%) (6346/6528)\n",
            "Epoch: 110 | Batch_idx: 60 |  Loss: (0.0938) | Acc: (97.17%) (7587/7808)\n",
            "Epoch: 110 | Batch_idx: 70 |  Loss: (0.0945) | Acc: (97.06%) (8821/9088)\n",
            "Epoch: 110 | Batch_idx: 80 |  Loss: (0.0948) | Acc: (97.03%) (10060/10368)\n",
            "Epoch: 110 | Batch_idx: 90 |  Loss: (0.0968) | Acc: (96.94%) (11291/11648)\n",
            "Epoch: 110 | Batch_idx: 100 |  Loss: (0.0959) | Acc: (96.97%) (12536/12928)\n",
            "Epoch: 110 | Batch_idx: 110 |  Loss: (0.0946) | Acc: (97.02%) (13784/14208)\n",
            "Epoch: 110 | Batch_idx: 120 |  Loss: (0.0961) | Acc: (97.00%) (15023/15488)\n",
            "Epoch: 110 | Batch_idx: 130 |  Loss: (0.0974) | Acc: (96.98%) (16262/16768)\n",
            "Epoch: 110 | Batch_idx: 140 |  Loss: (0.0965) | Acc: (97.00%) (17506/18048)\n",
            "Epoch: 110 | Batch_idx: 150 |  Loss: (0.0968) | Acc: (96.94%) (18737/19328)\n",
            "Epoch: 110 | Batch_idx: 160 |  Loss: (0.0967) | Acc: (96.90%) (19970/20608)\n",
            "Epoch: 110 | Batch_idx: 170 |  Loss: (0.0973) | Acc: (96.90%) (21209/21888)\n",
            "Epoch: 110 | Batch_idx: 180 |  Loss: (0.0976) | Acc: (96.87%) (22442/23168)\n",
            "Epoch: 110 | Batch_idx: 190 |  Loss: (0.0966) | Acc: (96.91%) (23693/24448)\n",
            "Epoch: 110 | Batch_idx: 200 |  Loss: (0.0975) | Acc: (96.89%) (24928/25728)\n",
            "Epoch: 110 | Batch_idx: 210 |  Loss: (0.0987) | Acc: (96.84%) (26154/27008)\n",
            "Epoch: 110 | Batch_idx: 220 |  Loss: (0.0981) | Acc: (96.85%) (27398/28288)\n",
            "Epoch: 110 | Batch_idx: 230 |  Loss: (0.0972) | Acc: (96.86%) (28641/29568)\n",
            "Epoch: 110 | Batch_idx: 240 |  Loss: (0.0975) | Acc: (96.85%) (29877/30848)\n",
            "Epoch: 110 | Batch_idx: 250 |  Loss: (0.0970) | Acc: (96.88%) (31126/32128)\n",
            "Epoch: 110 | Batch_idx: 260 |  Loss: (0.0968) | Acc: (96.87%) (32363/33408)\n",
            "Epoch: 110 | Batch_idx: 270 |  Loss: (0.0970) | Acc: (96.85%) (33596/34688)\n",
            "Epoch: 110 | Batch_idx: 280 |  Loss: (0.0968) | Acc: (96.87%) (34842/35968)\n",
            "Epoch: 110 | Batch_idx: 290 |  Loss: (0.0962) | Acc: (96.90%) (36092/37248)\n",
            "Epoch: 110 | Batch_idx: 300 |  Loss: (0.0967) | Acc: (96.87%) (37322/38528)\n",
            "Epoch: 110 | Batch_idx: 310 |  Loss: (0.0964) | Acc: (96.88%) (38567/39808)\n",
            "Epoch: 110 | Batch_idx: 320 |  Loss: (0.0953) | Acc: (96.91%) (39818/41088)\n",
            "Epoch: 110 | Batch_idx: 330 |  Loss: (0.0942) | Acc: (96.95%) (41074/42368)\n",
            "Epoch: 110 | Batch_idx: 340 |  Loss: (0.0942) | Acc: (96.95%) (42317/43648)\n",
            "Epoch: 110 | Batch_idx: 350 |  Loss: (0.0946) | Acc: (96.92%) (43546/44928)\n",
            "Epoch: 110 | Batch_idx: 360 |  Loss: (0.0947) | Acc: (96.94%) (44793/46208)\n",
            "Epoch: 110 | Batch_idx: 370 |  Loss: (0.0946) | Acc: (96.93%) (46030/47488)\n",
            "Epoch: 110 | Batch_idx: 380 |  Loss: (0.0945) | Acc: (96.94%) (47275/48768)\n",
            "Epoch: 110 | Batch_idx: 390 |  Loss: (0.0943) | Acc: (96.94%) (48468/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3437) | Acc: (90.89%) (9089/10000)\n",
            "Epoch: 111 | Batch_idx: 0 |  Loss: (0.1030) | Acc: (95.31%) (122/128)\n",
            "Epoch: 111 | Batch_idx: 10 |  Loss: (0.0951) | Acc: (96.45%) (1358/1408)\n",
            "Epoch: 111 | Batch_idx: 20 |  Loss: (0.0856) | Acc: (96.99%) (2607/2688)\n",
            "Epoch: 111 | Batch_idx: 30 |  Loss: (0.0821) | Acc: (97.05%) (3851/3968)\n",
            "Epoch: 111 | Batch_idx: 40 |  Loss: (0.0865) | Acc: (96.99%) (5090/5248)\n",
            "Epoch: 111 | Batch_idx: 50 |  Loss: (0.0864) | Acc: (97.06%) (6336/6528)\n",
            "Epoch: 111 | Batch_idx: 60 |  Loss: (0.0886) | Acc: (96.94%) (7569/7808)\n",
            "Epoch: 111 | Batch_idx: 70 |  Loss: (0.0886) | Acc: (97.01%) (8816/9088)\n",
            "Epoch: 111 | Batch_idx: 80 |  Loss: (0.0877) | Acc: (97.11%) (10068/10368)\n",
            "Epoch: 111 | Batch_idx: 90 |  Loss: (0.0872) | Acc: (97.06%) (11306/11648)\n",
            "Epoch: 111 | Batch_idx: 100 |  Loss: (0.0897) | Acc: (97.00%) (12540/12928)\n",
            "Epoch: 111 | Batch_idx: 110 |  Loss: (0.0880) | Acc: (97.07%) (13792/14208)\n",
            "Epoch: 111 | Batch_idx: 120 |  Loss: (0.0871) | Acc: (97.11%) (15040/15488)\n",
            "Epoch: 111 | Batch_idx: 130 |  Loss: (0.0870) | Acc: (97.08%) (16278/16768)\n",
            "Epoch: 111 | Batch_idx: 140 |  Loss: (0.0876) | Acc: (97.04%) (17514/18048)\n",
            "Epoch: 111 | Batch_idx: 150 |  Loss: (0.0880) | Acc: (97.03%) (18753/19328)\n",
            "Epoch: 111 | Batch_idx: 160 |  Loss: (0.0873) | Acc: (97.04%) (19998/20608)\n",
            "Epoch: 111 | Batch_idx: 170 |  Loss: (0.0879) | Acc: (97.02%) (21235/21888)\n",
            "Epoch: 111 | Batch_idx: 180 |  Loss: (0.0889) | Acc: (97.00%) (22474/23168)\n",
            "Epoch: 111 | Batch_idx: 190 |  Loss: (0.0904) | Acc: (96.96%) (23705/24448)\n",
            "Epoch: 111 | Batch_idx: 200 |  Loss: (0.0903) | Acc: (96.97%) (24949/25728)\n",
            "Epoch: 111 | Batch_idx: 210 |  Loss: (0.0905) | Acc: (96.97%) (26190/27008)\n",
            "Epoch: 111 | Batch_idx: 220 |  Loss: (0.0908) | Acc: (96.94%) (27423/28288)\n",
            "Epoch: 111 | Batch_idx: 230 |  Loss: (0.0912) | Acc: (96.93%) (28660/29568)\n",
            "Epoch: 111 | Batch_idx: 240 |  Loss: (0.0923) | Acc: (96.91%) (29896/30848)\n",
            "Epoch: 111 | Batch_idx: 250 |  Loss: (0.0920) | Acc: (96.92%) (31137/32128)\n",
            "Epoch: 111 | Batch_idx: 260 |  Loss: (0.0925) | Acc: (96.89%) (32369/33408)\n",
            "Epoch: 111 | Batch_idx: 270 |  Loss: (0.0923) | Acc: (96.91%) (33615/34688)\n",
            "Epoch: 111 | Batch_idx: 280 |  Loss: (0.0915) | Acc: (96.92%) (34860/35968)\n",
            "Epoch: 111 | Batch_idx: 290 |  Loss: (0.0910) | Acc: (96.94%) (36110/37248)\n",
            "Epoch: 111 | Batch_idx: 300 |  Loss: (0.0914) | Acc: (96.93%) (37347/38528)\n",
            "Epoch: 111 | Batch_idx: 310 |  Loss: (0.0917) | Acc: (96.92%) (38581/39808)\n",
            "Epoch: 111 | Batch_idx: 320 |  Loss: (0.0916) | Acc: (96.93%) (39827/41088)\n",
            "Epoch: 111 | Batch_idx: 330 |  Loss: (0.0915) | Acc: (96.93%) (41068/42368)\n",
            "Epoch: 111 | Batch_idx: 340 |  Loss: (0.0917) | Acc: (96.92%) (42302/43648)\n",
            "Epoch: 111 | Batch_idx: 350 |  Loss: (0.0921) | Acc: (96.89%) (43530/44928)\n",
            "Epoch: 111 | Batch_idx: 360 |  Loss: (0.0920) | Acc: (96.88%) (44768/46208)\n",
            "Epoch: 111 | Batch_idx: 370 |  Loss: (0.0919) | Acc: (96.89%) (46010/47488)\n",
            "Epoch: 111 | Batch_idx: 380 |  Loss: (0.0922) | Acc: (96.88%) (47245/48768)\n",
            "Epoch: 111 | Batch_idx: 390 |  Loss: (0.0919) | Acc: (96.89%) (48445/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3189) | Acc: (91.27%) (9127/10000)\n",
            "Epoch: 112 | Batch_idx: 0 |  Loss: (0.0808) | Acc: (96.88%) (124/128)\n",
            "Epoch: 112 | Batch_idx: 10 |  Loss: (0.0939) | Acc: (96.95%) (1365/1408)\n",
            "Epoch: 112 | Batch_idx: 20 |  Loss: (0.0847) | Acc: (97.14%) (2611/2688)\n",
            "Epoch: 112 | Batch_idx: 30 |  Loss: (0.0946) | Acc: (96.95%) (3847/3968)\n",
            "Epoch: 112 | Batch_idx: 40 |  Loss: (0.0939) | Acc: (96.97%) (5089/5248)\n",
            "Epoch: 112 | Batch_idx: 50 |  Loss: (0.0935) | Acc: (96.89%) (6325/6528)\n",
            "Epoch: 112 | Batch_idx: 60 |  Loss: (0.0907) | Acc: (97.02%) (7575/7808)\n",
            "Epoch: 112 | Batch_idx: 70 |  Loss: (0.0906) | Acc: (97.04%) (8819/9088)\n",
            "Epoch: 112 | Batch_idx: 80 |  Loss: (0.0888) | Acc: (97.10%) (10067/10368)\n",
            "Epoch: 112 | Batch_idx: 90 |  Loss: (0.0896) | Acc: (97.07%) (11307/11648)\n",
            "Epoch: 112 | Batch_idx: 100 |  Loss: (0.0901) | Acc: (97.05%) (12547/12928)\n",
            "Epoch: 112 | Batch_idx: 110 |  Loss: (0.0908) | Acc: (97.01%) (13783/14208)\n",
            "Epoch: 112 | Batch_idx: 120 |  Loss: (0.0916) | Acc: (96.97%) (15019/15488)\n",
            "Epoch: 112 | Batch_idx: 130 |  Loss: (0.0913) | Acc: (96.96%) (16259/16768)\n",
            "Epoch: 112 | Batch_idx: 140 |  Loss: (0.0909) | Acc: (96.97%) (17501/18048)\n",
            "Epoch: 112 | Batch_idx: 150 |  Loss: (0.0908) | Acc: (96.97%) (18743/19328)\n",
            "Epoch: 112 | Batch_idx: 160 |  Loss: (0.0923) | Acc: (96.93%) (19975/20608)\n",
            "Epoch: 112 | Batch_idx: 170 |  Loss: (0.0915) | Acc: (96.97%) (21225/21888)\n",
            "Epoch: 112 | Batch_idx: 180 |  Loss: (0.0920) | Acc: (96.93%) (22457/23168)\n",
            "Epoch: 112 | Batch_idx: 190 |  Loss: (0.0916) | Acc: (96.96%) (23706/24448)\n",
            "Epoch: 112 | Batch_idx: 200 |  Loss: (0.0921) | Acc: (96.94%) (24940/25728)\n",
            "Epoch: 112 | Batch_idx: 210 |  Loss: (0.0923) | Acc: (96.94%) (26181/27008)\n",
            "Epoch: 112 | Batch_idx: 220 |  Loss: (0.0930) | Acc: (96.92%) (27418/28288)\n",
            "Epoch: 112 | Batch_idx: 230 |  Loss: (0.0929) | Acc: (96.92%) (28657/29568)\n",
            "Epoch: 112 | Batch_idx: 240 |  Loss: (0.0927) | Acc: (96.93%) (29902/30848)\n",
            "Epoch: 112 | Batch_idx: 250 |  Loss: (0.0932) | Acc: (96.91%) (31136/32128)\n",
            "Epoch: 112 | Batch_idx: 260 |  Loss: (0.0942) | Acc: (96.88%) (32364/33408)\n",
            "Epoch: 112 | Batch_idx: 270 |  Loss: (0.0938) | Acc: (96.88%) (33607/34688)\n",
            "Epoch: 112 | Batch_idx: 280 |  Loss: (0.0938) | Acc: (96.88%) (34847/35968)\n",
            "Epoch: 112 | Batch_idx: 290 |  Loss: (0.0933) | Acc: (96.90%) (36094/37248)\n",
            "Epoch: 112 | Batch_idx: 300 |  Loss: (0.0927) | Acc: (96.91%) (37339/38528)\n",
            "Epoch: 112 | Batch_idx: 310 |  Loss: (0.0931) | Acc: (96.91%) (38577/39808)\n",
            "Epoch: 112 | Batch_idx: 320 |  Loss: (0.0929) | Acc: (96.93%) (39825/41088)\n",
            "Epoch: 112 | Batch_idx: 330 |  Loss: (0.0931) | Acc: (96.92%) (41063/42368)\n",
            "Epoch: 112 | Batch_idx: 340 |  Loss: (0.0938) | Acc: (96.90%) (42293/43648)\n",
            "Epoch: 112 | Batch_idx: 350 |  Loss: (0.0940) | Acc: (96.88%) (43524/44928)\n",
            "Epoch: 112 | Batch_idx: 360 |  Loss: (0.0937) | Acc: (96.88%) (44765/46208)\n",
            "Epoch: 112 | Batch_idx: 370 |  Loss: (0.0936) | Acc: (96.88%) (46008/47488)\n",
            "Epoch: 112 | Batch_idx: 380 |  Loss: (0.0935) | Acc: (96.88%) (47246/48768)\n",
            "Epoch: 112 | Batch_idx: 390 |  Loss: (0.0940) | Acc: (96.84%) (48419/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3286) | Acc: (91.16%) (9116/10000)\n",
            "Epoch: 113 | Batch_idx: 0 |  Loss: (0.0714) | Acc: (98.44%) (126/128)\n",
            "Epoch: 113 | Batch_idx: 10 |  Loss: (0.0782) | Acc: (97.09%) (1367/1408)\n",
            "Epoch: 113 | Batch_idx: 20 |  Loss: (0.0844) | Acc: (97.14%) (2611/2688)\n",
            "Epoch: 113 | Batch_idx: 30 |  Loss: (0.0831) | Acc: (97.23%) (3858/3968)\n",
            "Epoch: 113 | Batch_idx: 40 |  Loss: (0.0881) | Acc: (97.10%) (5096/5248)\n",
            "Epoch: 113 | Batch_idx: 50 |  Loss: (0.0888) | Acc: (97.10%) (6339/6528)\n",
            "Epoch: 113 | Batch_idx: 60 |  Loss: (0.0860) | Acc: (97.23%) (7592/7808)\n",
            "Epoch: 113 | Batch_idx: 70 |  Loss: (0.0880) | Acc: (97.04%) (8819/9088)\n",
            "Epoch: 113 | Batch_idx: 80 |  Loss: (0.0877) | Acc: (97.08%) (10065/10368)\n",
            "Epoch: 113 | Batch_idx: 90 |  Loss: (0.0863) | Acc: (97.15%) (11316/11648)\n",
            "Epoch: 113 | Batch_idx: 100 |  Loss: (0.0860) | Acc: (97.15%) (12559/12928)\n",
            "Epoch: 113 | Batch_idx: 110 |  Loss: (0.0868) | Acc: (97.11%) (13798/14208)\n",
            "Epoch: 113 | Batch_idx: 120 |  Loss: (0.0860) | Acc: (97.14%) (15045/15488)\n",
            "Epoch: 113 | Batch_idx: 130 |  Loss: (0.0863) | Acc: (97.14%) (16288/16768)\n",
            "Epoch: 113 | Batch_idx: 140 |  Loss: (0.0874) | Acc: (97.11%) (17527/18048)\n",
            "Epoch: 113 | Batch_idx: 150 |  Loss: (0.0875) | Acc: (97.09%) (18765/19328)\n",
            "Epoch: 113 | Batch_idx: 160 |  Loss: (0.0877) | Acc: (97.08%) (20006/20608)\n",
            "Epoch: 113 | Batch_idx: 170 |  Loss: (0.0872) | Acc: (97.11%) (21255/21888)\n",
            "Epoch: 113 | Batch_idx: 180 |  Loss: (0.0884) | Acc: (97.05%) (22484/23168)\n",
            "Epoch: 113 | Batch_idx: 190 |  Loss: (0.0888) | Acc: (97.02%) (23719/24448)\n",
            "Epoch: 113 | Batch_idx: 200 |  Loss: (0.0884) | Acc: (97.03%) (24965/25728)\n",
            "Epoch: 113 | Batch_idx: 210 |  Loss: (0.0882) | Acc: (97.04%) (26209/27008)\n",
            "Epoch: 113 | Batch_idx: 220 |  Loss: (0.0896) | Acc: (96.98%) (27434/28288)\n",
            "Epoch: 113 | Batch_idx: 230 |  Loss: (0.0895) | Acc: (96.97%) (28672/29568)\n",
            "Epoch: 113 | Batch_idx: 240 |  Loss: (0.0886) | Acc: (97.00%) (29922/30848)\n",
            "Epoch: 113 | Batch_idx: 250 |  Loss: (0.0882) | Acc: (97.00%) (31163/32128)\n",
            "Epoch: 113 | Batch_idx: 260 |  Loss: (0.0882) | Acc: (97.00%) (32406/33408)\n",
            "Epoch: 113 | Batch_idx: 270 |  Loss: (0.0878) | Acc: (97.02%) (33654/34688)\n",
            "Epoch: 113 | Batch_idx: 280 |  Loss: (0.0886) | Acc: (96.99%) (34887/35968)\n",
            "Epoch: 113 | Batch_idx: 290 |  Loss: (0.0888) | Acc: (96.99%) (36126/37248)\n",
            "Epoch: 113 | Batch_idx: 300 |  Loss: (0.0897) | Acc: (96.96%) (37357/38528)\n",
            "Epoch: 113 | Batch_idx: 310 |  Loss: (0.0893) | Acc: (96.99%) (38610/39808)\n",
            "Epoch: 113 | Batch_idx: 320 |  Loss: (0.0890) | Acc: (97.01%) (39858/41088)\n",
            "Epoch: 113 | Batch_idx: 330 |  Loss: (0.0887) | Acc: (97.01%) (41100/42368)\n",
            "Epoch: 113 | Batch_idx: 340 |  Loss: (0.0891) | Acc: (97.01%) (42343/43648)\n",
            "Epoch: 113 | Batch_idx: 350 |  Loss: (0.0895) | Acc: (97.00%) (43582/44928)\n",
            "Epoch: 113 | Batch_idx: 360 |  Loss: (0.0898) | Acc: (96.99%) (44817/46208)\n",
            "Epoch: 113 | Batch_idx: 370 |  Loss: (0.0907) | Acc: (96.97%) (46048/47488)\n",
            "Epoch: 113 | Batch_idx: 380 |  Loss: (0.0908) | Acc: (96.96%) (47287/48768)\n",
            "Epoch: 113 | Batch_idx: 390 |  Loss: (0.0911) | Acc: (96.95%) (48473/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3277) | Acc: (91.04%) (9104/10000)\n",
            "Epoch: 114 | Batch_idx: 0 |  Loss: (0.0497) | Acc: (98.44%) (126/128)\n",
            "Epoch: 114 | Batch_idx: 10 |  Loss: (0.1003) | Acc: (97.09%) (1367/1408)\n",
            "Epoch: 114 | Batch_idx: 20 |  Loss: (0.0953) | Acc: (97.14%) (2611/2688)\n",
            "Epoch: 114 | Batch_idx: 30 |  Loss: (0.0913) | Acc: (97.18%) (3856/3968)\n",
            "Epoch: 114 | Batch_idx: 40 |  Loss: (0.0921) | Acc: (97.10%) (5096/5248)\n",
            "Epoch: 114 | Batch_idx: 50 |  Loss: (0.0889) | Acc: (97.18%) (6344/6528)\n",
            "Epoch: 114 | Batch_idx: 60 |  Loss: (0.0904) | Acc: (97.09%) (7581/7808)\n",
            "Epoch: 114 | Batch_idx: 70 |  Loss: (0.0926) | Acc: (97.04%) (8819/9088)\n",
            "Epoch: 114 | Batch_idx: 80 |  Loss: (0.0911) | Acc: (97.04%) (10061/10368)\n",
            "Epoch: 114 | Batch_idx: 90 |  Loss: (0.0912) | Acc: (97.07%) (11307/11648)\n",
            "Epoch: 114 | Batch_idx: 100 |  Loss: (0.0878) | Acc: (97.18%) (12564/12928)\n",
            "Epoch: 114 | Batch_idx: 110 |  Loss: (0.0891) | Acc: (97.15%) (13803/14208)\n",
            "Epoch: 114 | Batch_idx: 120 |  Loss: (0.0896) | Acc: (97.09%) (15037/15488)\n",
            "Epoch: 114 | Batch_idx: 130 |  Loss: (0.0893) | Acc: (97.11%) (16283/16768)\n",
            "Epoch: 114 | Batch_idx: 140 |  Loss: (0.0895) | Acc: (97.09%) (17522/18048)\n",
            "Epoch: 114 | Batch_idx: 150 |  Loss: (0.0878) | Acc: (97.14%) (18775/19328)\n",
            "Epoch: 114 | Batch_idx: 160 |  Loss: (0.0878) | Acc: (97.16%) (20022/20608)\n",
            "Epoch: 114 | Batch_idx: 170 |  Loss: (0.0873) | Acc: (97.15%) (21264/21888)\n",
            "Epoch: 114 | Batch_idx: 180 |  Loss: (0.0879) | Acc: (97.13%) (22502/23168)\n",
            "Epoch: 114 | Batch_idx: 190 |  Loss: (0.0883) | Acc: (97.12%) (23744/24448)\n",
            "Epoch: 114 | Batch_idx: 200 |  Loss: (0.0878) | Acc: (97.12%) (24988/25728)\n",
            "Epoch: 114 | Batch_idx: 210 |  Loss: (0.0874) | Acc: (97.13%) (26234/27008)\n",
            "Epoch: 114 | Batch_idx: 220 |  Loss: (0.0880) | Acc: (97.09%) (27466/28288)\n",
            "Epoch: 114 | Batch_idx: 230 |  Loss: (0.0876) | Acc: (97.09%) (28709/29568)\n",
            "Epoch: 114 | Batch_idx: 240 |  Loss: (0.0884) | Acc: (97.07%) (29943/30848)\n",
            "Epoch: 114 | Batch_idx: 250 |  Loss: (0.0887) | Acc: (97.02%) (31172/32128)\n",
            "Epoch: 114 | Batch_idx: 260 |  Loss: (0.0883) | Acc: (97.02%) (32413/33408)\n",
            "Epoch: 114 | Batch_idx: 270 |  Loss: (0.0880) | Acc: (97.04%) (33661/34688)\n",
            "Epoch: 114 | Batch_idx: 280 |  Loss: (0.0885) | Acc: (97.04%) (34902/35968)\n",
            "Epoch: 114 | Batch_idx: 290 |  Loss: (0.0884) | Acc: (97.04%) (36146/37248)\n",
            "Epoch: 114 | Batch_idx: 300 |  Loss: (0.0885) | Acc: (97.04%) (37387/38528)\n",
            "Epoch: 114 | Batch_idx: 310 |  Loss: (0.0884) | Acc: (97.03%) (38627/39808)\n",
            "Epoch: 114 | Batch_idx: 320 |  Loss: (0.0885) | Acc: (97.02%) (39863/41088)\n",
            "Epoch: 114 | Batch_idx: 330 |  Loss: (0.0885) | Acc: (97.01%) (41103/42368)\n",
            "Epoch: 114 | Batch_idx: 340 |  Loss: (0.0893) | Acc: (97.00%) (42337/43648)\n",
            "Epoch: 114 | Batch_idx: 350 |  Loss: (0.0889) | Acc: (97.01%) (43586/44928)\n",
            "Epoch: 114 | Batch_idx: 360 |  Loss: (0.0893) | Acc: (97.00%) (44824/46208)\n",
            "Epoch: 114 | Batch_idx: 370 |  Loss: (0.0890) | Acc: (97.02%) (46071/47488)\n",
            "Epoch: 114 | Batch_idx: 380 |  Loss: (0.0888) | Acc: (97.02%) (47316/48768)\n",
            "Epoch: 114 | Batch_idx: 390 |  Loss: (0.0896) | Acc: (96.99%) (48496/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3290) | Acc: (91.19%) (9119/10000)\n",
            "Epoch: 115 | Batch_idx: 0 |  Loss: (0.1039) | Acc: (96.09%) (123/128)\n",
            "Epoch: 115 | Batch_idx: 10 |  Loss: (0.0986) | Acc: (96.73%) (1362/1408)\n",
            "Epoch: 115 | Batch_idx: 20 |  Loss: (0.0889) | Acc: (96.95%) (2606/2688)\n",
            "Epoch: 115 | Batch_idx: 30 |  Loss: (0.0855) | Acc: (97.10%) (3853/3968)\n",
            "Epoch: 115 | Batch_idx: 40 |  Loss: (0.0820) | Acc: (97.28%) (5105/5248)\n",
            "Epoch: 115 | Batch_idx: 50 |  Loss: (0.0811) | Acc: (97.35%) (6355/6528)\n",
            "Epoch: 115 | Batch_idx: 60 |  Loss: (0.0823) | Acc: (97.32%) (7599/7808)\n",
            "Epoch: 115 | Batch_idx: 70 |  Loss: (0.0815) | Acc: (97.32%) (8844/9088)\n",
            "Epoch: 115 | Batch_idx: 80 |  Loss: (0.0836) | Acc: (97.23%) (10081/10368)\n",
            "Epoch: 115 | Batch_idx: 90 |  Loss: (0.0829) | Acc: (97.23%) (11325/11648)\n",
            "Epoch: 115 | Batch_idx: 100 |  Loss: (0.0828) | Acc: (97.20%) (12566/12928)\n",
            "Epoch: 115 | Batch_idx: 110 |  Loss: (0.0835) | Acc: (97.17%) (13806/14208)\n",
            "Epoch: 115 | Batch_idx: 120 |  Loss: (0.0829) | Acc: (97.18%) (15052/15488)\n",
            "Epoch: 115 | Batch_idx: 130 |  Loss: (0.0851) | Acc: (97.08%) (16279/16768)\n",
            "Epoch: 115 | Batch_idx: 140 |  Loss: (0.0838) | Acc: (97.13%) (17530/18048)\n",
            "Epoch: 115 | Batch_idx: 150 |  Loss: (0.0831) | Acc: (97.17%) (18781/19328)\n",
            "Epoch: 115 | Batch_idx: 160 |  Loss: (0.0839) | Acc: (97.17%) (20024/20608)\n",
            "Epoch: 115 | Batch_idx: 170 |  Loss: (0.0843) | Acc: (97.15%) (21264/21888)\n",
            "Epoch: 115 | Batch_idx: 180 |  Loss: (0.0839) | Acc: (97.16%) (22510/23168)\n",
            "Epoch: 115 | Batch_idx: 190 |  Loss: (0.0855) | Acc: (97.10%) (23738/24448)\n",
            "Epoch: 115 | Batch_idx: 200 |  Loss: (0.0859) | Acc: (97.08%) (24977/25728)\n",
            "Epoch: 115 | Batch_idx: 210 |  Loss: (0.0858) | Acc: (97.10%) (26225/27008)\n",
            "Epoch: 115 | Batch_idx: 220 |  Loss: (0.0855) | Acc: (97.09%) (27466/28288)\n",
            "Epoch: 115 | Batch_idx: 230 |  Loss: (0.0851) | Acc: (97.11%) (28714/29568)\n",
            "Epoch: 115 | Batch_idx: 240 |  Loss: (0.0854) | Acc: (97.09%) (29950/30848)\n",
            "Epoch: 115 | Batch_idx: 250 |  Loss: (0.0857) | Acc: (97.09%) (31194/32128)\n",
            "Epoch: 115 | Batch_idx: 260 |  Loss: (0.0869) | Acc: (97.05%) (32421/33408)\n",
            "Epoch: 115 | Batch_idx: 270 |  Loss: (0.0870) | Acc: (97.03%) (33659/34688)\n",
            "Epoch: 115 | Batch_idx: 280 |  Loss: (0.0874) | Acc: (97.03%) (34898/35968)\n",
            "Epoch: 115 | Batch_idx: 290 |  Loss: (0.0881) | Acc: (97.03%) (36141/37248)\n",
            "Epoch: 115 | Batch_idx: 300 |  Loss: (0.0881) | Acc: (97.03%) (37383/38528)\n",
            "Epoch: 115 | Batch_idx: 310 |  Loss: (0.0892) | Acc: (97.00%) (38613/39808)\n",
            "Epoch: 115 | Batch_idx: 320 |  Loss: (0.0889) | Acc: (97.02%) (39863/41088)\n",
            "Epoch: 115 | Batch_idx: 330 |  Loss: (0.0886) | Acc: (97.02%) (41107/42368)\n",
            "Epoch: 115 | Batch_idx: 340 |  Loss: (0.0884) | Acc: (97.03%) (42352/43648)\n",
            "Epoch: 115 | Batch_idx: 350 |  Loss: (0.0881) | Acc: (97.05%) (43603/44928)\n",
            "Epoch: 115 | Batch_idx: 360 |  Loss: (0.0881) | Acc: (97.04%) (44842/46208)\n",
            "Epoch: 115 | Batch_idx: 370 |  Loss: (0.0879) | Acc: (97.06%) (46091/47488)\n",
            "Epoch: 115 | Batch_idx: 380 |  Loss: (0.0880) | Acc: (97.05%) (47331/48768)\n",
            "Epoch: 115 | Batch_idx: 390 |  Loss: (0.0879) | Acc: (97.06%) (48530/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3331) | Acc: (91.15%) (9115/10000)\n",
            "Epoch: 116 | Batch_idx: 0 |  Loss: (0.0617) | Acc: (98.44%) (126/128)\n",
            "Epoch: 116 | Batch_idx: 10 |  Loss: (0.0786) | Acc: (97.44%) (1372/1408)\n",
            "Epoch: 116 | Batch_idx: 20 |  Loss: (0.0791) | Acc: (97.40%) (2618/2688)\n",
            "Epoch: 116 | Batch_idx: 30 |  Loss: (0.0830) | Acc: (97.28%) (3860/3968)\n",
            "Epoch: 116 | Batch_idx: 40 |  Loss: (0.0856) | Acc: (96.99%) (5090/5248)\n",
            "Epoch: 116 | Batch_idx: 50 |  Loss: (0.0845) | Acc: (97.07%) (6337/6528)\n",
            "Epoch: 116 | Batch_idx: 60 |  Loss: (0.0844) | Acc: (97.04%) (7577/7808)\n",
            "Epoch: 116 | Batch_idx: 70 |  Loss: (0.0848) | Acc: (97.08%) (8823/9088)\n",
            "Epoch: 116 | Batch_idx: 80 |  Loss: (0.0825) | Acc: (97.15%) (10073/10368)\n",
            "Epoch: 116 | Batch_idx: 90 |  Loss: (0.0819) | Acc: (97.20%) (11322/11648)\n",
            "Epoch: 116 | Batch_idx: 100 |  Loss: (0.0818) | Acc: (97.22%) (12569/12928)\n",
            "Epoch: 116 | Batch_idx: 110 |  Loss: (0.0830) | Acc: (97.18%) (13807/14208)\n",
            "Epoch: 116 | Batch_idx: 120 |  Loss: (0.0835) | Acc: (97.16%) (15048/15488)\n",
            "Epoch: 116 | Batch_idx: 130 |  Loss: (0.0826) | Acc: (97.20%) (16299/16768)\n",
            "Epoch: 116 | Batch_idx: 140 |  Loss: (0.0829) | Acc: (97.19%) (17540/18048)\n",
            "Epoch: 116 | Batch_idx: 150 |  Loss: (0.0833) | Acc: (97.14%) (18776/19328)\n",
            "Epoch: 116 | Batch_idx: 160 |  Loss: (0.0833) | Acc: (97.17%) (20024/20608)\n",
            "Epoch: 116 | Batch_idx: 170 |  Loss: (0.0839) | Acc: (97.14%) (21262/21888)\n",
            "Epoch: 116 | Batch_idx: 180 |  Loss: (0.0845) | Acc: (97.13%) (22503/23168)\n",
            "Epoch: 116 | Batch_idx: 190 |  Loss: (0.0855) | Acc: (97.07%) (23731/24448)\n",
            "Epoch: 116 | Batch_idx: 200 |  Loss: (0.0857) | Acc: (97.06%) (24972/25728)\n",
            "Epoch: 116 | Batch_idx: 210 |  Loss: (0.0862) | Acc: (97.06%) (26213/27008)\n",
            "Epoch: 116 | Batch_idx: 220 |  Loss: (0.0863) | Acc: (97.06%) (27455/28288)\n",
            "Epoch: 116 | Batch_idx: 230 |  Loss: (0.0860) | Acc: (97.04%) (28694/29568)\n",
            "Epoch: 116 | Batch_idx: 240 |  Loss: (0.0861) | Acc: (97.05%) (29938/30848)\n",
            "Epoch: 116 | Batch_idx: 250 |  Loss: (0.0851) | Acc: (97.09%) (31192/32128)\n",
            "Epoch: 116 | Batch_idx: 260 |  Loss: (0.0854) | Acc: (97.05%) (32424/33408)\n",
            "Epoch: 116 | Batch_idx: 270 |  Loss: (0.0860) | Acc: (97.04%) (33660/34688)\n",
            "Epoch: 116 | Batch_idx: 280 |  Loss: (0.0858) | Acc: (97.05%) (34906/35968)\n",
            "Epoch: 116 | Batch_idx: 290 |  Loss: (0.0851) | Acc: (97.07%) (36156/37248)\n",
            "Epoch: 116 | Batch_idx: 300 |  Loss: (0.0854) | Acc: (97.09%) (37406/38528)\n",
            "Epoch: 116 | Batch_idx: 310 |  Loss: (0.0856) | Acc: (97.08%) (38646/39808)\n",
            "Epoch: 116 | Batch_idx: 320 |  Loss: (0.0865) | Acc: (97.05%) (39877/41088)\n",
            "Epoch: 116 | Batch_idx: 330 |  Loss: (0.0867) | Acc: (97.06%) (41123/42368)\n",
            "Epoch: 116 | Batch_idx: 340 |  Loss: (0.0867) | Acc: (97.07%) (42368/43648)\n",
            "Epoch: 116 | Batch_idx: 350 |  Loss: (0.0864) | Acc: (97.06%) (43608/44928)\n",
            "Epoch: 116 | Batch_idx: 360 |  Loss: (0.0860) | Acc: (97.08%) (44859/46208)\n",
            "Epoch: 116 | Batch_idx: 370 |  Loss: (0.0859) | Acc: (97.09%) (46105/47488)\n",
            "Epoch: 116 | Batch_idx: 380 |  Loss: (0.0860) | Acc: (97.08%) (47346/48768)\n",
            "Epoch: 116 | Batch_idx: 390 |  Loss: (0.0863) | Acc: (97.07%) (48534/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3343) | Acc: (90.98%) (9098/10000)\n",
            "Epoch: 117 | Batch_idx: 0 |  Loss: (0.0545) | Acc: (97.66%) (125/128)\n",
            "Epoch: 117 | Batch_idx: 10 |  Loss: (0.1005) | Acc: (96.88%) (1364/1408)\n",
            "Epoch: 117 | Batch_idx: 20 |  Loss: (0.0933) | Acc: (96.73%) (2600/2688)\n",
            "Epoch: 117 | Batch_idx: 30 |  Loss: (0.0988) | Acc: (96.65%) (3835/3968)\n",
            "Epoch: 117 | Batch_idx: 40 |  Loss: (0.0948) | Acc: (96.70%) (5075/5248)\n",
            "Epoch: 117 | Batch_idx: 50 |  Loss: (0.0946) | Acc: (96.74%) (6315/6528)\n",
            "Epoch: 117 | Batch_idx: 60 |  Loss: (0.0902) | Acc: (96.84%) (7561/7808)\n",
            "Epoch: 117 | Batch_idx: 70 |  Loss: (0.0866) | Acc: (96.97%) (8813/9088)\n",
            "Epoch: 117 | Batch_idx: 80 |  Loss: (0.0849) | Acc: (96.97%) (10054/10368)\n",
            "Epoch: 117 | Batch_idx: 90 |  Loss: (0.0857) | Acc: (96.98%) (11296/11648)\n",
            "Epoch: 117 | Batch_idx: 100 |  Loss: (0.0869) | Acc: (96.98%) (12538/12928)\n",
            "Epoch: 117 | Batch_idx: 110 |  Loss: (0.0857) | Acc: (97.00%) (13782/14208)\n",
            "Epoch: 117 | Batch_idx: 120 |  Loss: (0.0864) | Acc: (96.97%) (15018/15488)\n",
            "Epoch: 117 | Batch_idx: 130 |  Loss: (0.0859) | Acc: (96.98%) (16262/16768)\n",
            "Epoch: 117 | Batch_idx: 140 |  Loss: (0.0838) | Acc: (97.06%) (17517/18048)\n",
            "Epoch: 117 | Batch_idx: 150 |  Loss: (0.0835) | Acc: (97.06%) (18760/19328)\n",
            "Epoch: 117 | Batch_idx: 160 |  Loss: (0.0825) | Acc: (97.11%) (20013/20608)\n",
            "Epoch: 117 | Batch_idx: 170 |  Loss: (0.0825) | Acc: (97.10%) (21253/21888)\n",
            "Epoch: 117 | Batch_idx: 180 |  Loss: (0.0817) | Acc: (97.11%) (22499/23168)\n",
            "Epoch: 117 | Batch_idx: 190 |  Loss: (0.0813) | Acc: (97.12%) (23743/24448)\n",
            "Epoch: 117 | Batch_idx: 200 |  Loss: (0.0813) | Acc: (97.12%) (24986/25728)\n",
            "Epoch: 117 | Batch_idx: 210 |  Loss: (0.0824) | Acc: (97.06%) (26213/27008)\n",
            "Epoch: 117 | Batch_idx: 220 |  Loss: (0.0823) | Acc: (97.06%) (27455/28288)\n",
            "Epoch: 117 | Batch_idx: 230 |  Loss: (0.0830) | Acc: (97.02%) (28687/29568)\n",
            "Epoch: 117 | Batch_idx: 240 |  Loss: (0.0825) | Acc: (97.06%) (29940/30848)\n",
            "Epoch: 117 | Batch_idx: 250 |  Loss: (0.0823) | Acc: (97.08%) (31190/32128)\n",
            "Epoch: 117 | Batch_idx: 260 |  Loss: (0.0820) | Acc: (97.09%) (32435/33408)\n",
            "Epoch: 117 | Batch_idx: 270 |  Loss: (0.0818) | Acc: (97.11%) (33684/34688)\n",
            "Epoch: 117 | Batch_idx: 280 |  Loss: (0.0822) | Acc: (97.10%) (34926/35968)\n",
            "Epoch: 117 | Batch_idx: 290 |  Loss: (0.0811) | Acc: (97.15%) (36187/37248)\n",
            "Epoch: 117 | Batch_idx: 300 |  Loss: (0.0815) | Acc: (97.13%) (37424/38528)\n",
            "Epoch: 117 | Batch_idx: 310 |  Loss: (0.0815) | Acc: (97.14%) (38669/39808)\n",
            "Epoch: 117 | Batch_idx: 320 |  Loss: (0.0816) | Acc: (97.14%) (39912/41088)\n",
            "Epoch: 117 | Batch_idx: 330 |  Loss: (0.0811) | Acc: (97.16%) (41164/42368)\n",
            "Epoch: 117 | Batch_idx: 340 |  Loss: (0.0812) | Acc: (97.16%) (42410/43648)\n",
            "Epoch: 117 | Batch_idx: 350 |  Loss: (0.0819) | Acc: (97.15%) (43648/44928)\n",
            "Epoch: 117 | Batch_idx: 360 |  Loss: (0.0825) | Acc: (97.13%) (44883/46208)\n",
            "Epoch: 117 | Batch_idx: 370 |  Loss: (0.0828) | Acc: (97.12%) (46122/47488)\n",
            "Epoch: 117 | Batch_idx: 380 |  Loss: (0.0833) | Acc: (97.11%) (47361/48768)\n",
            "Epoch: 117 | Batch_idx: 390 |  Loss: (0.0836) | Acc: (97.11%) (48556/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3415) | Acc: (91.09%) (9109/10000)\n",
            "Epoch: 118 | Batch_idx: 0 |  Loss: (0.0814) | Acc: (96.09%) (123/128)\n",
            "Epoch: 118 | Batch_idx: 10 |  Loss: (0.0898) | Acc: (96.80%) (1363/1408)\n",
            "Epoch: 118 | Batch_idx: 20 |  Loss: (0.0944) | Acc: (97.10%) (2610/2688)\n",
            "Epoch: 118 | Batch_idx: 30 |  Loss: (0.0914) | Acc: (96.98%) (3848/3968)\n",
            "Epoch: 118 | Batch_idx: 40 |  Loss: (0.0879) | Acc: (97.10%) (5096/5248)\n",
            "Epoch: 118 | Batch_idx: 50 |  Loss: (0.0858) | Acc: (97.17%) (6343/6528)\n",
            "Epoch: 118 | Batch_idx: 60 |  Loss: (0.0839) | Acc: (97.21%) (7590/7808)\n",
            "Epoch: 118 | Batch_idx: 70 |  Loss: (0.0837) | Acc: (97.24%) (8837/9088)\n",
            "Epoch: 118 | Batch_idx: 80 |  Loss: (0.0880) | Acc: (97.11%) (10068/10368)\n",
            "Epoch: 118 | Batch_idx: 90 |  Loss: (0.0872) | Acc: (97.08%) (11308/11648)\n",
            "Epoch: 118 | Batch_idx: 100 |  Loss: (0.0860) | Acc: (97.08%) (12550/12928)\n",
            "Epoch: 118 | Batch_idx: 110 |  Loss: (0.0855) | Acc: (97.07%) (13792/14208)\n",
            "Epoch: 118 | Batch_idx: 120 |  Loss: (0.0850) | Acc: (97.09%) (15038/15488)\n",
            "Epoch: 118 | Batch_idx: 130 |  Loss: (0.0831) | Acc: (97.18%) (16295/16768)\n",
            "Epoch: 118 | Batch_idx: 140 |  Loss: (0.0823) | Acc: (97.18%) (17539/18048)\n",
            "Epoch: 118 | Batch_idx: 150 |  Loss: (0.0821) | Acc: (97.19%) (18784/19328)\n",
            "Epoch: 118 | Batch_idx: 160 |  Loss: (0.0822) | Acc: (97.20%) (20031/20608)\n",
            "Epoch: 118 | Batch_idx: 170 |  Loss: (0.0821) | Acc: (97.20%) (21276/21888)\n",
            "Epoch: 118 | Batch_idx: 180 |  Loss: (0.0829) | Acc: (97.19%) (22517/23168)\n",
            "Epoch: 118 | Batch_idx: 190 |  Loss: (0.0830) | Acc: (97.16%) (23754/24448)\n",
            "Epoch: 118 | Batch_idx: 200 |  Loss: (0.0830) | Acc: (97.15%) (24996/25728)\n",
            "Epoch: 118 | Batch_idx: 210 |  Loss: (0.0823) | Acc: (97.18%) (26246/27008)\n",
            "Epoch: 118 | Batch_idx: 220 |  Loss: (0.0828) | Acc: (97.15%) (27482/28288)\n",
            "Epoch: 118 | Batch_idx: 230 |  Loss: (0.0826) | Acc: (97.15%) (28726/29568)\n",
            "Epoch: 118 | Batch_idx: 240 |  Loss: (0.0831) | Acc: (97.15%) (29969/30848)\n",
            "Epoch: 118 | Batch_idx: 250 |  Loss: (0.0830) | Acc: (97.14%) (31209/32128)\n",
            "Epoch: 118 | Batch_idx: 260 |  Loss: (0.0837) | Acc: (97.11%) (32444/33408)\n",
            "Epoch: 118 | Batch_idx: 270 |  Loss: (0.0836) | Acc: (97.12%) (33689/34688)\n",
            "Epoch: 118 | Batch_idx: 280 |  Loss: (0.0839) | Acc: (97.11%) (34927/35968)\n",
            "Epoch: 118 | Batch_idx: 290 |  Loss: (0.0839) | Acc: (97.11%) (36171/37248)\n",
            "Epoch: 118 | Batch_idx: 300 |  Loss: (0.0836) | Acc: (97.12%) (37419/38528)\n",
            "Epoch: 118 | Batch_idx: 310 |  Loss: (0.0837) | Acc: (97.13%) (38666/39808)\n",
            "Epoch: 118 | Batch_idx: 320 |  Loss: (0.0841) | Acc: (97.11%) (39902/41088)\n",
            "Epoch: 118 | Batch_idx: 330 |  Loss: (0.0838) | Acc: (97.12%) (41146/42368)\n",
            "Epoch: 118 | Batch_idx: 340 |  Loss: (0.0840) | Acc: (97.11%) (42386/43648)\n",
            "Epoch: 118 | Batch_idx: 350 |  Loss: (0.0838) | Acc: (97.11%) (43630/44928)\n",
            "Epoch: 118 | Batch_idx: 360 |  Loss: (0.0836) | Acc: (97.10%) (44869/46208)\n",
            "Epoch: 118 | Batch_idx: 370 |  Loss: (0.0836) | Acc: (97.10%) (46113/47488)\n",
            "Epoch: 118 | Batch_idx: 380 |  Loss: (0.0837) | Acc: (97.11%) (47357/48768)\n",
            "Epoch: 118 | Batch_idx: 390 |  Loss: (0.0838) | Acc: (97.09%) (48547/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3335) | Acc: (91.08%) (9108/10000)\n",
            "Epoch: 119 | Batch_idx: 0 |  Loss: (0.0800) | Acc: (97.66%) (125/128)\n",
            "Epoch: 119 | Batch_idx: 10 |  Loss: (0.0720) | Acc: (97.59%) (1374/1408)\n",
            "Epoch: 119 | Batch_idx: 20 |  Loss: (0.0788) | Acc: (97.43%) (2619/2688)\n",
            "Epoch: 119 | Batch_idx: 30 |  Loss: (0.0767) | Acc: (97.45%) (3867/3968)\n",
            "Epoch: 119 | Batch_idx: 40 |  Loss: (0.0798) | Acc: (97.26%) (5104/5248)\n",
            "Epoch: 119 | Batch_idx: 50 |  Loss: (0.0826) | Acc: (97.07%) (6337/6528)\n",
            "Epoch: 119 | Batch_idx: 60 |  Loss: (0.0827) | Acc: (97.11%) (7582/7808)\n",
            "Epoch: 119 | Batch_idx: 70 |  Loss: (0.0831) | Acc: (97.16%) (8830/9088)\n",
            "Epoch: 119 | Batch_idx: 80 |  Loss: (0.0831) | Acc: (97.18%) (10076/10368)\n",
            "Epoch: 119 | Batch_idx: 90 |  Loss: (0.0841) | Acc: (97.18%) (11320/11648)\n",
            "Epoch: 119 | Batch_idx: 100 |  Loss: (0.0831) | Acc: (97.22%) (12568/12928)\n",
            "Epoch: 119 | Batch_idx: 110 |  Loss: (0.0851) | Acc: (97.19%) (13809/14208)\n",
            "Epoch: 119 | Batch_idx: 120 |  Loss: (0.0844) | Acc: (97.22%) (15058/15488)\n",
            "Epoch: 119 | Batch_idx: 130 |  Loss: (0.0858) | Acc: (97.16%) (16292/16768)\n",
            "Epoch: 119 | Batch_idx: 140 |  Loss: (0.0851) | Acc: (97.20%) (17542/18048)\n",
            "Epoch: 119 | Batch_idx: 150 |  Loss: (0.0842) | Acc: (97.20%) (18787/19328)\n",
            "Epoch: 119 | Batch_idx: 160 |  Loss: (0.0845) | Acc: (97.21%) (20034/20608)\n",
            "Epoch: 119 | Batch_idx: 170 |  Loss: (0.0838) | Acc: (97.23%) (21282/21888)\n",
            "Epoch: 119 | Batch_idx: 180 |  Loss: (0.0830) | Acc: (97.25%) (22532/23168)\n",
            "Epoch: 119 | Batch_idx: 190 |  Loss: (0.0827) | Acc: (97.28%) (23782/24448)\n",
            "Epoch: 119 | Batch_idx: 200 |  Loss: (0.0823) | Acc: (97.29%) (25032/25728)\n",
            "Epoch: 119 | Batch_idx: 210 |  Loss: (0.0818) | Acc: (97.30%) (26280/27008)\n",
            "Epoch: 119 | Batch_idx: 220 |  Loss: (0.0819) | Acc: (97.32%) (27529/28288)\n",
            "Epoch: 119 | Batch_idx: 230 |  Loss: (0.0826) | Acc: (97.30%) (28769/29568)\n",
            "Epoch: 119 | Batch_idx: 240 |  Loss: (0.0826) | Acc: (97.30%) (30016/30848)\n",
            "Epoch: 119 | Batch_idx: 250 |  Loss: (0.0820) | Acc: (97.30%) (31262/32128)\n",
            "Epoch: 119 | Batch_idx: 260 |  Loss: (0.0825) | Acc: (97.29%) (32501/33408)\n",
            "Epoch: 119 | Batch_idx: 270 |  Loss: (0.0830) | Acc: (97.26%) (33737/34688)\n",
            "Epoch: 119 | Batch_idx: 280 |  Loss: (0.0827) | Acc: (97.26%) (34982/35968)\n",
            "Epoch: 119 | Batch_idx: 290 |  Loss: (0.0823) | Acc: (97.26%) (36227/37248)\n",
            "Epoch: 119 | Batch_idx: 300 |  Loss: (0.0821) | Acc: (97.26%) (37474/38528)\n",
            "Epoch: 119 | Batch_idx: 310 |  Loss: (0.0817) | Acc: (97.29%) (38728/39808)\n",
            "Epoch: 119 | Batch_idx: 320 |  Loss: (0.0819) | Acc: (97.28%) (39972/41088)\n",
            "Epoch: 119 | Batch_idx: 330 |  Loss: (0.0819) | Acc: (97.29%) (41219/42368)\n",
            "Epoch: 119 | Batch_idx: 340 |  Loss: (0.0822) | Acc: (97.29%) (42465/43648)\n",
            "Epoch: 119 | Batch_idx: 350 |  Loss: (0.0815) | Acc: (97.32%) (43722/44928)\n",
            "Epoch: 119 | Batch_idx: 360 |  Loss: (0.0812) | Acc: (97.33%) (44975/46208)\n",
            "Epoch: 119 | Batch_idx: 370 |  Loss: (0.0815) | Acc: (97.33%) (46221/47488)\n",
            "Epoch: 119 | Batch_idx: 380 |  Loss: (0.0816) | Acc: (97.34%) (47469/48768)\n",
            "Epoch: 119 | Batch_idx: 390 |  Loss: (0.0816) | Acc: (97.32%) (48661/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3304) | Acc: (91.30%) (9130/10000)\n",
            "Epoch: 120 | Batch_idx: 0 |  Loss: (0.1320) | Acc: (97.66%) (125/128)\n",
            "Epoch: 120 | Batch_idx: 10 |  Loss: (0.0977) | Acc: (96.95%) (1365/1408)\n",
            "Epoch: 120 | Batch_idx: 20 |  Loss: (0.0863) | Acc: (96.99%) (2607/2688)\n",
            "Epoch: 120 | Batch_idx: 30 |  Loss: (0.0779) | Acc: (97.28%) (3860/3968)\n",
            "Epoch: 120 | Batch_idx: 40 |  Loss: (0.0787) | Acc: (97.33%) (5108/5248)\n",
            "Epoch: 120 | Batch_idx: 50 |  Loss: (0.0779) | Acc: (97.37%) (6356/6528)\n",
            "Epoch: 120 | Batch_idx: 60 |  Loss: (0.0756) | Acc: (97.43%) (7607/7808)\n",
            "Epoch: 120 | Batch_idx: 70 |  Loss: (0.0762) | Acc: (97.34%) (8846/9088)\n",
            "Epoch: 120 | Batch_idx: 80 |  Loss: (0.0752) | Acc: (97.35%) (10093/10368)\n",
            "Epoch: 120 | Batch_idx: 90 |  Loss: (0.0752) | Acc: (97.42%) (11347/11648)\n",
            "Epoch: 120 | Batch_idx: 100 |  Loss: (0.0746) | Acc: (97.41%) (12593/12928)\n",
            "Epoch: 120 | Batch_idx: 110 |  Loss: (0.0780) | Acc: (97.28%) (13822/14208)\n",
            "Epoch: 120 | Batch_idx: 120 |  Loss: (0.0775) | Acc: (97.33%) (15075/15488)\n",
            "Epoch: 120 | Batch_idx: 130 |  Loss: (0.0781) | Acc: (97.29%) (16313/16768)\n",
            "Epoch: 120 | Batch_idx: 140 |  Loss: (0.0768) | Acc: (97.36%) (17571/18048)\n",
            "Epoch: 120 | Batch_idx: 150 |  Loss: (0.0771) | Acc: (97.36%) (18818/19328)\n",
            "Epoch: 120 | Batch_idx: 160 |  Loss: (0.0763) | Acc: (97.41%) (20074/20608)\n",
            "Epoch: 120 | Batch_idx: 170 |  Loss: (0.0754) | Acc: (97.44%) (21328/21888)\n",
            "Epoch: 120 | Batch_idx: 180 |  Loss: (0.0753) | Acc: (97.47%) (22582/23168)\n",
            "Epoch: 120 | Batch_idx: 190 |  Loss: (0.0740) | Acc: (97.50%) (23838/24448)\n",
            "Epoch: 120 | Batch_idx: 200 |  Loss: (0.0734) | Acc: (97.52%) (25089/25728)\n",
            "Epoch: 120 | Batch_idx: 210 |  Loss: (0.0722) | Acc: (97.56%) (26350/27008)\n",
            "Epoch: 120 | Batch_idx: 220 |  Loss: (0.0723) | Acc: (97.57%) (27600/28288)\n",
            "Epoch: 120 | Batch_idx: 230 |  Loss: (0.0723) | Acc: (97.56%) (28847/29568)\n",
            "Epoch: 120 | Batch_idx: 240 |  Loss: (0.0726) | Acc: (97.55%) (30091/30848)\n",
            "Epoch: 120 | Batch_idx: 250 |  Loss: (0.0727) | Acc: (97.54%) (31337/32128)\n",
            "Epoch: 120 | Batch_idx: 260 |  Loss: (0.0718) | Acc: (97.57%) (32595/33408)\n",
            "Epoch: 120 | Batch_idx: 270 |  Loss: (0.0709) | Acc: (97.60%) (33854/34688)\n",
            "Epoch: 120 | Batch_idx: 280 |  Loss: (0.0706) | Acc: (97.60%) (35103/35968)\n",
            "Epoch: 120 | Batch_idx: 290 |  Loss: (0.0707) | Acc: (97.59%) (36352/37248)\n",
            "Epoch: 120 | Batch_idx: 300 |  Loss: (0.0710) | Acc: (97.58%) (37596/38528)\n",
            "Epoch: 120 | Batch_idx: 310 |  Loss: (0.0707) | Acc: (97.60%) (38851/39808)\n",
            "Epoch: 120 | Batch_idx: 320 |  Loss: (0.0705) | Acc: (97.60%) (40102/41088)\n",
            "Epoch: 120 | Batch_idx: 330 |  Loss: (0.0708) | Acc: (97.59%) (41347/42368)\n",
            "Epoch: 120 | Batch_idx: 340 |  Loss: (0.0708) | Acc: (97.60%) (42601/43648)\n",
            "Epoch: 120 | Batch_idx: 350 |  Loss: (0.0710) | Acc: (97.60%) (43850/44928)\n",
            "Epoch: 120 | Batch_idx: 360 |  Loss: (0.0710) | Acc: (97.60%) (45099/46208)\n",
            "Epoch: 120 | Batch_idx: 370 |  Loss: (0.0713) | Acc: (97.58%) (46337/47488)\n",
            "Epoch: 120 | Batch_idx: 380 |  Loss: (0.0711) | Acc: (97.58%) (47587/48768)\n",
            "Epoch: 120 | Batch_idx: 390 |  Loss: (0.0712) | Acc: (97.58%) (48792/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3245) | Acc: (91.51%) (9151/10000)\n",
            "Epoch: 121 | Batch_idx: 0 |  Loss: (0.1061) | Acc: (95.31%) (122/128)\n",
            "Epoch: 121 | Batch_idx: 10 |  Loss: (0.0683) | Acc: (97.66%) (1375/1408)\n",
            "Epoch: 121 | Batch_idx: 20 |  Loss: (0.0643) | Acc: (97.88%) (2631/2688)\n",
            "Epoch: 121 | Batch_idx: 30 |  Loss: (0.0668) | Acc: (97.76%) (3879/3968)\n",
            "Epoch: 121 | Batch_idx: 40 |  Loss: (0.0662) | Acc: (97.79%) (5132/5248)\n",
            "Epoch: 121 | Batch_idx: 50 |  Loss: (0.0716) | Acc: (97.75%) (6381/6528)\n",
            "Epoch: 121 | Batch_idx: 60 |  Loss: (0.0698) | Acc: (97.78%) (7635/7808)\n",
            "Epoch: 121 | Batch_idx: 70 |  Loss: (0.0707) | Acc: (97.71%) (8880/9088)\n",
            "Epoch: 121 | Batch_idx: 80 |  Loss: (0.0712) | Acc: (97.70%) (10130/10368)\n",
            "Epoch: 121 | Batch_idx: 90 |  Loss: (0.0695) | Acc: (97.73%) (11384/11648)\n",
            "Epoch: 121 | Batch_idx: 100 |  Loss: (0.0697) | Acc: (97.69%) (12629/12928)\n",
            "Epoch: 121 | Batch_idx: 110 |  Loss: (0.0692) | Acc: (97.66%) (13875/14208)\n",
            "Epoch: 121 | Batch_idx: 120 |  Loss: (0.0706) | Acc: (97.62%) (15120/15488)\n",
            "Epoch: 121 | Batch_idx: 130 |  Loss: (0.0692) | Acc: (97.69%) (16380/16768)\n",
            "Epoch: 121 | Batch_idx: 140 |  Loss: (0.0684) | Acc: (97.68%) (17630/18048)\n",
            "Epoch: 121 | Batch_idx: 150 |  Loss: (0.0687) | Acc: (97.65%) (18873/19328)\n",
            "Epoch: 121 | Batch_idx: 160 |  Loss: (0.0687) | Acc: (97.67%) (20127/20608)\n",
            "Epoch: 121 | Batch_idx: 170 |  Loss: (0.0689) | Acc: (97.64%) (21371/21888)\n",
            "Epoch: 121 | Batch_idx: 180 |  Loss: (0.0695) | Acc: (97.60%) (22613/23168)\n",
            "Epoch: 121 | Batch_idx: 190 |  Loss: (0.0692) | Acc: (97.61%) (23863/24448)\n",
            "Epoch: 121 | Batch_idx: 200 |  Loss: (0.0683) | Acc: (97.63%) (25117/25728)\n",
            "Epoch: 121 | Batch_idx: 210 |  Loss: (0.0680) | Acc: (97.63%) (26367/27008)\n",
            "Epoch: 121 | Batch_idx: 220 |  Loss: (0.0677) | Acc: (97.64%) (27621/28288)\n",
            "Epoch: 121 | Batch_idx: 230 |  Loss: (0.0678) | Acc: (97.65%) (28873/29568)\n",
            "Epoch: 121 | Batch_idx: 240 |  Loss: (0.0672) | Acc: (97.68%) (30132/30848)\n",
            "Epoch: 121 | Batch_idx: 250 |  Loss: (0.0672) | Acc: (97.69%) (31385/32128)\n",
            "Epoch: 121 | Batch_idx: 260 |  Loss: (0.0676) | Acc: (97.68%) (32632/33408)\n",
            "Epoch: 121 | Batch_idx: 270 |  Loss: (0.0670) | Acc: (97.71%) (33893/34688)\n",
            "Epoch: 121 | Batch_idx: 280 |  Loss: (0.0676) | Acc: (97.71%) (35143/35968)\n",
            "Epoch: 121 | Batch_idx: 290 |  Loss: (0.0678) | Acc: (97.70%) (36392/37248)\n",
            "Epoch: 121 | Batch_idx: 300 |  Loss: (0.0678) | Acc: (97.70%) (37641/38528)\n",
            "Epoch: 121 | Batch_idx: 310 |  Loss: (0.0680) | Acc: (97.68%) (38884/39808)\n",
            "Epoch: 121 | Batch_idx: 320 |  Loss: (0.0680) | Acc: (97.67%) (40132/41088)\n",
            "Epoch: 121 | Batch_idx: 330 |  Loss: (0.0677) | Acc: (97.68%) (41385/42368)\n",
            "Epoch: 121 | Batch_idx: 340 |  Loss: (0.0673) | Acc: (97.70%) (42642/43648)\n",
            "Epoch: 121 | Batch_idx: 350 |  Loss: (0.0673) | Acc: (97.69%) (43890/44928)\n",
            "Epoch: 121 | Batch_idx: 360 |  Loss: (0.0672) | Acc: (97.69%) (45139/46208)\n",
            "Epoch: 121 | Batch_idx: 370 |  Loss: (0.0673) | Acc: (97.68%) (46385/47488)\n",
            "Epoch: 121 | Batch_idx: 380 |  Loss: (0.0672) | Acc: (97.69%) (47640/48768)\n",
            "Epoch: 121 | Batch_idx: 390 |  Loss: (0.0675) | Acc: (97.66%) (48830/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3262) | Acc: (91.54%) (9154/10000)\n",
            "Epoch: 122 | Batch_idx: 0 |  Loss: (0.0667) | Acc: (96.88%) (124/128)\n",
            "Epoch: 122 | Batch_idx: 10 |  Loss: (0.0676) | Acc: (97.66%) (1375/1408)\n",
            "Epoch: 122 | Batch_idx: 20 |  Loss: (0.0649) | Acc: (97.84%) (2630/2688)\n",
            "Epoch: 122 | Batch_idx: 30 |  Loss: (0.0653) | Acc: (97.78%) (3880/3968)\n",
            "Epoch: 122 | Batch_idx: 40 |  Loss: (0.0661) | Acc: (97.75%) (5130/5248)\n",
            "Epoch: 122 | Batch_idx: 50 |  Loss: (0.0649) | Acc: (97.90%) (6391/6528)\n",
            "Epoch: 122 | Batch_idx: 60 |  Loss: (0.0669) | Acc: (97.72%) (7630/7808)\n",
            "Epoch: 122 | Batch_idx: 70 |  Loss: (0.0672) | Acc: (97.73%) (8882/9088)\n",
            "Epoch: 122 | Batch_idx: 80 |  Loss: (0.0679) | Acc: (97.70%) (10130/10368)\n",
            "Epoch: 122 | Batch_idx: 90 |  Loss: (0.0688) | Acc: (97.66%) (11375/11648)\n",
            "Epoch: 122 | Batch_idx: 100 |  Loss: (0.0687) | Acc: (97.66%) (12626/12928)\n",
            "Epoch: 122 | Batch_idx: 110 |  Loss: (0.0680) | Acc: (97.71%) (13882/14208)\n",
            "Epoch: 122 | Batch_idx: 120 |  Loss: (0.0664) | Acc: (97.77%) (15143/15488)\n",
            "Epoch: 122 | Batch_idx: 130 |  Loss: (0.0674) | Acc: (97.68%) (16379/16768)\n",
            "Epoch: 122 | Batch_idx: 140 |  Loss: (0.0686) | Acc: (97.66%) (17625/18048)\n",
            "Epoch: 122 | Batch_idx: 150 |  Loss: (0.0687) | Acc: (97.64%) (18872/19328)\n",
            "Epoch: 122 | Batch_idx: 160 |  Loss: (0.0684) | Acc: (97.65%) (20124/20608)\n",
            "Epoch: 122 | Batch_idx: 170 |  Loss: (0.0694) | Acc: (97.64%) (21371/21888)\n",
            "Epoch: 122 | Batch_idx: 180 |  Loss: (0.0695) | Acc: (97.63%) (22619/23168)\n",
            "Epoch: 122 | Batch_idx: 190 |  Loss: (0.0698) | Acc: (97.62%) (23866/24448)\n",
            "Epoch: 122 | Batch_idx: 200 |  Loss: (0.0689) | Acc: (97.64%) (25122/25728)\n",
            "Epoch: 122 | Batch_idx: 210 |  Loss: (0.0692) | Acc: (97.64%) (26370/27008)\n",
            "Epoch: 122 | Batch_idx: 220 |  Loss: (0.0699) | Acc: (97.64%) (27619/28288)\n",
            "Epoch: 122 | Batch_idx: 230 |  Loss: (0.0696) | Acc: (97.65%) (28873/29568)\n",
            "Epoch: 122 | Batch_idx: 240 |  Loss: (0.0706) | Acc: (97.62%) (30113/30848)\n",
            "Epoch: 122 | Batch_idx: 250 |  Loss: (0.0704) | Acc: (97.64%) (31369/32128)\n",
            "Epoch: 122 | Batch_idx: 260 |  Loss: (0.0701) | Acc: (97.65%) (32623/33408)\n",
            "Epoch: 122 | Batch_idx: 270 |  Loss: (0.0702) | Acc: (97.64%) (33870/34688)\n",
            "Epoch: 122 | Batch_idx: 280 |  Loss: (0.0703) | Acc: (97.65%) (35123/35968)\n",
            "Epoch: 122 | Batch_idx: 290 |  Loss: (0.0707) | Acc: (97.63%) (36364/37248)\n",
            "Epoch: 122 | Batch_idx: 300 |  Loss: (0.0705) | Acc: (97.63%) (37614/38528)\n",
            "Epoch: 122 | Batch_idx: 310 |  Loss: (0.0701) | Acc: (97.65%) (38872/39808)\n",
            "Epoch: 122 | Batch_idx: 320 |  Loss: (0.0701) | Acc: (97.63%) (40115/41088)\n",
            "Epoch: 122 | Batch_idx: 330 |  Loss: (0.0699) | Acc: (97.64%) (41370/42368)\n",
            "Epoch: 122 | Batch_idx: 340 |  Loss: (0.0706) | Acc: (97.63%) (42614/43648)\n",
            "Epoch: 122 | Batch_idx: 350 |  Loss: (0.0706) | Acc: (97.62%) (43859/44928)\n",
            "Epoch: 122 | Batch_idx: 360 |  Loss: (0.0702) | Acc: (97.63%) (45111/46208)\n",
            "Epoch: 122 | Batch_idx: 370 |  Loss: (0.0701) | Acc: (97.63%) (46361/47488)\n",
            "Epoch: 122 | Batch_idx: 380 |  Loss: (0.0700) | Acc: (97.64%) (47616/48768)\n",
            "Epoch: 122 | Batch_idx: 390 |  Loss: (0.0703) | Acc: (97.62%) (48809/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3272) | Acc: (91.56%) (9156/10000)\n",
            "Epoch: 123 | Batch_idx: 0 |  Loss: (0.0667) | Acc: (97.66%) (125/128)\n",
            "Epoch: 123 | Batch_idx: 10 |  Loss: (0.0534) | Acc: (98.08%) (1381/1408)\n",
            "Epoch: 123 | Batch_idx: 20 |  Loss: (0.0590) | Acc: (97.99%) (2634/2688)\n",
            "Epoch: 123 | Batch_idx: 30 |  Loss: (0.0589) | Acc: (98.03%) (3890/3968)\n",
            "Epoch: 123 | Batch_idx: 40 |  Loss: (0.0605) | Acc: (98.04%) (5145/5248)\n",
            "Epoch: 123 | Batch_idx: 50 |  Loss: (0.0589) | Acc: (98.05%) (6401/6528)\n",
            "Epoch: 123 | Batch_idx: 60 |  Loss: (0.0603) | Acc: (97.96%) (7649/7808)\n",
            "Epoch: 123 | Batch_idx: 70 |  Loss: (0.0592) | Acc: (97.99%) (8905/9088)\n",
            "Epoch: 123 | Batch_idx: 80 |  Loss: (0.0584) | Acc: (98.04%) (10165/10368)\n",
            "Epoch: 123 | Batch_idx: 90 |  Loss: (0.0598) | Acc: (98.03%) (11419/11648)\n",
            "Epoch: 123 | Batch_idx: 100 |  Loss: (0.0599) | Acc: (98.03%) (12673/12928)\n",
            "Epoch: 123 | Batch_idx: 110 |  Loss: (0.0604) | Acc: (98.00%) (13924/14208)\n",
            "Epoch: 123 | Batch_idx: 120 |  Loss: (0.0618) | Acc: (97.95%) (15170/15488)\n",
            "Epoch: 123 | Batch_idx: 130 |  Loss: (0.0616) | Acc: (97.95%) (16425/16768)\n",
            "Epoch: 123 | Batch_idx: 140 |  Loss: (0.0628) | Acc: (97.93%) (17674/18048)\n",
            "Epoch: 123 | Batch_idx: 150 |  Loss: (0.0624) | Acc: (97.94%) (18929/19328)\n",
            "Epoch: 123 | Batch_idx: 160 |  Loss: (0.0627) | Acc: (97.90%) (20176/20608)\n",
            "Epoch: 123 | Batch_idx: 170 |  Loss: (0.0628) | Acc: (97.91%) (21431/21888)\n",
            "Epoch: 123 | Batch_idx: 180 |  Loss: (0.0629) | Acc: (97.90%) (22682/23168)\n",
            "Epoch: 123 | Batch_idx: 190 |  Loss: (0.0633) | Acc: (97.88%) (23929/24448)\n",
            "Epoch: 123 | Batch_idx: 200 |  Loss: (0.0632) | Acc: (97.88%) (25182/25728)\n",
            "Epoch: 123 | Batch_idx: 210 |  Loss: (0.0636) | Acc: (97.85%) (26428/27008)\n",
            "Epoch: 123 | Batch_idx: 220 |  Loss: (0.0637) | Acc: (97.84%) (27678/28288)\n",
            "Epoch: 123 | Batch_idx: 230 |  Loss: (0.0634) | Acc: (97.86%) (28936/29568)\n",
            "Epoch: 123 | Batch_idx: 240 |  Loss: (0.0632) | Acc: (97.87%) (30191/30848)\n",
            "Epoch: 123 | Batch_idx: 250 |  Loss: (0.0636) | Acc: (97.88%) (31446/32128)\n",
            "Epoch: 123 | Batch_idx: 260 |  Loss: (0.0639) | Acc: (97.86%) (32692/33408)\n",
            "Epoch: 123 | Batch_idx: 270 |  Loss: (0.0638) | Acc: (97.87%) (33949/34688)\n",
            "Epoch: 123 | Batch_idx: 280 |  Loss: (0.0638) | Acc: (97.87%) (35201/35968)\n",
            "Epoch: 123 | Batch_idx: 290 |  Loss: (0.0634) | Acc: (97.88%) (36460/37248)\n",
            "Epoch: 123 | Batch_idx: 300 |  Loss: (0.0633) | Acc: (97.90%) (37719/38528)\n",
            "Epoch: 123 | Batch_idx: 310 |  Loss: (0.0632) | Acc: (97.90%) (38972/39808)\n",
            "Epoch: 123 | Batch_idx: 320 |  Loss: (0.0633) | Acc: (97.89%) (40219/41088)\n",
            "Epoch: 123 | Batch_idx: 330 |  Loss: (0.0633) | Acc: (97.88%) (41469/42368)\n",
            "Epoch: 123 | Batch_idx: 340 |  Loss: (0.0631) | Acc: (97.89%) (42725/43648)\n",
            "Epoch: 123 | Batch_idx: 350 |  Loss: (0.0636) | Acc: (97.86%) (43968/44928)\n",
            "Epoch: 123 | Batch_idx: 360 |  Loss: (0.0638) | Acc: (97.86%) (45220/46208)\n",
            "Epoch: 123 | Batch_idx: 370 |  Loss: (0.0638) | Acc: (97.86%) (46472/47488)\n",
            "Epoch: 123 | Batch_idx: 380 |  Loss: (0.0636) | Acc: (97.86%) (47726/48768)\n",
            "Epoch: 123 | Batch_idx: 390 |  Loss: (0.0633) | Acc: (97.87%) (48936/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3281) | Acc: (91.52%) (9152/10000)\n",
            "Epoch: 124 | Batch_idx: 0 |  Loss: (0.0761) | Acc: (97.66%) (125/128)\n",
            "Epoch: 124 | Batch_idx: 10 |  Loss: (0.0494) | Acc: (98.44%) (1386/1408)\n",
            "Epoch: 124 | Batch_idx: 20 |  Loss: (0.0524) | Acc: (98.18%) (2639/2688)\n",
            "Epoch: 124 | Batch_idx: 30 |  Loss: (0.0570) | Acc: (97.98%) (3888/3968)\n",
            "Epoch: 124 | Batch_idx: 40 |  Loss: (0.0599) | Acc: (97.87%) (5136/5248)\n",
            "Epoch: 124 | Batch_idx: 50 |  Loss: (0.0599) | Acc: (97.86%) (6388/6528)\n",
            "Epoch: 124 | Batch_idx: 60 |  Loss: (0.0617) | Acc: (97.81%) (7637/7808)\n",
            "Epoch: 124 | Batch_idx: 70 |  Loss: (0.0619) | Acc: (97.81%) (8889/9088)\n",
            "Epoch: 124 | Batch_idx: 80 |  Loss: (0.0619) | Acc: (97.77%) (10137/10368)\n",
            "Epoch: 124 | Batch_idx: 90 |  Loss: (0.0644) | Acc: (97.73%) (11384/11648)\n",
            "Epoch: 124 | Batch_idx: 100 |  Loss: (0.0648) | Acc: (97.72%) (12633/12928)\n",
            "Epoch: 124 | Batch_idx: 110 |  Loss: (0.0647) | Acc: (97.75%) (13888/14208)\n",
            "Epoch: 124 | Batch_idx: 120 |  Loss: (0.0642) | Acc: (97.75%) (15140/15488)\n",
            "Epoch: 124 | Batch_idx: 130 |  Loss: (0.0632) | Acc: (97.81%) (16401/16768)\n",
            "Epoch: 124 | Batch_idx: 140 |  Loss: (0.0612) | Acc: (97.91%) (17671/18048)\n",
            "Epoch: 124 | Batch_idx: 150 |  Loss: (0.0614) | Acc: (97.91%) (18925/19328)\n",
            "Epoch: 124 | Batch_idx: 160 |  Loss: (0.0619) | Acc: (97.89%) (20173/20608)\n",
            "Epoch: 124 | Batch_idx: 170 |  Loss: (0.0623) | Acc: (97.85%) (21417/21888)\n",
            "Epoch: 124 | Batch_idx: 180 |  Loss: (0.0621) | Acc: (97.84%) (22668/23168)\n",
            "Epoch: 124 | Batch_idx: 190 |  Loss: (0.0619) | Acc: (97.84%) (23921/24448)\n",
            "Epoch: 124 | Batch_idx: 200 |  Loss: (0.0622) | Acc: (97.85%) (25175/25728)\n",
            "Epoch: 124 | Batch_idx: 210 |  Loss: (0.0633) | Acc: (97.84%) (26424/27008)\n",
            "Epoch: 124 | Batch_idx: 220 |  Loss: (0.0628) | Acc: (97.84%) (27676/28288)\n",
            "Epoch: 124 | Batch_idx: 230 |  Loss: (0.0622) | Acc: (97.85%) (28932/29568)\n",
            "Epoch: 124 | Batch_idx: 240 |  Loss: (0.0624) | Acc: (97.84%) (30183/30848)\n",
            "Epoch: 124 | Batch_idx: 250 |  Loss: (0.0627) | Acc: (97.84%) (31435/32128)\n",
            "Epoch: 124 | Batch_idx: 260 |  Loss: (0.0627) | Acc: (97.83%) (32683/33408)\n",
            "Epoch: 124 | Batch_idx: 270 |  Loss: (0.0624) | Acc: (97.84%) (33938/34688)\n",
            "Epoch: 124 | Batch_idx: 280 |  Loss: (0.0619) | Acc: (97.87%) (35201/35968)\n",
            "Epoch: 124 | Batch_idx: 290 |  Loss: (0.0624) | Acc: (97.86%) (36451/37248)\n",
            "Epoch: 124 | Batch_idx: 300 |  Loss: (0.0624) | Acc: (97.88%) (37710/38528)\n",
            "Epoch: 124 | Batch_idx: 310 |  Loss: (0.0625) | Acc: (97.87%) (38959/39808)\n",
            "Epoch: 124 | Batch_idx: 320 |  Loss: (0.0620) | Acc: (97.88%) (40216/41088)\n",
            "Epoch: 124 | Batch_idx: 330 |  Loss: (0.0628) | Acc: (97.86%) (41461/42368)\n",
            "Epoch: 124 | Batch_idx: 340 |  Loss: (0.0629) | Acc: (97.86%) (42714/43648)\n",
            "Epoch: 124 | Batch_idx: 350 |  Loss: (0.0627) | Acc: (97.86%) (43967/44928)\n",
            "Epoch: 124 | Batch_idx: 360 |  Loss: (0.0629) | Acc: (97.86%) (45217/46208)\n",
            "Epoch: 124 | Batch_idx: 370 |  Loss: (0.0630) | Acc: (97.85%) (46468/47488)\n",
            "Epoch: 124 | Batch_idx: 380 |  Loss: (0.0630) | Acc: (97.85%) (47719/48768)\n",
            "Epoch: 124 | Batch_idx: 390 |  Loss: (0.0627) | Acc: (97.86%) (48931/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3293) | Acc: (91.41%) (9141/10000)\n",
            "Epoch: 125 | Batch_idx: 0 |  Loss: (0.0572) | Acc: (97.66%) (125/128)\n",
            "Epoch: 125 | Batch_idx: 10 |  Loss: (0.0700) | Acc: (97.44%) (1372/1408)\n",
            "Epoch: 125 | Batch_idx: 20 |  Loss: (0.0672) | Acc: (97.54%) (2622/2688)\n",
            "Epoch: 125 | Batch_idx: 30 |  Loss: (0.0695) | Acc: (97.53%) (3870/3968)\n",
            "Epoch: 125 | Batch_idx: 40 |  Loss: (0.0716) | Acc: (97.50%) (5117/5248)\n",
            "Epoch: 125 | Batch_idx: 50 |  Loss: (0.0691) | Acc: (97.55%) (6368/6528)\n",
            "Epoch: 125 | Batch_idx: 60 |  Loss: (0.0706) | Acc: (97.53%) (7615/7808)\n",
            "Epoch: 125 | Batch_idx: 70 |  Loss: (0.0689) | Acc: (97.65%) (8874/9088)\n",
            "Epoch: 125 | Batch_idx: 80 |  Loss: (0.0701) | Acc: (97.58%) (10117/10368)\n",
            "Epoch: 125 | Batch_idx: 90 |  Loss: (0.0693) | Acc: (97.59%) (11367/11648)\n",
            "Epoch: 125 | Batch_idx: 100 |  Loss: (0.0673) | Acc: (97.64%) (12623/12928)\n",
            "Epoch: 125 | Batch_idx: 110 |  Loss: (0.0670) | Acc: (97.66%) (13876/14208)\n",
            "Epoch: 125 | Batch_idx: 120 |  Loss: (0.0675) | Acc: (97.68%) (15129/15488)\n",
            "Epoch: 125 | Batch_idx: 130 |  Loss: (0.0666) | Acc: (97.70%) (16382/16768)\n",
            "Epoch: 125 | Batch_idx: 140 |  Loss: (0.0658) | Acc: (97.72%) (17637/18048)\n",
            "Epoch: 125 | Batch_idx: 150 |  Loss: (0.0653) | Acc: (97.74%) (18891/19328)\n",
            "Epoch: 125 | Batch_idx: 160 |  Loss: (0.0656) | Acc: (97.73%) (20140/20608)\n",
            "Epoch: 125 | Batch_idx: 170 |  Loss: (0.0658) | Acc: (97.75%) (21395/21888)\n",
            "Epoch: 125 | Batch_idx: 180 |  Loss: (0.0658) | Acc: (97.74%) (22644/23168)\n",
            "Epoch: 125 | Batch_idx: 190 |  Loss: (0.0645) | Acc: (97.80%) (23909/24448)\n",
            "Epoch: 125 | Batch_idx: 200 |  Loss: (0.0650) | Acc: (97.78%) (25157/25728)\n",
            "Epoch: 125 | Batch_idx: 210 |  Loss: (0.0657) | Acc: (97.76%) (26404/27008)\n",
            "Epoch: 125 | Batch_idx: 220 |  Loss: (0.0660) | Acc: (97.77%) (27656/28288)\n",
            "Epoch: 125 | Batch_idx: 230 |  Loss: (0.0666) | Acc: (97.75%) (28902/29568)\n",
            "Epoch: 125 | Batch_idx: 240 |  Loss: (0.0670) | Acc: (97.72%) (30146/30848)\n",
            "Epoch: 125 | Batch_idx: 250 |  Loss: (0.0673) | Acc: (97.72%) (31394/32128)\n",
            "Epoch: 125 | Batch_idx: 260 |  Loss: (0.0671) | Acc: (97.70%) (32641/33408)\n",
            "Epoch: 125 | Batch_idx: 270 |  Loss: (0.0669) | Acc: (97.71%) (33895/34688)\n",
            "Epoch: 125 | Batch_idx: 280 |  Loss: (0.0665) | Acc: (97.74%) (35156/35968)\n",
            "Epoch: 125 | Batch_idx: 290 |  Loss: (0.0670) | Acc: (97.73%) (36403/37248)\n",
            "Epoch: 125 | Batch_idx: 300 |  Loss: (0.0671) | Acc: (97.73%) (37654/38528)\n",
            "Epoch: 125 | Batch_idx: 310 |  Loss: (0.0667) | Acc: (97.73%) (38906/39808)\n",
            "Epoch: 125 | Batch_idx: 320 |  Loss: (0.0666) | Acc: (97.73%) (40157/41088)\n",
            "Epoch: 125 | Batch_idx: 330 |  Loss: (0.0665) | Acc: (97.72%) (41404/42368)\n",
            "Epoch: 125 | Batch_idx: 340 |  Loss: (0.0670) | Acc: (97.72%) (42652/43648)\n",
            "Epoch: 125 | Batch_idx: 350 |  Loss: (0.0670) | Acc: (97.71%) (43901/44928)\n",
            "Epoch: 125 | Batch_idx: 360 |  Loss: (0.0671) | Acc: (97.71%) (45150/46208)\n",
            "Epoch: 125 | Batch_idx: 370 |  Loss: (0.0669) | Acc: (97.71%) (46399/47488)\n",
            "Epoch: 125 | Batch_idx: 380 |  Loss: (0.0673) | Acc: (97.69%) (47642/48768)\n",
            "Epoch: 125 | Batch_idx: 390 |  Loss: (0.0672) | Acc: (97.69%) (48846/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3279) | Acc: (91.55%) (9155/10000)\n",
            "Epoch: 126 | Batch_idx: 0 |  Loss: (0.0635) | Acc: (97.66%) (125/128)\n",
            "Epoch: 126 | Batch_idx: 10 |  Loss: (0.0764) | Acc: (97.09%) (1367/1408)\n",
            "Epoch: 126 | Batch_idx: 20 |  Loss: (0.0704) | Acc: (97.40%) (2618/2688)\n",
            "Epoch: 126 | Batch_idx: 30 |  Loss: (0.0673) | Acc: (97.53%) (3870/3968)\n",
            "Epoch: 126 | Batch_idx: 40 |  Loss: (0.0652) | Acc: (97.66%) (5125/5248)\n",
            "Epoch: 126 | Batch_idx: 50 |  Loss: (0.0598) | Acc: (97.84%) (6387/6528)\n",
            "Epoch: 126 | Batch_idx: 60 |  Loss: (0.0602) | Acc: (97.85%) (7640/7808)\n",
            "Epoch: 126 | Batch_idx: 70 |  Loss: (0.0613) | Acc: (97.82%) (8890/9088)\n",
            "Epoch: 126 | Batch_idx: 80 |  Loss: (0.0624) | Acc: (97.75%) (10135/10368)\n",
            "Epoch: 126 | Batch_idx: 90 |  Loss: (0.0628) | Acc: (97.78%) (11389/11648)\n",
            "Epoch: 126 | Batch_idx: 100 |  Loss: (0.0630) | Acc: (97.74%) (12636/12928)\n",
            "Epoch: 126 | Batch_idx: 110 |  Loss: (0.0630) | Acc: (97.76%) (13890/14208)\n",
            "Epoch: 126 | Batch_idx: 120 |  Loss: (0.0624) | Acc: (97.77%) (15143/15488)\n",
            "Epoch: 126 | Batch_idx: 130 |  Loss: (0.0622) | Acc: (97.77%) (16394/16768)\n",
            "Epoch: 126 | Batch_idx: 140 |  Loss: (0.0618) | Acc: (97.79%) (17650/18048)\n",
            "Epoch: 126 | Batch_idx: 150 |  Loss: (0.0619) | Acc: (97.78%) (18898/19328)\n",
            "Epoch: 126 | Batch_idx: 160 |  Loss: (0.0611) | Acc: (97.84%) (20163/20608)\n",
            "Epoch: 126 | Batch_idx: 170 |  Loss: (0.0626) | Acc: (97.80%) (21406/21888)\n",
            "Epoch: 126 | Batch_idx: 180 |  Loss: (0.0627) | Acc: (97.80%) (22658/23168)\n",
            "Epoch: 126 | Batch_idx: 190 |  Loss: (0.0628) | Acc: (97.78%) (23906/24448)\n",
            "Epoch: 126 | Batch_idx: 200 |  Loss: (0.0623) | Acc: (97.81%) (25165/25728)\n",
            "Epoch: 126 | Batch_idx: 210 |  Loss: (0.0626) | Acc: (97.79%) (26410/27008)\n",
            "Epoch: 126 | Batch_idx: 220 |  Loss: (0.0624) | Acc: (97.79%) (27663/28288)\n",
            "Epoch: 126 | Batch_idx: 230 |  Loss: (0.0629) | Acc: (97.75%) (28903/29568)\n",
            "Epoch: 126 | Batch_idx: 240 |  Loss: (0.0635) | Acc: (97.73%) (30147/30848)\n",
            "Epoch: 126 | Batch_idx: 250 |  Loss: (0.0634) | Acc: (97.75%) (31404/32128)\n",
            "Epoch: 126 | Batch_idx: 260 |  Loss: (0.0633) | Acc: (97.78%) (32665/33408)\n",
            "Epoch: 126 | Batch_idx: 270 |  Loss: (0.0634) | Acc: (97.78%) (33918/34688)\n",
            "Epoch: 126 | Batch_idx: 280 |  Loss: (0.0634) | Acc: (97.79%) (35172/35968)\n",
            "Epoch: 126 | Batch_idx: 290 |  Loss: (0.0631) | Acc: (97.81%) (36431/37248)\n",
            "Epoch: 126 | Batch_idx: 300 |  Loss: (0.0628) | Acc: (97.82%) (37689/38528)\n",
            "Epoch: 126 | Batch_idx: 310 |  Loss: (0.0632) | Acc: (97.80%) (38933/39808)\n",
            "Epoch: 126 | Batch_idx: 320 |  Loss: (0.0639) | Acc: (97.78%) (40177/41088)\n",
            "Epoch: 126 | Batch_idx: 330 |  Loss: (0.0640) | Acc: (97.76%) (41420/42368)\n",
            "Epoch: 126 | Batch_idx: 340 |  Loss: (0.0635) | Acc: (97.77%) (42674/43648)\n",
            "Epoch: 126 | Batch_idx: 350 |  Loss: (0.0635) | Acc: (97.77%) (43928/44928)\n",
            "Epoch: 126 | Batch_idx: 360 |  Loss: (0.0633) | Acc: (97.79%) (45186/46208)\n",
            "Epoch: 126 | Batch_idx: 370 |  Loss: (0.0632) | Acc: (97.79%) (46440/47488)\n",
            "Epoch: 126 | Batch_idx: 380 |  Loss: (0.0638) | Acc: (97.77%) (47682/48768)\n",
            "Epoch: 126 | Batch_idx: 390 |  Loss: (0.0637) | Acc: (97.78%) (48890/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3343) | Acc: (91.52%) (9152/10000)\n",
            "Epoch: 127 | Batch_idx: 0 |  Loss: (0.1350) | Acc: (93.75%) (120/128)\n",
            "Epoch: 127 | Batch_idx: 10 |  Loss: (0.0687) | Acc: (97.73%) (1376/1408)\n",
            "Epoch: 127 | Batch_idx: 20 |  Loss: (0.0666) | Acc: (97.69%) (2626/2688)\n",
            "Epoch: 127 | Batch_idx: 30 |  Loss: (0.0643) | Acc: (97.76%) (3879/3968)\n",
            "Epoch: 127 | Batch_idx: 40 |  Loss: (0.0646) | Acc: (97.71%) (5128/5248)\n",
            "Epoch: 127 | Batch_idx: 50 |  Loss: (0.0634) | Acc: (97.82%) (6386/6528)\n",
            "Epoch: 127 | Batch_idx: 60 |  Loss: (0.0635) | Acc: (97.85%) (7640/7808)\n",
            "Epoch: 127 | Batch_idx: 70 |  Loss: (0.0610) | Acc: (97.94%) (8901/9088)\n",
            "Epoch: 127 | Batch_idx: 80 |  Loss: (0.0598) | Acc: (97.99%) (10160/10368)\n",
            "Epoch: 127 | Batch_idx: 90 |  Loss: (0.0586) | Acc: (98.03%) (11419/11648)\n",
            "Epoch: 127 | Batch_idx: 100 |  Loss: (0.0594) | Acc: (98.00%) (12670/12928)\n",
            "Epoch: 127 | Batch_idx: 110 |  Loss: (0.0592) | Acc: (97.98%) (13921/14208)\n",
            "Epoch: 127 | Batch_idx: 120 |  Loss: (0.0592) | Acc: (97.93%) (15167/15488)\n",
            "Epoch: 127 | Batch_idx: 130 |  Loss: (0.0615) | Acc: (97.86%) (16410/16768)\n",
            "Epoch: 127 | Batch_idx: 140 |  Loss: (0.0627) | Acc: (97.84%) (17658/18048)\n",
            "Epoch: 127 | Batch_idx: 150 |  Loss: (0.0631) | Acc: (97.82%) (18907/19328)\n",
            "Epoch: 127 | Batch_idx: 160 |  Loss: (0.0628) | Acc: (97.81%) (20156/20608)\n",
            "Epoch: 127 | Batch_idx: 170 |  Loss: (0.0622) | Acc: (97.85%) (21418/21888)\n",
            "Epoch: 127 | Batch_idx: 180 |  Loss: (0.0612) | Acc: (97.89%) (22678/23168)\n",
            "Epoch: 127 | Batch_idx: 190 |  Loss: (0.0607) | Acc: (97.89%) (23931/24448)\n",
            "Epoch: 127 | Batch_idx: 200 |  Loss: (0.0601) | Acc: (97.91%) (25190/25728)\n",
            "Epoch: 127 | Batch_idx: 210 |  Loss: (0.0593) | Acc: (97.94%) (26452/27008)\n",
            "Epoch: 127 | Batch_idx: 220 |  Loss: (0.0596) | Acc: (97.92%) (27701/28288)\n",
            "Epoch: 127 | Batch_idx: 230 |  Loss: (0.0598) | Acc: (97.93%) (28957/29568)\n",
            "Epoch: 127 | Batch_idx: 240 |  Loss: (0.0604) | Acc: (97.91%) (30203/30848)\n",
            "Epoch: 127 | Batch_idx: 250 |  Loss: (0.0604) | Acc: (97.92%) (31459/32128)\n",
            "Epoch: 127 | Batch_idx: 260 |  Loss: (0.0601) | Acc: (97.93%) (32715/33408)\n",
            "Epoch: 127 | Batch_idx: 270 |  Loss: (0.0606) | Acc: (97.91%) (33963/34688)\n",
            "Epoch: 127 | Batch_idx: 280 |  Loss: (0.0601) | Acc: (97.92%) (35220/35968)\n",
            "Epoch: 127 | Batch_idx: 290 |  Loss: (0.0604) | Acc: (97.91%) (36470/37248)\n",
            "Epoch: 127 | Batch_idx: 300 |  Loss: (0.0602) | Acc: (97.91%) (37722/38528)\n",
            "Epoch: 127 | Batch_idx: 310 |  Loss: (0.0601) | Acc: (97.92%) (38980/39808)\n",
            "Epoch: 127 | Batch_idx: 320 |  Loss: (0.0600) | Acc: (97.91%) (40231/41088)\n",
            "Epoch: 127 | Batch_idx: 330 |  Loss: (0.0598) | Acc: (97.92%) (41486/42368)\n",
            "Epoch: 127 | Batch_idx: 340 |  Loss: (0.0600) | Acc: (97.91%) (42736/43648)\n",
            "Epoch: 127 | Batch_idx: 350 |  Loss: (0.0596) | Acc: (97.92%) (43995/44928)\n",
            "Epoch: 127 | Batch_idx: 360 |  Loss: (0.0593) | Acc: (97.94%) (45254/46208)\n",
            "Epoch: 127 | Batch_idx: 370 |  Loss: (0.0598) | Acc: (97.92%) (46498/47488)\n",
            "Epoch: 127 | Batch_idx: 380 |  Loss: (0.0598) | Acc: (97.92%) (47753/48768)\n",
            "Epoch: 127 | Batch_idx: 390 |  Loss: (0.0601) | Acc: (97.92%) (48959/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3320) | Acc: (91.60%) (9160/10000)\n",
            "Epoch: 128 | Batch_idx: 0 |  Loss: (0.0427) | Acc: (98.44%) (126/128)\n",
            "Epoch: 128 | Batch_idx: 10 |  Loss: (0.0616) | Acc: (97.80%) (1377/1408)\n",
            "Epoch: 128 | Batch_idx: 20 |  Loss: (0.0657) | Acc: (97.81%) (2629/2688)\n",
            "Epoch: 128 | Batch_idx: 30 |  Loss: (0.0651) | Acc: (97.83%) (3882/3968)\n",
            "Epoch: 128 | Batch_idx: 40 |  Loss: (0.0646) | Acc: (97.96%) (5141/5248)\n",
            "Epoch: 128 | Batch_idx: 50 |  Loss: (0.0668) | Acc: (97.81%) (6385/6528)\n",
            "Epoch: 128 | Batch_idx: 60 |  Loss: (0.0657) | Acc: (97.89%) (7643/7808)\n",
            "Epoch: 128 | Batch_idx: 70 |  Loss: (0.0663) | Acc: (97.82%) (8890/9088)\n",
            "Epoch: 128 | Batch_idx: 80 |  Loss: (0.0646) | Acc: (97.87%) (10147/10368)\n",
            "Epoch: 128 | Batch_idx: 90 |  Loss: (0.0646) | Acc: (97.90%) (11403/11648)\n",
            "Epoch: 128 | Batch_idx: 100 |  Loss: (0.0630) | Acc: (97.95%) (12663/12928)\n",
            "Epoch: 128 | Batch_idx: 110 |  Loss: (0.0632) | Acc: (97.92%) (13912/14208)\n",
            "Epoch: 128 | Batch_idx: 120 |  Loss: (0.0630) | Acc: (97.90%) (15162/15488)\n",
            "Epoch: 128 | Batch_idx: 130 |  Loss: (0.0635) | Acc: (97.82%) (16403/16768)\n",
            "Epoch: 128 | Batch_idx: 140 |  Loss: (0.0623) | Acc: (97.88%) (17666/18048)\n",
            "Epoch: 128 | Batch_idx: 150 |  Loss: (0.0622) | Acc: (97.87%) (18916/19328)\n",
            "Epoch: 128 | Batch_idx: 160 |  Loss: (0.0618) | Acc: (97.88%) (20171/20608)\n",
            "Epoch: 128 | Batch_idx: 170 |  Loss: (0.0625) | Acc: (97.87%) (21422/21888)\n",
            "Epoch: 128 | Batch_idx: 180 |  Loss: (0.0622) | Acc: (97.89%) (22678/23168)\n",
            "Epoch: 128 | Batch_idx: 190 |  Loss: (0.0615) | Acc: (97.89%) (23932/24448)\n",
            "Epoch: 128 | Batch_idx: 200 |  Loss: (0.0624) | Acc: (97.85%) (25176/25728)\n",
            "Epoch: 128 | Batch_idx: 210 |  Loss: (0.0618) | Acc: (97.89%) (26437/27008)\n",
            "Epoch: 128 | Batch_idx: 220 |  Loss: (0.0619) | Acc: (97.88%) (27689/28288)\n",
            "Epoch: 128 | Batch_idx: 230 |  Loss: (0.0614) | Acc: (97.89%) (28944/29568)\n",
            "Epoch: 128 | Batch_idx: 240 |  Loss: (0.0611) | Acc: (97.89%) (30198/30848)\n",
            "Epoch: 128 | Batch_idx: 250 |  Loss: (0.0611) | Acc: (97.90%) (31453/32128)\n",
            "Epoch: 128 | Batch_idx: 260 |  Loss: (0.0607) | Acc: (97.93%) (32716/33408)\n",
            "Epoch: 128 | Batch_idx: 270 |  Loss: (0.0608) | Acc: (97.93%) (33971/34688)\n",
            "Epoch: 128 | Batch_idx: 280 |  Loss: (0.0610) | Acc: (97.93%) (35225/35968)\n",
            "Epoch: 128 | Batch_idx: 290 |  Loss: (0.0610) | Acc: (97.93%) (36476/37248)\n",
            "Epoch: 128 | Batch_idx: 300 |  Loss: (0.0611) | Acc: (97.93%) (37730/38528)\n",
            "Epoch: 128 | Batch_idx: 310 |  Loss: (0.0611) | Acc: (97.93%) (38983/39808)\n",
            "Epoch: 128 | Batch_idx: 320 |  Loss: (0.0609) | Acc: (97.94%) (40240/41088)\n",
            "Epoch: 128 | Batch_idx: 330 |  Loss: (0.0607) | Acc: (97.93%) (41490/42368)\n",
            "Epoch: 128 | Batch_idx: 340 |  Loss: (0.0603) | Acc: (97.95%) (42753/43648)\n",
            "Epoch: 128 | Batch_idx: 350 |  Loss: (0.0598) | Acc: (97.96%) (44013/44928)\n",
            "Epoch: 128 | Batch_idx: 360 |  Loss: (0.0602) | Acc: (97.94%) (45254/46208)\n",
            "Epoch: 128 | Batch_idx: 370 |  Loss: (0.0602) | Acc: (97.94%) (46508/47488)\n",
            "Epoch: 128 | Batch_idx: 380 |  Loss: (0.0604) | Acc: (97.92%) (47756/48768)\n",
            "Epoch: 128 | Batch_idx: 390 |  Loss: (0.0605) | Acc: (97.92%) (48960/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3310) | Acc: (91.60%) (9160/10000)\n",
            "Epoch: 129 | Batch_idx: 0 |  Loss: (0.0330) | Acc: (99.22%) (127/128)\n",
            "Epoch: 129 | Batch_idx: 10 |  Loss: (0.0480) | Acc: (98.15%) (1382/1408)\n",
            "Epoch: 129 | Batch_idx: 20 |  Loss: (0.0588) | Acc: (97.88%) (2631/2688)\n",
            "Epoch: 129 | Batch_idx: 30 |  Loss: (0.0641) | Acc: (97.68%) (3876/3968)\n",
            "Epoch: 129 | Batch_idx: 40 |  Loss: (0.0647) | Acc: (97.68%) (5126/5248)\n",
            "Epoch: 129 | Batch_idx: 50 |  Loss: (0.0623) | Acc: (97.75%) (6381/6528)\n",
            "Epoch: 129 | Batch_idx: 60 |  Loss: (0.0611) | Acc: (97.84%) (7639/7808)\n",
            "Epoch: 129 | Batch_idx: 70 |  Loss: (0.0607) | Acc: (97.90%) (8897/9088)\n",
            "Epoch: 129 | Batch_idx: 80 |  Loss: (0.0620) | Acc: (97.86%) (10146/10368)\n",
            "Epoch: 129 | Batch_idx: 90 |  Loss: (0.0627) | Acc: (97.87%) (11400/11648)\n",
            "Epoch: 129 | Batch_idx: 100 |  Loss: (0.0630) | Acc: (97.85%) (12650/12928)\n",
            "Epoch: 129 | Batch_idx: 110 |  Loss: (0.0620) | Acc: (97.88%) (13907/14208)\n",
            "Epoch: 129 | Batch_idx: 120 |  Loss: (0.0623) | Acc: (97.88%) (15159/15488)\n",
            "Epoch: 129 | Batch_idx: 130 |  Loss: (0.0617) | Acc: (97.88%) (16413/16768)\n",
            "Epoch: 129 | Batch_idx: 140 |  Loss: (0.0615) | Acc: (97.91%) (17670/18048)\n",
            "Epoch: 129 | Batch_idx: 150 |  Loss: (0.0621) | Acc: (97.89%) (18921/19328)\n",
            "Epoch: 129 | Batch_idx: 160 |  Loss: (0.0617) | Acc: (97.88%) (20172/20608)\n",
            "Epoch: 129 | Batch_idx: 170 |  Loss: (0.0631) | Acc: (97.84%) (21415/21888)\n",
            "Epoch: 129 | Batch_idx: 180 |  Loss: (0.0633) | Acc: (97.85%) (22670/23168)\n",
            "Epoch: 129 | Batch_idx: 190 |  Loss: (0.0634) | Acc: (97.86%) (23924/24448)\n",
            "Epoch: 129 | Batch_idx: 200 |  Loss: (0.0631) | Acc: (97.86%) (25178/25728)\n",
            "Epoch: 129 | Batch_idx: 210 |  Loss: (0.0634) | Acc: (97.84%) (26425/27008)\n",
            "Epoch: 129 | Batch_idx: 220 |  Loss: (0.0637) | Acc: (97.84%) (27677/28288)\n",
            "Epoch: 129 | Batch_idx: 230 |  Loss: (0.0638) | Acc: (97.83%) (28926/29568)\n",
            "Epoch: 129 | Batch_idx: 240 |  Loss: (0.0642) | Acc: (97.82%) (30175/30848)\n",
            "Epoch: 129 | Batch_idx: 250 |  Loss: (0.0636) | Acc: (97.84%) (31435/32128)\n",
            "Epoch: 129 | Batch_idx: 260 |  Loss: (0.0634) | Acc: (97.84%) (32688/33408)\n",
            "Epoch: 129 | Batch_idx: 270 |  Loss: (0.0629) | Acc: (97.85%) (33942/34688)\n",
            "Epoch: 129 | Batch_idx: 280 |  Loss: (0.0635) | Acc: (97.83%) (35188/35968)\n",
            "Epoch: 129 | Batch_idx: 290 |  Loss: (0.0633) | Acc: (97.83%) (36439/37248)\n",
            "Epoch: 129 | Batch_idx: 300 |  Loss: (0.0638) | Acc: (97.79%) (37677/38528)\n",
            "Epoch: 129 | Batch_idx: 310 |  Loss: (0.0645) | Acc: (97.78%) (38926/39808)\n",
            "Epoch: 129 | Batch_idx: 320 |  Loss: (0.0641) | Acc: (97.80%) (40183/41088)\n",
            "Epoch: 129 | Batch_idx: 330 |  Loss: (0.0639) | Acc: (97.80%) (41438/42368)\n",
            "Epoch: 129 | Batch_idx: 340 |  Loss: (0.0636) | Acc: (97.81%) (42693/43648)\n",
            "Epoch: 129 | Batch_idx: 350 |  Loss: (0.0638) | Acc: (97.80%) (43941/44928)\n",
            "Epoch: 129 | Batch_idx: 360 |  Loss: (0.0632) | Acc: (97.82%) (45202/46208)\n",
            "Epoch: 129 | Batch_idx: 370 |  Loss: (0.0630) | Acc: (97.82%) (46454/47488)\n",
            "Epoch: 129 | Batch_idx: 380 |  Loss: (0.0636) | Acc: (97.81%) (47698/48768)\n",
            "Epoch: 129 | Batch_idx: 390 |  Loss: (0.0641) | Acc: (97.81%) (48906/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3353) | Acc: (91.33%) (9133/10000)\n",
            "Epoch: 130 | Batch_idx: 0 |  Loss: (0.0252) | Acc: (99.22%) (127/128)\n",
            "Epoch: 130 | Batch_idx: 10 |  Loss: (0.0545) | Acc: (98.15%) (1382/1408)\n",
            "Epoch: 130 | Batch_idx: 20 |  Loss: (0.0569) | Acc: (97.99%) (2634/2688)\n",
            "Epoch: 130 | Batch_idx: 30 |  Loss: (0.0642) | Acc: (97.76%) (3879/3968)\n",
            "Epoch: 130 | Batch_idx: 40 |  Loss: (0.0620) | Acc: (97.83%) (5134/5248)\n",
            "Epoch: 130 | Batch_idx: 50 |  Loss: (0.0614) | Acc: (97.81%) (6385/6528)\n",
            "Epoch: 130 | Batch_idx: 60 |  Loss: (0.0628) | Acc: (97.78%) (7635/7808)\n",
            "Epoch: 130 | Batch_idx: 70 |  Loss: (0.0613) | Acc: (97.81%) (8889/9088)\n",
            "Epoch: 130 | Batch_idx: 80 |  Loss: (0.0619) | Acc: (97.79%) (10139/10368)\n",
            "Epoch: 130 | Batch_idx: 90 |  Loss: (0.0635) | Acc: (97.81%) (11393/11648)\n",
            "Epoch: 130 | Batch_idx: 100 |  Loss: (0.0633) | Acc: (97.84%) (12649/12928)\n",
            "Epoch: 130 | Batch_idx: 110 |  Loss: (0.0627) | Acc: (97.86%) (13904/14208)\n",
            "Epoch: 130 | Batch_idx: 120 |  Loss: (0.0635) | Acc: (97.84%) (15154/15488)\n",
            "Epoch: 130 | Batch_idx: 130 |  Loss: (0.0631) | Acc: (97.88%) (16412/16768)\n",
            "Epoch: 130 | Batch_idx: 140 |  Loss: (0.0633) | Acc: (97.87%) (17664/18048)\n",
            "Epoch: 130 | Batch_idx: 150 |  Loss: (0.0629) | Acc: (97.89%) (18920/19328)\n",
            "Epoch: 130 | Batch_idx: 160 |  Loss: (0.0636) | Acc: (97.86%) (20166/20608)\n",
            "Epoch: 130 | Batch_idx: 170 |  Loss: (0.0633) | Acc: (97.86%) (21419/21888)\n",
            "Epoch: 130 | Batch_idx: 180 |  Loss: (0.0636) | Acc: (97.84%) (22668/23168)\n",
            "Epoch: 130 | Batch_idx: 190 |  Loss: (0.0631) | Acc: (97.85%) (23922/24448)\n",
            "Epoch: 130 | Batch_idx: 200 |  Loss: (0.0638) | Acc: (97.83%) (25170/25728)\n",
            "Epoch: 130 | Batch_idx: 210 |  Loss: (0.0634) | Acc: (97.87%) (26433/27008)\n",
            "Epoch: 130 | Batch_idx: 220 |  Loss: (0.0633) | Acc: (97.87%) (27685/28288)\n",
            "Epoch: 130 | Batch_idx: 230 |  Loss: (0.0633) | Acc: (97.87%) (28937/29568)\n",
            "Epoch: 130 | Batch_idx: 240 |  Loss: (0.0623) | Acc: (97.90%) (30200/30848)\n",
            "Epoch: 130 | Batch_idx: 250 |  Loss: (0.0623) | Acc: (97.91%) (31455/32128)\n",
            "Epoch: 130 | Batch_idx: 260 |  Loss: (0.0624) | Acc: (97.89%) (32704/33408)\n",
            "Epoch: 130 | Batch_idx: 270 |  Loss: (0.0625) | Acc: (97.89%) (33955/34688)\n",
            "Epoch: 130 | Batch_idx: 280 |  Loss: (0.0625) | Acc: (97.88%) (35205/35968)\n",
            "Epoch: 130 | Batch_idx: 290 |  Loss: (0.0623) | Acc: (97.88%) (36457/37248)\n",
            "Epoch: 130 | Batch_idx: 300 |  Loss: (0.0626) | Acc: (97.86%) (37705/38528)\n",
            "Epoch: 130 | Batch_idx: 310 |  Loss: (0.0621) | Acc: (97.88%) (38964/39808)\n",
            "Epoch: 130 | Batch_idx: 320 |  Loss: (0.0620) | Acc: (97.89%) (40222/41088)\n",
            "Epoch: 130 | Batch_idx: 330 |  Loss: (0.0620) | Acc: (97.89%) (41475/42368)\n",
            "Epoch: 130 | Batch_idx: 340 |  Loss: (0.0628) | Acc: (97.88%) (42724/43648)\n",
            "Epoch: 130 | Batch_idx: 350 |  Loss: (0.0623) | Acc: (97.90%) (43984/44928)\n",
            "Epoch: 130 | Batch_idx: 360 |  Loss: (0.0624) | Acc: (97.91%) (45240/46208)\n",
            "Epoch: 130 | Batch_idx: 370 |  Loss: (0.0623) | Acc: (97.90%) (46489/47488)\n",
            "Epoch: 130 | Batch_idx: 380 |  Loss: (0.0623) | Acc: (97.90%) (47742/48768)\n",
            "Epoch: 130 | Batch_idx: 390 |  Loss: (0.0624) | Acc: (97.90%) (48950/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3365) | Acc: (91.48%) (9148/10000)\n",
            "Epoch: 131 | Batch_idx: 0 |  Loss: (0.0168) | Acc: (100.00%) (128/128)\n",
            "Epoch: 131 | Batch_idx: 10 |  Loss: (0.0465) | Acc: (98.51%) (1387/1408)\n",
            "Epoch: 131 | Batch_idx: 20 |  Loss: (0.0607) | Acc: (98.07%) (2636/2688)\n",
            "Epoch: 131 | Batch_idx: 30 |  Loss: (0.0677) | Acc: (97.78%) (3880/3968)\n",
            "Epoch: 131 | Batch_idx: 40 |  Loss: (0.0692) | Acc: (97.68%) (5126/5248)\n",
            "Epoch: 131 | Batch_idx: 50 |  Loss: (0.0638) | Acc: (97.82%) (6386/6528)\n",
            "Epoch: 131 | Batch_idx: 60 |  Loss: (0.0619) | Acc: (97.94%) (7647/7808)\n",
            "Epoch: 131 | Batch_idx: 70 |  Loss: (0.0606) | Acc: (98.00%) (8906/9088)\n",
            "Epoch: 131 | Batch_idx: 80 |  Loss: (0.0627) | Acc: (97.94%) (10154/10368)\n",
            "Epoch: 131 | Batch_idx: 90 |  Loss: (0.0649) | Acc: (97.84%) (11396/11648)\n",
            "Epoch: 131 | Batch_idx: 100 |  Loss: (0.0631) | Acc: (97.92%) (12659/12928)\n",
            "Epoch: 131 | Batch_idx: 110 |  Loss: (0.0616) | Acc: (97.95%) (13917/14208)\n",
            "Epoch: 131 | Batch_idx: 120 |  Loss: (0.0620) | Acc: (97.92%) (15166/15488)\n",
            "Epoch: 131 | Batch_idx: 130 |  Loss: (0.0625) | Acc: (97.88%) (16413/16768)\n",
            "Epoch: 131 | Batch_idx: 140 |  Loss: (0.0614) | Acc: (97.91%) (17670/18048)\n",
            "Epoch: 131 | Batch_idx: 150 |  Loss: (0.0604) | Acc: (97.94%) (18930/19328)\n",
            "Epoch: 131 | Batch_idx: 160 |  Loss: (0.0605) | Acc: (97.95%) (20186/20608)\n",
            "Epoch: 131 | Batch_idx: 170 |  Loss: (0.0611) | Acc: (97.92%) (21433/21888)\n",
            "Epoch: 131 | Batch_idx: 180 |  Loss: (0.0608) | Acc: (97.93%) (22689/23168)\n",
            "Epoch: 131 | Batch_idx: 190 |  Loss: (0.0601) | Acc: (97.96%) (23949/24448)\n",
            "Epoch: 131 | Batch_idx: 200 |  Loss: (0.0603) | Acc: (97.94%) (25199/25728)\n",
            "Epoch: 131 | Batch_idx: 210 |  Loss: (0.0601) | Acc: (97.94%) (26452/27008)\n",
            "Epoch: 131 | Batch_idx: 220 |  Loss: (0.0607) | Acc: (97.94%) (27706/28288)\n",
            "Epoch: 131 | Batch_idx: 230 |  Loss: (0.0612) | Acc: (97.91%) (28949/29568)\n",
            "Epoch: 131 | Batch_idx: 240 |  Loss: (0.0615) | Acc: (97.89%) (30197/30848)\n",
            "Epoch: 131 | Batch_idx: 250 |  Loss: (0.0607) | Acc: (97.91%) (31457/32128)\n",
            "Epoch: 131 | Batch_idx: 260 |  Loss: (0.0604) | Acc: (97.92%) (32712/33408)\n",
            "Epoch: 131 | Batch_idx: 270 |  Loss: (0.0604) | Acc: (97.92%) (33966/34688)\n",
            "Epoch: 131 | Batch_idx: 280 |  Loss: (0.0605) | Acc: (97.90%) (35213/35968)\n",
            "Epoch: 131 | Batch_idx: 290 |  Loss: (0.0608) | Acc: (97.88%) (36458/37248)\n",
            "Epoch: 131 | Batch_idx: 300 |  Loss: (0.0606) | Acc: (97.88%) (37710/38528)\n",
            "Epoch: 131 | Batch_idx: 310 |  Loss: (0.0610) | Acc: (97.86%) (38957/39808)\n",
            "Epoch: 131 | Batch_idx: 320 |  Loss: (0.0611) | Acc: (97.86%) (40207/41088)\n",
            "Epoch: 131 | Batch_idx: 330 |  Loss: (0.0608) | Acc: (97.87%) (41467/42368)\n",
            "Epoch: 131 | Batch_idx: 340 |  Loss: (0.0610) | Acc: (97.87%) (42719/43648)\n",
            "Epoch: 131 | Batch_idx: 350 |  Loss: (0.0611) | Acc: (97.87%) (43972/44928)\n",
            "Epoch: 131 | Batch_idx: 360 |  Loss: (0.0611) | Acc: (97.87%) (45225/46208)\n",
            "Epoch: 131 | Batch_idx: 370 |  Loss: (0.0610) | Acc: (97.88%) (46480/47488)\n",
            "Epoch: 131 | Batch_idx: 380 |  Loss: (0.0605) | Acc: (97.90%) (47742/48768)\n",
            "Epoch: 131 | Batch_idx: 390 |  Loss: (0.0605) | Acc: (97.90%) (48949/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3348) | Acc: (91.50%) (9150/10000)\n",
            "Epoch: 132 | Batch_idx: 0 |  Loss: (0.0262) | Acc: (99.22%) (127/128)\n",
            "Epoch: 132 | Batch_idx: 10 |  Loss: (0.0514) | Acc: (98.30%) (1384/1408)\n",
            "Epoch: 132 | Batch_idx: 20 |  Loss: (0.0632) | Acc: (98.07%) (2636/2688)\n",
            "Epoch: 132 | Batch_idx: 30 |  Loss: (0.0680) | Acc: (97.96%) (3887/3968)\n",
            "Epoch: 132 | Batch_idx: 40 |  Loss: (0.0639) | Acc: (98.08%) (5147/5248)\n",
            "Epoch: 132 | Batch_idx: 50 |  Loss: (0.0653) | Acc: (98.04%) (6400/6528)\n",
            "Epoch: 132 | Batch_idx: 60 |  Loss: (0.0651) | Acc: (97.99%) (7651/7808)\n",
            "Epoch: 132 | Batch_idx: 70 |  Loss: (0.0664) | Acc: (97.93%) (8900/9088)\n",
            "Epoch: 132 | Batch_idx: 80 |  Loss: (0.0627) | Acc: (98.05%) (10166/10368)\n",
            "Epoch: 132 | Batch_idx: 90 |  Loss: (0.0614) | Acc: (98.09%) (11425/11648)\n",
            "Epoch: 132 | Batch_idx: 100 |  Loss: (0.0616) | Acc: (98.03%) (12673/12928)\n",
            "Epoch: 132 | Batch_idx: 110 |  Loss: (0.0604) | Acc: (98.04%) (13929/14208)\n",
            "Epoch: 132 | Batch_idx: 120 |  Loss: (0.0614) | Acc: (97.96%) (15172/15488)\n",
            "Epoch: 132 | Batch_idx: 130 |  Loss: (0.0618) | Acc: (97.93%) (16421/16768)\n",
            "Epoch: 132 | Batch_idx: 140 |  Loss: (0.0603) | Acc: (97.97%) (17681/18048)\n",
            "Epoch: 132 | Batch_idx: 150 |  Loss: (0.0598) | Acc: (97.98%) (18937/19328)\n",
            "Epoch: 132 | Batch_idx: 160 |  Loss: (0.0600) | Acc: (97.97%) (20190/20608)\n",
            "Epoch: 132 | Batch_idx: 170 |  Loss: (0.0608) | Acc: (97.93%) (21436/21888)\n",
            "Epoch: 132 | Batch_idx: 180 |  Loss: (0.0614) | Acc: (97.92%) (22685/23168)\n",
            "Epoch: 132 | Batch_idx: 190 |  Loss: (0.0619) | Acc: (97.91%) (23938/24448)\n",
            "Epoch: 132 | Batch_idx: 200 |  Loss: (0.0619) | Acc: (97.92%) (25192/25728)\n",
            "Epoch: 132 | Batch_idx: 210 |  Loss: (0.0626) | Acc: (97.90%) (26442/27008)\n",
            "Epoch: 132 | Batch_idx: 220 |  Loss: (0.0626) | Acc: (97.91%) (27698/28288)\n",
            "Epoch: 132 | Batch_idx: 230 |  Loss: (0.0627) | Acc: (97.91%) (28951/29568)\n",
            "Epoch: 132 | Batch_idx: 240 |  Loss: (0.0620) | Acc: (97.93%) (30209/30848)\n",
            "Epoch: 132 | Batch_idx: 250 |  Loss: (0.0622) | Acc: (97.91%) (31456/32128)\n",
            "Epoch: 132 | Batch_idx: 260 |  Loss: (0.0620) | Acc: (97.92%) (32714/33408)\n",
            "Epoch: 132 | Batch_idx: 270 |  Loss: (0.0617) | Acc: (97.92%) (33968/34688)\n",
            "Epoch: 132 | Batch_idx: 280 |  Loss: (0.0611) | Acc: (97.95%) (35232/35968)\n",
            "Epoch: 132 | Batch_idx: 290 |  Loss: (0.0609) | Acc: (97.96%) (36488/37248)\n",
            "Epoch: 132 | Batch_idx: 300 |  Loss: (0.0612) | Acc: (97.95%) (37737/38528)\n",
            "Epoch: 132 | Batch_idx: 310 |  Loss: (0.0614) | Acc: (97.95%) (38991/39808)\n",
            "Epoch: 132 | Batch_idx: 320 |  Loss: (0.0612) | Acc: (97.95%) (40244/41088)\n",
            "Epoch: 132 | Batch_idx: 330 |  Loss: (0.0611) | Acc: (97.94%) (41495/42368)\n",
            "Epoch: 132 | Batch_idx: 340 |  Loss: (0.0610) | Acc: (97.94%) (42747/43648)\n",
            "Epoch: 132 | Batch_idx: 350 |  Loss: (0.0610) | Acc: (97.94%) (44001/44928)\n",
            "Epoch: 132 | Batch_idx: 360 |  Loss: (0.0609) | Acc: (97.93%) (45253/46208)\n",
            "Epoch: 132 | Batch_idx: 370 |  Loss: (0.0610) | Acc: (97.93%) (46507/47488)\n",
            "Epoch: 132 | Batch_idx: 380 |  Loss: (0.0610) | Acc: (97.94%) (47761/48768)\n",
            "Epoch: 132 | Batch_idx: 390 |  Loss: (0.0610) | Acc: (97.93%) (48967/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3372) | Acc: (91.48%) (9148/10000)\n",
            "Epoch: 133 | Batch_idx: 0 |  Loss: (0.0226) | Acc: (99.22%) (127/128)\n",
            "Epoch: 133 | Batch_idx: 10 |  Loss: (0.0505) | Acc: (98.08%) (1381/1408)\n",
            "Epoch: 133 | Batch_idx: 20 |  Loss: (0.0489) | Acc: (98.29%) (2642/2688)\n",
            "Epoch: 133 | Batch_idx: 30 |  Loss: (0.0489) | Acc: (98.34%) (3902/3968)\n",
            "Epoch: 133 | Batch_idx: 40 |  Loss: (0.0539) | Acc: (98.19%) (5153/5248)\n",
            "Epoch: 133 | Batch_idx: 50 |  Loss: (0.0557) | Acc: (98.10%) (6404/6528)\n",
            "Epoch: 133 | Batch_idx: 60 |  Loss: (0.0560) | Acc: (98.07%) (7657/7808)\n",
            "Epoch: 133 | Batch_idx: 70 |  Loss: (0.0586) | Acc: (98.06%) (8912/9088)\n",
            "Epoch: 133 | Batch_idx: 80 |  Loss: (0.0592) | Acc: (98.05%) (10166/10368)\n",
            "Epoch: 133 | Batch_idx: 90 |  Loss: (0.0610) | Acc: (97.99%) (11414/11648)\n",
            "Epoch: 133 | Batch_idx: 100 |  Loss: (0.0609) | Acc: (98.00%) (12670/12928)\n",
            "Epoch: 133 | Batch_idx: 110 |  Loss: (0.0608) | Acc: (98.03%) (13928/14208)\n",
            "Epoch: 133 | Batch_idx: 120 |  Loss: (0.0610) | Acc: (98.01%) (15180/15488)\n",
            "Epoch: 133 | Batch_idx: 130 |  Loss: (0.0604) | Acc: (98.03%) (16438/16768)\n",
            "Epoch: 133 | Batch_idx: 140 |  Loss: (0.0606) | Acc: (98.01%) (17688/18048)\n",
            "Epoch: 133 | Batch_idx: 150 |  Loss: (0.0600) | Acc: (98.04%) (18950/19328)\n",
            "Epoch: 133 | Batch_idx: 160 |  Loss: (0.0604) | Acc: (98.03%) (20203/20608)\n",
            "Epoch: 133 | Batch_idx: 170 |  Loss: (0.0599) | Acc: (98.04%) (21460/21888)\n",
            "Epoch: 133 | Batch_idx: 180 |  Loss: (0.0594) | Acc: (98.06%) (22718/23168)\n",
            "Epoch: 133 | Batch_idx: 190 |  Loss: (0.0595) | Acc: (98.05%) (23972/24448)\n",
            "Epoch: 133 | Batch_idx: 200 |  Loss: (0.0590) | Acc: (98.06%) (25230/25728)\n",
            "Epoch: 133 | Batch_idx: 210 |  Loss: (0.0594) | Acc: (98.05%) (26480/27008)\n",
            "Epoch: 133 | Batch_idx: 220 |  Loss: (0.0588) | Acc: (98.06%) (27738/28288)\n",
            "Epoch: 133 | Batch_idx: 230 |  Loss: (0.0581) | Acc: (98.08%) (29001/29568)\n",
            "Epoch: 133 | Batch_idx: 240 |  Loss: (0.0585) | Acc: (98.06%) (30250/30848)\n",
            "Epoch: 133 | Batch_idx: 250 |  Loss: (0.0584) | Acc: (98.06%) (31506/32128)\n",
            "Epoch: 133 | Batch_idx: 260 |  Loss: (0.0583) | Acc: (98.06%) (32760/33408)\n",
            "Epoch: 133 | Batch_idx: 270 |  Loss: (0.0580) | Acc: (98.06%) (34016/34688)\n",
            "Epoch: 133 | Batch_idx: 280 |  Loss: (0.0576) | Acc: (98.08%) (35276/35968)\n",
            "Epoch: 133 | Batch_idx: 290 |  Loss: (0.0575) | Acc: (98.09%) (36535/37248)\n",
            "Epoch: 133 | Batch_idx: 300 |  Loss: (0.0579) | Acc: (98.07%) (37786/38528)\n",
            "Epoch: 133 | Batch_idx: 310 |  Loss: (0.0580) | Acc: (98.07%) (39041/39808)\n",
            "Epoch: 133 | Batch_idx: 320 |  Loss: (0.0583) | Acc: (98.07%) (40295/41088)\n",
            "Epoch: 133 | Batch_idx: 330 |  Loss: (0.0588) | Acc: (98.03%) (41533/42368)\n",
            "Epoch: 133 | Batch_idx: 340 |  Loss: (0.0588) | Acc: (98.03%) (42786/43648)\n",
            "Epoch: 133 | Batch_idx: 350 |  Loss: (0.0590) | Acc: (98.01%) (44034/44928)\n",
            "Epoch: 133 | Batch_idx: 360 |  Loss: (0.0588) | Acc: (98.02%) (45294/46208)\n",
            "Epoch: 133 | Batch_idx: 370 |  Loss: (0.0591) | Acc: (98.01%) (46544/47488)\n",
            "Epoch: 133 | Batch_idx: 380 |  Loss: (0.0595) | Acc: (97.98%) (47785/48768)\n",
            "Epoch: 133 | Batch_idx: 390 |  Loss: (0.0594) | Acc: (97.99%) (48993/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3390) | Acc: (91.47%) (9147/10000)\n",
            "Epoch: 134 | Batch_idx: 0 |  Loss: (0.0704) | Acc: (97.66%) (125/128)\n",
            "Epoch: 134 | Batch_idx: 10 |  Loss: (0.0834) | Acc: (97.30%) (1370/1408)\n",
            "Epoch: 134 | Batch_idx: 20 |  Loss: (0.0700) | Acc: (97.77%) (2628/2688)\n",
            "Epoch: 134 | Batch_idx: 30 |  Loss: (0.0671) | Acc: (97.83%) (3882/3968)\n",
            "Epoch: 134 | Batch_idx: 40 |  Loss: (0.0632) | Acc: (97.94%) (5140/5248)\n",
            "Epoch: 134 | Batch_idx: 50 |  Loss: (0.0655) | Acc: (97.81%) (6385/6528)\n",
            "Epoch: 134 | Batch_idx: 60 |  Loss: (0.0648) | Acc: (97.84%) (7639/7808)\n",
            "Epoch: 134 | Batch_idx: 70 |  Loss: (0.0644) | Acc: (97.88%) (8895/9088)\n",
            "Epoch: 134 | Batch_idx: 80 |  Loss: (0.0618) | Acc: (97.97%) (10158/10368)\n",
            "Epoch: 134 | Batch_idx: 90 |  Loss: (0.0621) | Acc: (97.96%) (11410/11648)\n",
            "Epoch: 134 | Batch_idx: 100 |  Loss: (0.0607) | Acc: (98.00%) (12669/12928)\n",
            "Epoch: 134 | Batch_idx: 110 |  Loss: (0.0600) | Acc: (98.02%) (13926/14208)\n",
            "Epoch: 134 | Batch_idx: 120 |  Loss: (0.0605) | Acc: (98.02%) (15182/15488)\n",
            "Epoch: 134 | Batch_idx: 130 |  Loss: (0.0613) | Acc: (98.00%) (16432/16768)\n",
            "Epoch: 134 | Batch_idx: 140 |  Loss: (0.0618) | Acc: (97.97%) (17681/18048)\n",
            "Epoch: 134 | Batch_idx: 150 |  Loss: (0.0622) | Acc: (97.95%) (18932/19328)\n",
            "Epoch: 134 | Batch_idx: 160 |  Loss: (0.0615) | Acc: (97.99%) (20194/20608)\n",
            "Epoch: 134 | Batch_idx: 170 |  Loss: (0.0607) | Acc: (98.01%) (21452/21888)\n",
            "Epoch: 134 | Batch_idx: 180 |  Loss: (0.0602) | Acc: (98.01%) (22708/23168)\n",
            "Epoch: 134 | Batch_idx: 190 |  Loss: (0.0603) | Acc: (98.01%) (23961/24448)\n",
            "Epoch: 134 | Batch_idx: 200 |  Loss: (0.0599) | Acc: (97.99%) (25212/25728)\n",
            "Epoch: 134 | Batch_idx: 210 |  Loss: (0.0599) | Acc: (97.99%) (26466/27008)\n",
            "Epoch: 134 | Batch_idx: 220 |  Loss: (0.0601) | Acc: (97.98%) (27717/28288)\n",
            "Epoch: 134 | Batch_idx: 230 |  Loss: (0.0604) | Acc: (97.97%) (28968/29568)\n",
            "Epoch: 134 | Batch_idx: 240 |  Loss: (0.0605) | Acc: (97.98%) (30225/30848)\n",
            "Epoch: 134 | Batch_idx: 250 |  Loss: (0.0604) | Acc: (97.97%) (31477/32128)\n",
            "Epoch: 134 | Batch_idx: 260 |  Loss: (0.0602) | Acc: (97.99%) (32736/33408)\n",
            "Epoch: 134 | Batch_idx: 270 |  Loss: (0.0604) | Acc: (97.97%) (33984/34688)\n",
            "Epoch: 134 | Batch_idx: 280 |  Loss: (0.0606) | Acc: (97.97%) (35237/35968)\n",
            "Epoch: 134 | Batch_idx: 290 |  Loss: (0.0609) | Acc: (97.95%) (36485/37248)\n",
            "Epoch: 134 | Batch_idx: 300 |  Loss: (0.0607) | Acc: (97.95%) (37738/38528)\n",
            "Epoch: 134 | Batch_idx: 310 |  Loss: (0.0610) | Acc: (97.93%) (38985/39808)\n",
            "Epoch: 134 | Batch_idx: 320 |  Loss: (0.0612) | Acc: (97.94%) (40242/41088)\n",
            "Epoch: 134 | Batch_idx: 330 |  Loss: (0.0611) | Acc: (97.94%) (41496/42368)\n",
            "Epoch: 134 | Batch_idx: 340 |  Loss: (0.0612) | Acc: (97.94%) (42748/43648)\n",
            "Epoch: 134 | Batch_idx: 350 |  Loss: (0.0614) | Acc: (97.92%) (43994/44928)\n",
            "Epoch: 134 | Batch_idx: 360 |  Loss: (0.0614) | Acc: (97.91%) (45242/46208)\n",
            "Epoch: 134 | Batch_idx: 370 |  Loss: (0.0614) | Acc: (97.92%) (46500/47488)\n",
            "Epoch: 134 | Batch_idx: 380 |  Loss: (0.0616) | Acc: (97.92%) (47755/48768)\n",
            "Epoch: 134 | Batch_idx: 390 |  Loss: (0.0613) | Acc: (97.94%) (48970/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3344) | Acc: (91.49%) (9149/10000)\n",
            "Epoch: 135 | Batch_idx: 0 |  Loss: (0.0337) | Acc: (99.22%) (127/128)\n",
            "Epoch: 135 | Batch_idx: 10 |  Loss: (0.0564) | Acc: (98.08%) (1381/1408)\n",
            "Epoch: 135 | Batch_idx: 20 |  Loss: (0.0570) | Acc: (98.21%) (2640/2688)\n",
            "Epoch: 135 | Batch_idx: 30 |  Loss: (0.0552) | Acc: (98.26%) (3899/3968)\n",
            "Epoch: 135 | Batch_idx: 40 |  Loss: (0.0554) | Acc: (98.11%) (5149/5248)\n",
            "Epoch: 135 | Batch_idx: 50 |  Loss: (0.0584) | Acc: (98.02%) (6399/6528)\n",
            "Epoch: 135 | Batch_idx: 60 |  Loss: (0.0570) | Acc: (98.10%) (7660/7808)\n",
            "Epoch: 135 | Batch_idx: 70 |  Loss: (0.0583) | Acc: (98.10%) (8915/9088)\n",
            "Epoch: 135 | Batch_idx: 80 |  Loss: (0.0587) | Acc: (98.10%) (10171/10368)\n",
            "Epoch: 135 | Batch_idx: 90 |  Loss: (0.0597) | Acc: (98.02%) (11417/11648)\n",
            "Epoch: 135 | Batch_idx: 100 |  Loss: (0.0619) | Acc: (97.97%) (12665/12928)\n",
            "Epoch: 135 | Batch_idx: 110 |  Loss: (0.0624) | Acc: (97.94%) (13915/14208)\n",
            "Epoch: 135 | Batch_idx: 120 |  Loss: (0.0623) | Acc: (97.92%) (15166/15488)\n",
            "Epoch: 135 | Batch_idx: 130 |  Loss: (0.0620) | Acc: (97.92%) (16420/16768)\n",
            "Epoch: 135 | Batch_idx: 140 |  Loss: (0.0624) | Acc: (97.84%) (17659/18048)\n",
            "Epoch: 135 | Batch_idx: 150 |  Loss: (0.0624) | Acc: (97.84%) (18910/19328)\n",
            "Epoch: 135 | Batch_idx: 160 |  Loss: (0.0627) | Acc: (97.82%) (20159/20608)\n",
            "Epoch: 135 | Batch_idx: 170 |  Loss: (0.0625) | Acc: (97.82%) (21410/21888)\n",
            "Epoch: 135 | Batch_idx: 180 |  Loss: (0.0620) | Acc: (97.83%) (22666/23168)\n",
            "Epoch: 135 | Batch_idx: 190 |  Loss: (0.0619) | Acc: (97.83%) (23918/24448)\n",
            "Epoch: 135 | Batch_idx: 200 |  Loss: (0.0608) | Acc: (97.87%) (25179/25728)\n",
            "Epoch: 135 | Batch_idx: 210 |  Loss: (0.0603) | Acc: (97.89%) (26438/27008)\n",
            "Epoch: 135 | Batch_idx: 220 |  Loss: (0.0612) | Acc: (97.86%) (27683/28288)\n",
            "Epoch: 135 | Batch_idx: 230 |  Loss: (0.0611) | Acc: (97.86%) (28934/29568)\n",
            "Epoch: 135 | Batch_idx: 240 |  Loss: (0.0610) | Acc: (97.86%) (30189/30848)\n",
            "Epoch: 135 | Batch_idx: 250 |  Loss: (0.0613) | Acc: (97.86%) (31441/32128)\n",
            "Epoch: 135 | Batch_idx: 260 |  Loss: (0.0615) | Acc: (97.86%) (32692/33408)\n",
            "Epoch: 135 | Batch_idx: 270 |  Loss: (0.0612) | Acc: (97.87%) (33948/34688)\n",
            "Epoch: 135 | Batch_idx: 280 |  Loss: (0.0612) | Acc: (97.87%) (35202/35968)\n",
            "Epoch: 135 | Batch_idx: 290 |  Loss: (0.0611) | Acc: (97.87%) (36456/37248)\n",
            "Epoch: 135 | Batch_idx: 300 |  Loss: (0.0609) | Acc: (97.90%) (37718/38528)\n",
            "Epoch: 135 | Batch_idx: 310 |  Loss: (0.0607) | Acc: (97.91%) (38977/39808)\n",
            "Epoch: 135 | Batch_idx: 320 |  Loss: (0.0608) | Acc: (97.92%) (40232/41088)\n",
            "Epoch: 135 | Batch_idx: 330 |  Loss: (0.0605) | Acc: (97.93%) (41490/42368)\n",
            "Epoch: 135 | Batch_idx: 340 |  Loss: (0.0604) | Acc: (97.93%) (42746/43648)\n",
            "Epoch: 135 | Batch_idx: 350 |  Loss: (0.0608) | Acc: (97.90%) (43986/44928)\n",
            "Epoch: 135 | Batch_idx: 360 |  Loss: (0.0608) | Acc: (97.91%) (45240/46208)\n",
            "Epoch: 135 | Batch_idx: 370 |  Loss: (0.0607) | Acc: (97.90%) (46491/47488)\n",
            "Epoch: 135 | Batch_idx: 380 |  Loss: (0.0610) | Acc: (97.89%) (47737/48768)\n",
            "Epoch: 135 | Batch_idx: 390 |  Loss: (0.0612) | Acc: (97.87%) (48937/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3401) | Acc: (91.48%) (9148/10000)\n",
            "Epoch: 136 | Batch_idx: 0 |  Loss: (0.0399) | Acc: (98.44%) (126/128)\n",
            "Epoch: 136 | Batch_idx: 10 |  Loss: (0.0498) | Acc: (97.87%) (1378/1408)\n",
            "Epoch: 136 | Batch_idx: 20 |  Loss: (0.0627) | Acc: (97.62%) (2624/2688)\n",
            "Epoch: 136 | Batch_idx: 30 |  Loss: (0.0612) | Acc: (97.73%) (3878/3968)\n",
            "Epoch: 136 | Batch_idx: 40 |  Loss: (0.0608) | Acc: (97.77%) (5131/5248)\n",
            "Epoch: 136 | Batch_idx: 50 |  Loss: (0.0583) | Acc: (97.95%) (6394/6528)\n",
            "Epoch: 136 | Batch_idx: 60 |  Loss: (0.0616) | Acc: (97.84%) (7639/7808)\n",
            "Epoch: 136 | Batch_idx: 70 |  Loss: (0.0612) | Acc: (97.87%) (8894/9088)\n",
            "Epoch: 136 | Batch_idx: 80 |  Loss: (0.0610) | Acc: (97.88%) (10148/10368)\n",
            "Epoch: 136 | Batch_idx: 90 |  Loss: (0.0601) | Acc: (97.88%) (11401/11648)\n",
            "Epoch: 136 | Batch_idx: 100 |  Loss: (0.0599) | Acc: (97.90%) (12656/12928)\n",
            "Epoch: 136 | Batch_idx: 110 |  Loss: (0.0608) | Acc: (97.87%) (13905/14208)\n",
            "Epoch: 136 | Batch_idx: 120 |  Loss: (0.0617) | Acc: (97.84%) (15153/15488)\n",
            "Epoch: 136 | Batch_idx: 130 |  Loss: (0.0611) | Acc: (97.88%) (16412/16768)\n",
            "Epoch: 136 | Batch_idx: 140 |  Loss: (0.0609) | Acc: (97.91%) (17670/18048)\n",
            "Epoch: 136 | Batch_idx: 150 |  Loss: (0.0616) | Acc: (97.89%) (18920/19328)\n",
            "Epoch: 136 | Batch_idx: 160 |  Loss: (0.0615) | Acc: (97.90%) (20175/20608)\n",
            "Epoch: 136 | Batch_idx: 170 |  Loss: (0.0610) | Acc: (97.92%) (21433/21888)\n",
            "Epoch: 136 | Batch_idx: 180 |  Loss: (0.0618) | Acc: (97.92%) (22687/23168)\n",
            "Epoch: 136 | Batch_idx: 190 |  Loss: (0.0614) | Acc: (97.93%) (23941/24448)\n",
            "Epoch: 136 | Batch_idx: 200 |  Loss: (0.0614) | Acc: (97.94%) (25198/25728)\n",
            "Epoch: 136 | Batch_idx: 210 |  Loss: (0.0611) | Acc: (97.94%) (26451/27008)\n",
            "Epoch: 136 | Batch_idx: 220 |  Loss: (0.0613) | Acc: (97.93%) (27703/28288)\n",
            "Epoch: 136 | Batch_idx: 230 |  Loss: (0.0616) | Acc: (97.92%) (28952/29568)\n",
            "Epoch: 136 | Batch_idx: 240 |  Loss: (0.0610) | Acc: (97.94%) (30211/30848)\n",
            "Epoch: 136 | Batch_idx: 250 |  Loss: (0.0613) | Acc: (97.92%) (31461/32128)\n",
            "Epoch: 136 | Batch_idx: 260 |  Loss: (0.0611) | Acc: (97.92%) (32713/33408)\n",
            "Epoch: 136 | Batch_idx: 270 |  Loss: (0.0611) | Acc: (97.91%) (33962/34688)\n",
            "Epoch: 136 | Batch_idx: 280 |  Loss: (0.0609) | Acc: (97.91%) (35215/35968)\n",
            "Epoch: 136 | Batch_idx: 290 |  Loss: (0.0612) | Acc: (97.91%) (36470/37248)\n",
            "Epoch: 136 | Batch_idx: 300 |  Loss: (0.0606) | Acc: (97.94%) (37735/38528)\n",
            "Epoch: 136 | Batch_idx: 310 |  Loss: (0.0605) | Acc: (97.94%) (38986/39808)\n",
            "Epoch: 136 | Batch_idx: 320 |  Loss: (0.0608) | Acc: (97.94%) (40242/41088)\n",
            "Epoch: 136 | Batch_idx: 330 |  Loss: (0.0614) | Acc: (97.92%) (41485/42368)\n",
            "Epoch: 136 | Batch_idx: 340 |  Loss: (0.0621) | Acc: (97.89%) (42727/43648)\n",
            "Epoch: 136 | Batch_idx: 350 |  Loss: (0.0620) | Acc: (97.89%) (43978/44928)\n",
            "Epoch: 136 | Batch_idx: 360 |  Loss: (0.0620) | Acc: (97.89%) (45235/46208)\n",
            "Epoch: 136 | Batch_idx: 370 |  Loss: (0.0617) | Acc: (97.91%) (46494/47488)\n",
            "Epoch: 136 | Batch_idx: 380 |  Loss: (0.0615) | Acc: (97.91%) (47748/48768)\n",
            "Epoch: 136 | Batch_idx: 390 |  Loss: (0.0620) | Acc: (97.90%) (48949/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3371) | Acc: (91.57%) (9157/10000)\n",
            "Epoch: 137 | Batch_idx: 0 |  Loss: (0.0339) | Acc: (97.66%) (125/128)\n",
            "Epoch: 137 | Batch_idx: 10 |  Loss: (0.0634) | Acc: (97.87%) (1378/1408)\n",
            "Epoch: 137 | Batch_idx: 20 |  Loss: (0.0610) | Acc: (97.84%) (2630/2688)\n",
            "Epoch: 137 | Batch_idx: 30 |  Loss: (0.0574) | Acc: (97.91%) (3885/3968)\n",
            "Epoch: 137 | Batch_idx: 40 |  Loss: (0.0594) | Acc: (97.83%) (5134/5248)\n",
            "Epoch: 137 | Batch_idx: 50 |  Loss: (0.0618) | Acc: (97.73%) (6380/6528)\n",
            "Epoch: 137 | Batch_idx: 60 |  Loss: (0.0601) | Acc: (97.82%) (7638/7808)\n",
            "Epoch: 137 | Batch_idx: 70 |  Loss: (0.0602) | Acc: (97.85%) (8893/9088)\n",
            "Epoch: 137 | Batch_idx: 80 |  Loss: (0.0623) | Acc: (97.79%) (10139/10368)\n",
            "Epoch: 137 | Batch_idx: 90 |  Loss: (0.0619) | Acc: (97.79%) (11390/11648)\n",
            "Epoch: 137 | Batch_idx: 100 |  Loss: (0.0625) | Acc: (97.76%) (12638/12928)\n",
            "Epoch: 137 | Batch_idx: 110 |  Loss: (0.0612) | Acc: (97.84%) (13901/14208)\n",
            "Epoch: 137 | Batch_idx: 120 |  Loss: (0.0599) | Acc: (97.88%) (15160/15488)\n",
            "Epoch: 137 | Batch_idx: 130 |  Loss: (0.0600) | Acc: (97.90%) (16416/16768)\n",
            "Epoch: 137 | Batch_idx: 140 |  Loss: (0.0602) | Acc: (97.92%) (17672/18048)\n",
            "Epoch: 137 | Batch_idx: 150 |  Loss: (0.0594) | Acc: (97.95%) (18932/19328)\n",
            "Epoch: 137 | Batch_idx: 160 |  Loss: (0.0594) | Acc: (97.92%) (20180/20608)\n",
            "Epoch: 137 | Batch_idx: 170 |  Loss: (0.0602) | Acc: (97.91%) (21430/21888)\n",
            "Epoch: 137 | Batch_idx: 180 |  Loss: (0.0593) | Acc: (97.94%) (22690/23168)\n",
            "Epoch: 137 | Batch_idx: 190 |  Loss: (0.0591) | Acc: (97.94%) (23945/24448)\n",
            "Epoch: 137 | Batch_idx: 200 |  Loss: (0.0598) | Acc: (97.91%) (25191/25728)\n",
            "Epoch: 137 | Batch_idx: 210 |  Loss: (0.0601) | Acc: (97.92%) (26445/27008)\n",
            "Epoch: 137 | Batch_idx: 220 |  Loss: (0.0605) | Acc: (97.89%) (27692/28288)\n",
            "Epoch: 137 | Batch_idx: 230 |  Loss: (0.0594) | Acc: (97.93%) (28957/29568)\n",
            "Epoch: 137 | Batch_idx: 240 |  Loss: (0.0589) | Acc: (97.96%) (30220/30848)\n",
            "Epoch: 137 | Batch_idx: 250 |  Loss: (0.0593) | Acc: (97.96%) (31472/32128)\n",
            "Epoch: 137 | Batch_idx: 260 |  Loss: (0.0592) | Acc: (97.96%) (32727/33408)\n",
            "Epoch: 137 | Batch_idx: 270 |  Loss: (0.0588) | Acc: (97.98%) (33989/34688)\n",
            "Epoch: 137 | Batch_idx: 280 |  Loss: (0.0592) | Acc: (97.97%) (35237/35968)\n",
            "Epoch: 137 | Batch_idx: 290 |  Loss: (0.0594) | Acc: (97.95%) (36485/37248)\n",
            "Epoch: 137 | Batch_idx: 300 |  Loss: (0.0593) | Acc: (97.97%) (37746/38528)\n",
            "Epoch: 137 | Batch_idx: 310 |  Loss: (0.0594) | Acc: (97.97%) (38998/39808)\n",
            "Epoch: 137 | Batch_idx: 320 |  Loss: (0.0594) | Acc: (97.97%) (40255/41088)\n",
            "Epoch: 137 | Batch_idx: 330 |  Loss: (0.0593) | Acc: (97.97%) (41510/42368)\n",
            "Epoch: 137 | Batch_idx: 340 |  Loss: (0.0591) | Acc: (97.98%) (42767/43648)\n",
            "Epoch: 137 | Batch_idx: 350 |  Loss: (0.0594) | Acc: (97.97%) (44015/44928)\n",
            "Epoch: 137 | Batch_idx: 360 |  Loss: (0.0596) | Acc: (97.96%) (45266/46208)\n",
            "Epoch: 137 | Batch_idx: 370 |  Loss: (0.0594) | Acc: (97.97%) (46525/47488)\n",
            "Epoch: 137 | Batch_idx: 380 |  Loss: (0.0597) | Acc: (97.96%) (47771/48768)\n",
            "Epoch: 137 | Batch_idx: 390 |  Loss: (0.0599) | Acc: (97.95%) (48976/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3377) | Acc: (91.56%) (9156/10000)\n",
            "Epoch: 138 | Batch_idx: 0 |  Loss: (0.0460) | Acc: (98.44%) (126/128)\n",
            "Epoch: 138 | Batch_idx: 10 |  Loss: (0.0393) | Acc: (98.79%) (1391/1408)\n",
            "Epoch: 138 | Batch_idx: 20 |  Loss: (0.0495) | Acc: (98.44%) (2646/2688)\n",
            "Epoch: 138 | Batch_idx: 30 |  Loss: (0.0491) | Acc: (98.34%) (3902/3968)\n",
            "Epoch: 138 | Batch_idx: 40 |  Loss: (0.0511) | Acc: (98.29%) (5158/5248)\n",
            "Epoch: 138 | Batch_idx: 50 |  Loss: (0.0520) | Acc: (98.33%) (6419/6528)\n",
            "Epoch: 138 | Batch_idx: 60 |  Loss: (0.0529) | Acc: (98.23%) (7670/7808)\n",
            "Epoch: 138 | Batch_idx: 70 |  Loss: (0.0510) | Acc: (98.32%) (8935/9088)\n",
            "Epoch: 138 | Batch_idx: 80 |  Loss: (0.0499) | Acc: (98.34%) (10196/10368)\n",
            "Epoch: 138 | Batch_idx: 90 |  Loss: (0.0516) | Acc: (98.32%) (11452/11648)\n",
            "Epoch: 138 | Batch_idx: 100 |  Loss: (0.0511) | Acc: (98.31%) (12709/12928)\n",
            "Epoch: 138 | Batch_idx: 110 |  Loss: (0.0513) | Acc: (98.30%) (13967/14208)\n",
            "Epoch: 138 | Batch_idx: 120 |  Loss: (0.0513) | Acc: (98.32%) (15228/15488)\n",
            "Epoch: 138 | Batch_idx: 130 |  Loss: (0.0531) | Acc: (98.31%) (16485/16768)\n",
            "Epoch: 138 | Batch_idx: 140 |  Loss: (0.0543) | Acc: (98.29%) (17739/18048)\n",
            "Epoch: 138 | Batch_idx: 150 |  Loss: (0.0537) | Acc: (98.28%) (18996/19328)\n",
            "Epoch: 138 | Batch_idx: 160 |  Loss: (0.0535) | Acc: (98.27%) (20251/20608)\n",
            "Epoch: 138 | Batch_idx: 170 |  Loss: (0.0540) | Acc: (98.26%) (21507/21888)\n",
            "Epoch: 138 | Batch_idx: 180 |  Loss: (0.0535) | Acc: (98.25%) (22762/23168)\n",
            "Epoch: 138 | Batch_idx: 190 |  Loss: (0.0532) | Acc: (98.25%) (24021/24448)\n",
            "Epoch: 138 | Batch_idx: 200 |  Loss: (0.0527) | Acc: (98.25%) (25279/25728)\n",
            "Epoch: 138 | Batch_idx: 210 |  Loss: (0.0543) | Acc: (98.21%) (26525/27008)\n",
            "Epoch: 138 | Batch_idx: 220 |  Loss: (0.0548) | Acc: (98.20%) (27778/28288)\n",
            "Epoch: 138 | Batch_idx: 230 |  Loss: (0.0559) | Acc: (98.17%) (29027/29568)\n",
            "Epoch: 138 | Batch_idx: 240 |  Loss: (0.0565) | Acc: (98.15%) (30276/30848)\n",
            "Epoch: 138 | Batch_idx: 250 |  Loss: (0.0568) | Acc: (98.13%) (31527/32128)\n",
            "Epoch: 138 | Batch_idx: 260 |  Loss: (0.0578) | Acc: (98.08%) (32766/33408)\n",
            "Epoch: 138 | Batch_idx: 270 |  Loss: (0.0577) | Acc: (98.07%) (34018/34688)\n",
            "Epoch: 138 | Batch_idx: 280 |  Loss: (0.0572) | Acc: (98.08%) (35276/35968)\n",
            "Epoch: 138 | Batch_idx: 290 |  Loss: (0.0570) | Acc: (98.09%) (36535/37248)\n",
            "Epoch: 138 | Batch_idx: 300 |  Loss: (0.0571) | Acc: (98.08%) (37788/38528)\n",
            "Epoch: 138 | Batch_idx: 310 |  Loss: (0.0571) | Acc: (98.08%) (39042/39808)\n",
            "Epoch: 138 | Batch_idx: 320 |  Loss: (0.0571) | Acc: (98.08%) (40299/41088)\n",
            "Epoch: 138 | Batch_idx: 330 |  Loss: (0.0573) | Acc: (98.08%) (41554/42368)\n",
            "Epoch: 138 | Batch_idx: 340 |  Loss: (0.0573) | Acc: (98.09%) (42813/43648)\n",
            "Epoch: 138 | Batch_idx: 350 |  Loss: (0.0571) | Acc: (98.10%) (44074/44928)\n",
            "Epoch: 138 | Batch_idx: 360 |  Loss: (0.0571) | Acc: (98.10%) (45330/46208)\n",
            "Epoch: 138 | Batch_idx: 370 |  Loss: (0.0569) | Acc: (98.09%) (46582/47488)\n",
            "Epoch: 138 | Batch_idx: 380 |  Loss: (0.0567) | Acc: (98.10%) (47841/48768)\n",
            "Epoch: 138 | Batch_idx: 390 |  Loss: (0.0569) | Acc: (98.10%) (49048/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3339) | Acc: (91.61%) (9161/10000)\n",
            "Epoch: 139 | Batch_idx: 0 |  Loss: (0.0423) | Acc: (99.22%) (127/128)\n",
            "Epoch: 139 | Batch_idx: 10 |  Loss: (0.0657) | Acc: (97.80%) (1377/1408)\n",
            "Epoch: 139 | Batch_idx: 20 |  Loss: (0.0637) | Acc: (97.95%) (2633/2688)\n",
            "Epoch: 139 | Batch_idx: 30 |  Loss: (0.0581) | Acc: (98.21%) (3897/3968)\n",
            "Epoch: 139 | Batch_idx: 40 |  Loss: (0.0566) | Acc: (98.21%) (5154/5248)\n",
            "Epoch: 139 | Batch_idx: 50 |  Loss: (0.0580) | Acc: (98.16%) (6408/6528)\n",
            "Epoch: 139 | Batch_idx: 60 |  Loss: (0.0575) | Acc: (98.14%) (7663/7808)\n",
            "Epoch: 139 | Batch_idx: 70 |  Loss: (0.0565) | Acc: (98.16%) (8921/9088)\n",
            "Epoch: 139 | Batch_idx: 80 |  Loss: (0.0589) | Acc: (98.13%) (10174/10368)\n",
            "Epoch: 139 | Batch_idx: 90 |  Loss: (0.0592) | Acc: (98.11%) (11428/11648)\n",
            "Epoch: 139 | Batch_idx: 100 |  Loss: (0.0592) | Acc: (98.08%) (12680/12928)\n",
            "Epoch: 139 | Batch_idx: 110 |  Loss: (0.0595) | Acc: (98.08%) (13935/14208)\n",
            "Epoch: 139 | Batch_idx: 120 |  Loss: (0.0600) | Acc: (98.07%) (15189/15488)\n",
            "Epoch: 139 | Batch_idx: 130 |  Loss: (0.0603) | Acc: (98.05%) (16441/16768)\n",
            "Epoch: 139 | Batch_idx: 140 |  Loss: (0.0605) | Acc: (98.03%) (17692/18048)\n",
            "Epoch: 139 | Batch_idx: 150 |  Loss: (0.0603) | Acc: (98.02%) (18946/19328)\n",
            "Epoch: 139 | Batch_idx: 160 |  Loss: (0.0593) | Acc: (98.03%) (20202/20608)\n",
            "Epoch: 139 | Batch_idx: 170 |  Loss: (0.0588) | Acc: (98.05%) (21461/21888)\n",
            "Epoch: 139 | Batch_idx: 180 |  Loss: (0.0580) | Acc: (98.05%) (22717/23168)\n",
            "Epoch: 139 | Batch_idx: 190 |  Loss: (0.0576) | Acc: (98.06%) (23973/24448)\n",
            "Epoch: 139 | Batch_idx: 200 |  Loss: (0.0572) | Acc: (98.07%) (25232/25728)\n",
            "Epoch: 139 | Batch_idx: 210 |  Loss: (0.0571) | Acc: (98.09%) (26492/27008)\n",
            "Epoch: 139 | Batch_idx: 220 |  Loss: (0.0572) | Acc: (98.10%) (27750/28288)\n",
            "Epoch: 139 | Batch_idx: 230 |  Loss: (0.0575) | Acc: (98.08%) (29001/29568)\n",
            "Epoch: 139 | Batch_idx: 240 |  Loss: (0.0581) | Acc: (98.06%) (30250/30848)\n",
            "Epoch: 139 | Batch_idx: 250 |  Loss: (0.0585) | Acc: (98.07%) (31507/32128)\n",
            "Epoch: 139 | Batch_idx: 260 |  Loss: (0.0579) | Acc: (98.07%) (32763/33408)\n",
            "Epoch: 139 | Batch_idx: 270 |  Loss: (0.0581) | Acc: (98.04%) (34008/34688)\n",
            "Epoch: 139 | Batch_idx: 280 |  Loss: (0.0578) | Acc: (98.06%) (35270/35968)\n",
            "Epoch: 139 | Batch_idx: 290 |  Loss: (0.0581) | Acc: (98.05%) (36523/37248)\n",
            "Epoch: 139 | Batch_idx: 300 |  Loss: (0.0586) | Acc: (98.05%) (37776/38528)\n",
            "Epoch: 139 | Batch_idx: 310 |  Loss: (0.0585) | Acc: (98.06%) (39034/39808)\n",
            "Epoch: 139 | Batch_idx: 320 |  Loss: (0.0588) | Acc: (98.04%) (40283/41088)\n",
            "Epoch: 139 | Batch_idx: 330 |  Loss: (0.0588) | Acc: (98.05%) (41541/42368)\n",
            "Epoch: 139 | Batch_idx: 340 |  Loss: (0.0586) | Acc: (98.05%) (42796/43648)\n",
            "Epoch: 139 | Batch_idx: 350 |  Loss: (0.0587) | Acc: (98.05%) (44050/44928)\n",
            "Epoch: 139 | Batch_idx: 360 |  Loss: (0.0584) | Acc: (98.05%) (45306/46208)\n",
            "Epoch: 139 | Batch_idx: 370 |  Loss: (0.0585) | Acc: (98.05%) (46562/47488)\n",
            "Epoch: 139 | Batch_idx: 380 |  Loss: (0.0588) | Acc: (98.05%) (47818/48768)\n",
            "Epoch: 139 | Batch_idx: 390 |  Loss: (0.0586) | Acc: (98.05%) (49023/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3403) | Acc: (91.47%) (9147/10000)\n",
            "Epoch: 140 | Batch_idx: 0 |  Loss: (0.0378) | Acc: (98.44%) (126/128)\n",
            "Epoch: 140 | Batch_idx: 10 |  Loss: (0.0346) | Acc: (98.86%) (1392/1408)\n",
            "Epoch: 140 | Batch_idx: 20 |  Loss: (0.0450) | Acc: (98.55%) (2649/2688)\n",
            "Epoch: 140 | Batch_idx: 30 |  Loss: (0.0488) | Acc: (98.44%) (3906/3968)\n",
            "Epoch: 140 | Batch_idx: 40 |  Loss: (0.0483) | Acc: (98.42%) (5165/5248)\n",
            "Epoch: 140 | Batch_idx: 50 |  Loss: (0.0491) | Acc: (98.36%) (6421/6528)\n",
            "Epoch: 140 | Batch_idx: 60 |  Loss: (0.0474) | Acc: (98.42%) (7685/7808)\n",
            "Epoch: 140 | Batch_idx: 70 |  Loss: (0.0476) | Acc: (98.44%) (8946/9088)\n",
            "Epoch: 140 | Batch_idx: 80 |  Loss: (0.0480) | Acc: (98.44%) (10206/10368)\n",
            "Epoch: 140 | Batch_idx: 90 |  Loss: (0.0479) | Acc: (98.41%) (11463/11648)\n",
            "Epoch: 140 | Batch_idx: 100 |  Loss: (0.0492) | Acc: (98.42%) (12724/12928)\n",
            "Epoch: 140 | Batch_idx: 110 |  Loss: (0.0499) | Acc: (98.40%) (13981/14208)\n",
            "Epoch: 140 | Batch_idx: 120 |  Loss: (0.0492) | Acc: (98.42%) (15243/15488)\n",
            "Epoch: 140 | Batch_idx: 130 |  Loss: (0.0501) | Acc: (98.37%) (16494/16768)\n",
            "Epoch: 140 | Batch_idx: 140 |  Loss: (0.0508) | Acc: (98.33%) (17747/18048)\n",
            "Epoch: 140 | Batch_idx: 150 |  Loss: (0.0520) | Acc: (98.25%) (18989/19328)\n",
            "Epoch: 140 | Batch_idx: 160 |  Loss: (0.0516) | Acc: (98.26%) (20249/20608)\n",
            "Epoch: 140 | Batch_idx: 170 |  Loss: (0.0519) | Acc: (98.23%) (21501/21888)\n",
            "Epoch: 140 | Batch_idx: 180 |  Loss: (0.0520) | Acc: (98.25%) (22762/23168)\n",
            "Epoch: 140 | Batch_idx: 190 |  Loss: (0.0534) | Acc: (98.21%) (24010/24448)\n",
            "Epoch: 140 | Batch_idx: 200 |  Loss: (0.0538) | Acc: (98.20%) (25264/25728)\n",
            "Epoch: 140 | Batch_idx: 210 |  Loss: (0.0540) | Acc: (98.20%) (26521/27008)\n",
            "Epoch: 140 | Batch_idx: 220 |  Loss: (0.0545) | Acc: (98.19%) (27775/28288)\n",
            "Epoch: 140 | Batch_idx: 230 |  Loss: (0.0545) | Acc: (98.19%) (29033/29568)\n",
            "Epoch: 140 | Batch_idx: 240 |  Loss: (0.0548) | Acc: (98.17%) (30285/30848)\n",
            "Epoch: 140 | Batch_idx: 250 |  Loss: (0.0552) | Acc: (98.16%) (31538/32128)\n",
            "Epoch: 140 | Batch_idx: 260 |  Loss: (0.0552) | Acc: (98.14%) (32787/33408)\n",
            "Epoch: 140 | Batch_idx: 270 |  Loss: (0.0555) | Acc: (98.14%) (34042/34688)\n",
            "Epoch: 140 | Batch_idx: 280 |  Loss: (0.0562) | Acc: (98.11%) (35288/35968)\n",
            "Epoch: 140 | Batch_idx: 290 |  Loss: (0.0571) | Acc: (98.07%) (36528/37248)\n",
            "Epoch: 140 | Batch_idx: 300 |  Loss: (0.0575) | Acc: (98.05%) (37776/38528)\n",
            "Epoch: 140 | Batch_idx: 310 |  Loss: (0.0581) | Acc: (98.03%) (39024/39808)\n",
            "Epoch: 140 | Batch_idx: 320 |  Loss: (0.0580) | Acc: (98.03%) (40278/41088)\n",
            "Epoch: 140 | Batch_idx: 330 |  Loss: (0.0584) | Acc: (98.02%) (41527/42368)\n",
            "Epoch: 140 | Batch_idx: 340 |  Loss: (0.0582) | Acc: (98.03%) (42787/43648)\n",
            "Epoch: 140 | Batch_idx: 350 |  Loss: (0.0580) | Acc: (98.04%) (44046/44928)\n",
            "Epoch: 140 | Batch_idx: 360 |  Loss: (0.0583) | Acc: (98.03%) (45297/46208)\n",
            "Epoch: 140 | Batch_idx: 370 |  Loss: (0.0584) | Acc: (98.02%) (46548/47488)\n",
            "Epoch: 140 | Batch_idx: 380 |  Loss: (0.0585) | Acc: (98.02%) (47804/48768)\n",
            "Epoch: 140 | Batch_idx: 390 |  Loss: (0.0585) | Acc: (98.03%) (49016/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3404) | Acc: (91.58%) (9158/10000)\n",
            "Epoch: 141 | Batch_idx: 0 |  Loss: (0.0396) | Acc: (99.22%) (127/128)\n",
            "Epoch: 141 | Batch_idx: 10 |  Loss: (0.0551) | Acc: (98.22%) (1383/1408)\n",
            "Epoch: 141 | Batch_idx: 20 |  Loss: (0.0565) | Acc: (98.25%) (2641/2688)\n",
            "Epoch: 141 | Batch_idx: 30 |  Loss: (0.0576) | Acc: (98.11%) (3893/3968)\n",
            "Epoch: 141 | Batch_idx: 40 |  Loss: (0.0565) | Acc: (98.19%) (5153/5248)\n",
            "Epoch: 141 | Batch_idx: 50 |  Loss: (0.0543) | Acc: (98.27%) (6415/6528)\n",
            "Epoch: 141 | Batch_idx: 60 |  Loss: (0.0544) | Acc: (98.23%) (7670/7808)\n",
            "Epoch: 141 | Batch_idx: 70 |  Loss: (0.0535) | Acc: (98.27%) (8931/9088)\n",
            "Epoch: 141 | Batch_idx: 80 |  Loss: (0.0540) | Acc: (98.21%) (10182/10368)\n",
            "Epoch: 141 | Batch_idx: 90 |  Loss: (0.0551) | Acc: (98.16%) (11434/11648)\n",
            "Epoch: 141 | Batch_idx: 100 |  Loss: (0.0572) | Acc: (98.12%) (12685/12928)\n",
            "Epoch: 141 | Batch_idx: 110 |  Loss: (0.0570) | Acc: (98.13%) (13942/14208)\n",
            "Epoch: 141 | Batch_idx: 120 |  Loss: (0.0577) | Acc: (98.10%) (15193/15488)\n",
            "Epoch: 141 | Batch_idx: 130 |  Loss: (0.0578) | Acc: (98.12%) (16452/16768)\n",
            "Epoch: 141 | Batch_idx: 140 |  Loss: (0.0583) | Acc: (98.09%) (17704/18048)\n",
            "Epoch: 141 | Batch_idx: 150 |  Loss: (0.0583) | Acc: (98.09%) (18959/19328)\n",
            "Epoch: 141 | Batch_idx: 160 |  Loss: (0.0583) | Acc: (98.09%) (20214/20608)\n",
            "Epoch: 141 | Batch_idx: 170 |  Loss: (0.0584) | Acc: (98.08%) (21468/21888)\n",
            "Epoch: 141 | Batch_idx: 180 |  Loss: (0.0589) | Acc: (98.05%) (22717/23168)\n",
            "Epoch: 141 | Batch_idx: 190 |  Loss: (0.0598) | Acc: (98.04%) (23970/24448)\n",
            "Epoch: 141 | Batch_idx: 200 |  Loss: (0.0598) | Acc: (98.04%) (25225/25728)\n",
            "Epoch: 141 | Batch_idx: 210 |  Loss: (0.0592) | Acc: (98.07%) (26487/27008)\n",
            "Epoch: 141 | Batch_idx: 220 |  Loss: (0.0596) | Acc: (98.06%) (27738/28288)\n",
            "Epoch: 141 | Batch_idx: 230 |  Loss: (0.0588) | Acc: (98.08%) (29000/29568)\n",
            "Epoch: 141 | Batch_idx: 240 |  Loss: (0.0585) | Acc: (98.09%) (30258/30848)\n",
            "Epoch: 141 | Batch_idx: 250 |  Loss: (0.0582) | Acc: (98.09%) (31515/32128)\n",
            "Epoch: 141 | Batch_idx: 260 |  Loss: (0.0580) | Acc: (98.10%) (32773/33408)\n",
            "Epoch: 141 | Batch_idx: 270 |  Loss: (0.0581) | Acc: (98.09%) (34025/34688)\n",
            "Epoch: 141 | Batch_idx: 280 |  Loss: (0.0578) | Acc: (98.10%) (35283/35968)\n",
            "Epoch: 141 | Batch_idx: 290 |  Loss: (0.0573) | Acc: (98.12%) (36547/37248)\n",
            "Epoch: 141 | Batch_idx: 300 |  Loss: (0.0571) | Acc: (98.14%) (37811/38528)\n",
            "Epoch: 141 | Batch_idx: 310 |  Loss: (0.0570) | Acc: (98.15%) (39071/39808)\n",
            "Epoch: 141 | Batch_idx: 320 |  Loss: (0.0573) | Acc: (98.13%) (40321/41088)\n",
            "Epoch: 141 | Batch_idx: 330 |  Loss: (0.0577) | Acc: (98.10%) (41562/42368)\n",
            "Epoch: 141 | Batch_idx: 340 |  Loss: (0.0570) | Acc: (98.12%) (42829/43648)\n",
            "Epoch: 141 | Batch_idx: 350 |  Loss: (0.0566) | Acc: (98.13%) (44087/44928)\n",
            "Epoch: 141 | Batch_idx: 360 |  Loss: (0.0566) | Acc: (98.12%) (45339/46208)\n",
            "Epoch: 141 | Batch_idx: 370 |  Loss: (0.0568) | Acc: (98.12%) (46595/47488)\n",
            "Epoch: 141 | Batch_idx: 380 |  Loss: (0.0572) | Acc: (98.10%) (47842/48768)\n",
            "Epoch: 141 | Batch_idx: 390 |  Loss: (0.0571) | Acc: (98.11%) (49056/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3444) | Acc: (91.46%) (9146/10000)\n",
            "Epoch: 142 | Batch_idx: 0 |  Loss: (0.0720) | Acc: (98.44%) (126/128)\n",
            "Epoch: 142 | Batch_idx: 10 |  Loss: (0.0632) | Acc: (97.87%) (1378/1408)\n",
            "Epoch: 142 | Batch_idx: 20 |  Loss: (0.0511) | Acc: (98.33%) (2643/2688)\n",
            "Epoch: 142 | Batch_idx: 30 |  Loss: (0.0565) | Acc: (98.14%) (3894/3968)\n",
            "Epoch: 142 | Batch_idx: 40 |  Loss: (0.0575) | Acc: (98.17%) (5152/5248)\n",
            "Epoch: 142 | Batch_idx: 50 |  Loss: (0.0599) | Acc: (98.13%) (6406/6528)\n",
            "Epoch: 142 | Batch_idx: 60 |  Loss: (0.0605) | Acc: (98.13%) (7662/7808)\n",
            "Epoch: 142 | Batch_idx: 70 |  Loss: (0.0584) | Acc: (98.14%) (8919/9088)\n",
            "Epoch: 142 | Batch_idx: 80 |  Loss: (0.0594) | Acc: (98.04%) (10165/10368)\n",
            "Epoch: 142 | Batch_idx: 90 |  Loss: (0.0619) | Acc: (97.99%) (11414/11648)\n",
            "Epoch: 142 | Batch_idx: 100 |  Loss: (0.0609) | Acc: (98.03%) (12673/12928)\n",
            "Epoch: 142 | Batch_idx: 110 |  Loss: (0.0623) | Acc: (98.02%) (13927/14208)\n",
            "Epoch: 142 | Batch_idx: 120 |  Loss: (0.0612) | Acc: (98.01%) (15180/15488)\n",
            "Epoch: 142 | Batch_idx: 130 |  Loss: (0.0610) | Acc: (98.03%) (16438/16768)\n",
            "Epoch: 142 | Batch_idx: 140 |  Loss: (0.0607) | Acc: (98.05%) (17696/18048)\n",
            "Epoch: 142 | Batch_idx: 150 |  Loss: (0.0605) | Acc: (98.05%) (18951/19328)\n",
            "Epoch: 142 | Batch_idx: 160 |  Loss: (0.0599) | Acc: (98.05%) (20207/20608)\n",
            "Epoch: 142 | Batch_idx: 170 |  Loss: (0.0596) | Acc: (98.04%) (21459/21888)\n",
            "Epoch: 142 | Batch_idx: 180 |  Loss: (0.0590) | Acc: (98.07%) (22721/23168)\n",
            "Epoch: 142 | Batch_idx: 190 |  Loss: (0.0586) | Acc: (98.07%) (23977/24448)\n",
            "Epoch: 142 | Batch_idx: 200 |  Loss: (0.0589) | Acc: (98.07%) (25231/25728)\n",
            "Epoch: 142 | Batch_idx: 210 |  Loss: (0.0583) | Acc: (98.09%) (26492/27008)\n",
            "Epoch: 142 | Batch_idx: 220 |  Loss: (0.0585) | Acc: (98.07%) (27743/28288)\n",
            "Epoch: 142 | Batch_idx: 230 |  Loss: (0.0586) | Acc: (98.09%) (29004/29568)\n",
            "Epoch: 142 | Batch_idx: 240 |  Loss: (0.0593) | Acc: (98.05%) (30245/30848)\n",
            "Epoch: 142 | Batch_idx: 250 |  Loss: (0.0599) | Acc: (98.03%) (31494/32128)\n",
            "Epoch: 142 | Batch_idx: 260 |  Loss: (0.0592) | Acc: (98.04%) (32752/33408)\n",
            "Epoch: 142 | Batch_idx: 270 |  Loss: (0.0585) | Acc: (98.05%) (34013/34688)\n",
            "Epoch: 142 | Batch_idx: 280 |  Loss: (0.0585) | Acc: (98.05%) (35267/35968)\n",
            "Epoch: 142 | Batch_idx: 290 |  Loss: (0.0581) | Acc: (98.06%) (36526/37248)\n",
            "Epoch: 142 | Batch_idx: 300 |  Loss: (0.0579) | Acc: (98.07%) (37784/38528)\n",
            "Epoch: 142 | Batch_idx: 310 |  Loss: (0.0578) | Acc: (98.07%) (39041/39808)\n",
            "Epoch: 142 | Batch_idx: 320 |  Loss: (0.0573) | Acc: (98.09%) (40304/41088)\n",
            "Epoch: 142 | Batch_idx: 330 |  Loss: (0.0573) | Acc: (98.09%) (41558/42368)\n",
            "Epoch: 142 | Batch_idx: 340 |  Loss: (0.0568) | Acc: (98.09%) (42816/43648)\n",
            "Epoch: 142 | Batch_idx: 350 |  Loss: (0.0568) | Acc: (98.08%) (44067/44928)\n",
            "Epoch: 142 | Batch_idx: 360 |  Loss: (0.0570) | Acc: (98.07%) (45316/46208)\n",
            "Epoch: 142 | Batch_idx: 370 |  Loss: (0.0566) | Acc: (98.08%) (46578/47488)\n",
            "Epoch: 142 | Batch_idx: 380 |  Loss: (0.0566) | Acc: (98.08%) (47832/48768)\n",
            "Epoch: 142 | Batch_idx: 390 |  Loss: (0.0568) | Acc: (98.07%) (49037/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3378) | Acc: (91.54%) (9154/10000)\n",
            "Epoch: 143 | Batch_idx: 0 |  Loss: (0.0569) | Acc: (97.66%) (125/128)\n",
            "Epoch: 143 | Batch_idx: 10 |  Loss: (0.0681) | Acc: (97.44%) (1372/1408)\n",
            "Epoch: 143 | Batch_idx: 20 |  Loss: (0.0634) | Acc: (97.88%) (2631/2688)\n",
            "Epoch: 143 | Batch_idx: 30 |  Loss: (0.0581) | Acc: (98.11%) (3893/3968)\n",
            "Epoch: 143 | Batch_idx: 40 |  Loss: (0.0563) | Acc: (98.15%) (5151/5248)\n",
            "Epoch: 143 | Batch_idx: 50 |  Loss: (0.0578) | Acc: (98.05%) (6401/6528)\n",
            "Epoch: 143 | Batch_idx: 60 |  Loss: (0.0553) | Acc: (98.14%) (7663/7808)\n",
            "Epoch: 143 | Batch_idx: 70 |  Loss: (0.0574) | Acc: (98.07%) (8913/9088)\n",
            "Epoch: 143 | Batch_idx: 80 |  Loss: (0.0569) | Acc: (98.07%) (10168/10368)\n",
            "Epoch: 143 | Batch_idx: 90 |  Loss: (0.0565) | Acc: (98.08%) (11424/11648)\n",
            "Epoch: 143 | Batch_idx: 100 |  Loss: (0.0584) | Acc: (98.04%) (12674/12928)\n",
            "Epoch: 143 | Batch_idx: 110 |  Loss: (0.0570) | Acc: (98.10%) (13938/14208)\n",
            "Epoch: 143 | Batch_idx: 120 |  Loss: (0.0564) | Acc: (98.10%) (15194/15488)\n",
            "Epoch: 143 | Batch_idx: 130 |  Loss: (0.0579) | Acc: (98.06%) (16443/16768)\n",
            "Epoch: 143 | Batch_idx: 140 |  Loss: (0.0583) | Acc: (98.02%) (17691/18048)\n",
            "Epoch: 143 | Batch_idx: 150 |  Loss: (0.0585) | Acc: (98.01%) (18943/19328)\n",
            "Epoch: 143 | Batch_idx: 160 |  Loss: (0.0586) | Acc: (98.00%) (20195/20608)\n",
            "Epoch: 143 | Batch_idx: 170 |  Loss: (0.0602) | Acc: (97.97%) (21443/21888)\n",
            "Epoch: 143 | Batch_idx: 180 |  Loss: (0.0597) | Acc: (97.98%) (22699/23168)\n",
            "Epoch: 143 | Batch_idx: 190 |  Loss: (0.0593) | Acc: (98.00%) (23958/24448)\n",
            "Epoch: 143 | Batch_idx: 200 |  Loss: (0.0589) | Acc: (98.00%) (25213/25728)\n",
            "Epoch: 143 | Batch_idx: 210 |  Loss: (0.0589) | Acc: (97.99%) (26466/27008)\n",
            "Epoch: 143 | Batch_idx: 220 |  Loss: (0.0584) | Acc: (98.02%) (27728/28288)\n",
            "Epoch: 143 | Batch_idx: 230 |  Loss: (0.0588) | Acc: (98.01%) (28979/29568)\n",
            "Epoch: 143 | Batch_idx: 240 |  Loss: (0.0591) | Acc: (98.00%) (30230/30848)\n",
            "Epoch: 143 | Batch_idx: 250 |  Loss: (0.0590) | Acc: (97.99%) (31482/32128)\n",
            "Epoch: 143 | Batch_idx: 260 |  Loss: (0.0588) | Acc: (97.99%) (32738/33408)\n",
            "Epoch: 143 | Batch_idx: 270 |  Loss: (0.0582) | Acc: (98.01%) (33998/34688)\n",
            "Epoch: 143 | Batch_idx: 280 |  Loss: (0.0577) | Acc: (98.02%) (35256/35968)\n",
            "Epoch: 143 | Batch_idx: 290 |  Loss: (0.0578) | Acc: (98.00%) (36504/37248)\n",
            "Epoch: 143 | Batch_idx: 300 |  Loss: (0.0576) | Acc: (98.00%) (37759/38528)\n",
            "Epoch: 143 | Batch_idx: 310 |  Loss: (0.0573) | Acc: (98.01%) (39014/39808)\n",
            "Epoch: 143 | Batch_idx: 320 |  Loss: (0.0571) | Acc: (98.01%) (40270/41088)\n",
            "Epoch: 143 | Batch_idx: 330 |  Loss: (0.0576) | Acc: (97.99%) (41518/42368)\n",
            "Epoch: 143 | Batch_idx: 340 |  Loss: (0.0574) | Acc: (98.00%) (42776/43648)\n",
            "Epoch: 143 | Batch_idx: 350 |  Loss: (0.0571) | Acc: (98.01%) (44036/44928)\n",
            "Epoch: 143 | Batch_idx: 360 |  Loss: (0.0567) | Acc: (98.04%) (45302/46208)\n",
            "Epoch: 143 | Batch_idx: 370 |  Loss: (0.0566) | Acc: (98.05%) (46564/47488)\n",
            "Epoch: 143 | Batch_idx: 380 |  Loss: (0.0562) | Acc: (98.07%) (47829/48768)\n",
            "Epoch: 143 | Batch_idx: 390 |  Loss: (0.0564) | Acc: (98.07%) (49037/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3405) | Acc: (91.50%) (9150/10000)\n",
            "Epoch: 144 | Batch_idx: 0 |  Loss: (0.0466) | Acc: (98.44%) (126/128)\n",
            "Epoch: 144 | Batch_idx: 10 |  Loss: (0.0519) | Acc: (98.37%) (1385/1408)\n",
            "Epoch: 144 | Batch_idx: 20 |  Loss: (0.0622) | Acc: (98.14%) (2638/2688)\n",
            "Epoch: 144 | Batch_idx: 30 |  Loss: (0.0662) | Acc: (97.93%) (3886/3968)\n",
            "Epoch: 144 | Batch_idx: 40 |  Loss: (0.0665) | Acc: (97.88%) (5137/5248)\n",
            "Epoch: 144 | Batch_idx: 50 |  Loss: (0.0598) | Acc: (98.05%) (6401/6528)\n",
            "Epoch: 144 | Batch_idx: 60 |  Loss: (0.0588) | Acc: (98.03%) (7654/7808)\n",
            "Epoch: 144 | Batch_idx: 70 |  Loss: (0.0589) | Acc: (98.06%) (8912/9088)\n",
            "Epoch: 144 | Batch_idx: 80 |  Loss: (0.0603) | Acc: (98.01%) (10162/10368)\n",
            "Epoch: 144 | Batch_idx: 90 |  Loss: (0.0603) | Acc: (98.03%) (11418/11648)\n",
            "Epoch: 144 | Batch_idx: 100 |  Loss: (0.0620) | Acc: (97.98%) (12667/12928)\n",
            "Epoch: 144 | Batch_idx: 110 |  Loss: (0.0630) | Acc: (97.94%) (13916/14208)\n",
            "Epoch: 144 | Batch_idx: 120 |  Loss: (0.0627) | Acc: (97.95%) (15171/15488)\n",
            "Epoch: 144 | Batch_idx: 130 |  Loss: (0.0615) | Acc: (98.00%) (16432/16768)\n",
            "Epoch: 144 | Batch_idx: 140 |  Loss: (0.0609) | Acc: (98.01%) (17688/18048)\n",
            "Epoch: 144 | Batch_idx: 150 |  Loss: (0.0609) | Acc: (98.00%) (18941/19328)\n",
            "Epoch: 144 | Batch_idx: 160 |  Loss: (0.0607) | Acc: (98.00%) (20195/20608)\n",
            "Epoch: 144 | Batch_idx: 170 |  Loss: (0.0607) | Acc: (97.99%) (21447/21888)\n",
            "Epoch: 144 | Batch_idx: 180 |  Loss: (0.0599) | Acc: (98.02%) (22709/23168)\n",
            "Epoch: 144 | Batch_idx: 190 |  Loss: (0.0594) | Acc: (98.04%) (23968/24448)\n",
            "Epoch: 144 | Batch_idx: 200 |  Loss: (0.0587) | Acc: (98.06%) (25228/25728)\n",
            "Epoch: 144 | Batch_idx: 210 |  Loss: (0.0579) | Acc: (98.09%) (26493/27008)\n",
            "Epoch: 144 | Batch_idx: 220 |  Loss: (0.0575) | Acc: (98.11%) (27752/28288)\n",
            "Epoch: 144 | Batch_idx: 230 |  Loss: (0.0575) | Acc: (98.10%) (29007/29568)\n",
            "Epoch: 144 | Batch_idx: 240 |  Loss: (0.0575) | Acc: (98.09%) (30258/30848)\n",
            "Epoch: 144 | Batch_idx: 250 |  Loss: (0.0571) | Acc: (98.10%) (31519/32128)\n",
            "Epoch: 144 | Batch_idx: 260 |  Loss: (0.0565) | Acc: (98.12%) (32781/33408)\n",
            "Epoch: 144 | Batch_idx: 270 |  Loss: (0.0565) | Acc: (98.11%) (34033/34688)\n",
            "Epoch: 144 | Batch_idx: 280 |  Loss: (0.0572) | Acc: (98.06%) (35272/35968)\n",
            "Epoch: 144 | Batch_idx: 290 |  Loss: (0.0571) | Acc: (98.06%) (36527/37248)\n",
            "Epoch: 144 | Batch_idx: 300 |  Loss: (0.0574) | Acc: (98.06%) (37782/38528)\n",
            "Epoch: 144 | Batch_idx: 310 |  Loss: (0.0575) | Acc: (98.07%) (39040/39808)\n",
            "Epoch: 144 | Batch_idx: 320 |  Loss: (0.0577) | Acc: (98.06%) (40291/41088)\n",
            "Epoch: 144 | Batch_idx: 330 |  Loss: (0.0577) | Acc: (98.05%) (41543/42368)\n",
            "Epoch: 144 | Batch_idx: 340 |  Loss: (0.0577) | Acc: (98.05%) (42798/43648)\n",
            "Epoch: 144 | Batch_idx: 350 |  Loss: (0.0575) | Acc: (98.06%) (44058/44928)\n",
            "Epoch: 144 | Batch_idx: 360 |  Loss: (0.0573) | Acc: (98.08%) (45319/46208)\n",
            "Epoch: 144 | Batch_idx: 370 |  Loss: (0.0576) | Acc: (98.07%) (46571/47488)\n",
            "Epoch: 144 | Batch_idx: 380 |  Loss: (0.0579) | Acc: (98.06%) (47821/48768)\n",
            "Epoch: 144 | Batch_idx: 390 |  Loss: (0.0577) | Acc: (98.06%) (49032/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3397) | Acc: (91.52%) (9152/10000)\n",
            "Epoch: 145 | Batch_idx: 0 |  Loss: (0.0361) | Acc: (98.44%) (126/128)\n",
            "Epoch: 145 | Batch_idx: 10 |  Loss: (0.0548) | Acc: (98.15%) (1382/1408)\n",
            "Epoch: 145 | Batch_idx: 20 |  Loss: (0.0564) | Acc: (98.07%) (2636/2688)\n",
            "Epoch: 145 | Batch_idx: 30 |  Loss: (0.0599) | Acc: (97.88%) (3884/3968)\n",
            "Epoch: 145 | Batch_idx: 40 |  Loss: (0.0622) | Acc: (97.85%) (5135/5248)\n",
            "Epoch: 145 | Batch_idx: 50 |  Loss: (0.0590) | Acc: (97.98%) (6396/6528)\n",
            "Epoch: 145 | Batch_idx: 60 |  Loss: (0.0610) | Acc: (97.95%) (7648/7808)\n",
            "Epoch: 145 | Batch_idx: 70 |  Loss: (0.0612) | Acc: (97.93%) (8900/9088)\n",
            "Epoch: 145 | Batch_idx: 80 |  Loss: (0.0601) | Acc: (97.98%) (10159/10368)\n",
            "Epoch: 145 | Batch_idx: 90 |  Loss: (0.0586) | Acc: (98.03%) (11418/11648)\n",
            "Epoch: 145 | Batch_idx: 100 |  Loss: (0.0595) | Acc: (98.04%) (12674/12928)\n",
            "Epoch: 145 | Batch_idx: 110 |  Loss: (0.0612) | Acc: (97.99%) (13922/14208)\n",
            "Epoch: 145 | Batch_idx: 120 |  Loss: (0.0603) | Acc: (98.01%) (15180/15488)\n",
            "Epoch: 145 | Batch_idx: 130 |  Loss: (0.0602) | Acc: (98.01%) (16435/16768)\n",
            "Epoch: 145 | Batch_idx: 140 |  Loss: (0.0593) | Acc: (98.05%) (17696/18048)\n",
            "Epoch: 145 | Batch_idx: 150 |  Loss: (0.0595) | Acc: (98.02%) (18945/19328)\n",
            "Epoch: 145 | Batch_idx: 160 |  Loss: (0.0593) | Acc: (98.05%) (20206/20608)\n",
            "Epoch: 145 | Batch_idx: 170 |  Loss: (0.0593) | Acc: (98.05%) (21462/21888)\n",
            "Epoch: 145 | Batch_idx: 180 |  Loss: (0.0580) | Acc: (98.09%) (22725/23168)\n",
            "Epoch: 145 | Batch_idx: 190 |  Loss: (0.0581) | Acc: (98.09%) (23982/24448)\n",
            "Epoch: 145 | Batch_idx: 200 |  Loss: (0.0576) | Acc: (98.10%) (25240/25728)\n",
            "Epoch: 145 | Batch_idx: 210 |  Loss: (0.0571) | Acc: (98.11%) (26498/27008)\n",
            "Epoch: 145 | Batch_idx: 220 |  Loss: (0.0568) | Acc: (98.11%) (27752/28288)\n",
            "Epoch: 145 | Batch_idx: 230 |  Loss: (0.0565) | Acc: (98.12%) (29011/29568)\n",
            "Epoch: 145 | Batch_idx: 240 |  Loss: (0.0572) | Acc: (98.12%) (30267/30848)\n",
            "Epoch: 145 | Batch_idx: 250 |  Loss: (0.0566) | Acc: (98.14%) (31530/32128)\n",
            "Epoch: 145 | Batch_idx: 260 |  Loss: (0.0564) | Acc: (98.14%) (32785/33408)\n",
            "Epoch: 145 | Batch_idx: 270 |  Loss: (0.0563) | Acc: (98.13%) (34039/34688)\n",
            "Epoch: 145 | Batch_idx: 280 |  Loss: (0.0561) | Acc: (98.13%) (35294/35968)\n",
            "Epoch: 145 | Batch_idx: 290 |  Loss: (0.0560) | Acc: (98.12%) (36547/37248)\n",
            "Epoch: 145 | Batch_idx: 300 |  Loss: (0.0562) | Acc: (98.11%) (37801/38528)\n",
            "Epoch: 145 | Batch_idx: 310 |  Loss: (0.0561) | Acc: (98.13%) (39062/39808)\n",
            "Epoch: 145 | Batch_idx: 320 |  Loss: (0.0560) | Acc: (98.13%) (40321/41088)\n",
            "Epoch: 145 | Batch_idx: 330 |  Loss: (0.0559) | Acc: (98.14%) (41580/42368)\n",
            "Epoch: 145 | Batch_idx: 340 |  Loss: (0.0563) | Acc: (98.12%) (42828/43648)\n",
            "Epoch: 145 | Batch_idx: 350 |  Loss: (0.0563) | Acc: (98.12%) (44084/44928)\n",
            "Epoch: 145 | Batch_idx: 360 |  Loss: (0.0560) | Acc: (98.12%) (45341/46208)\n",
            "Epoch: 145 | Batch_idx: 370 |  Loss: (0.0560) | Acc: (98.11%) (46592/47488)\n",
            "Epoch: 145 | Batch_idx: 380 |  Loss: (0.0559) | Acc: (98.12%) (47850/48768)\n",
            "Epoch: 145 | Batch_idx: 390 |  Loss: (0.0560) | Acc: (98.12%) (49058/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3380) | Acc: (91.58%) (9158/10000)\n",
            "Epoch: 146 | Batch_idx: 0 |  Loss: (0.0890) | Acc: (96.88%) (124/128)\n",
            "Epoch: 146 | Batch_idx: 10 |  Loss: (0.0577) | Acc: (97.80%) (1377/1408)\n",
            "Epoch: 146 | Batch_idx: 20 |  Loss: (0.0567) | Acc: (97.81%) (2629/2688)\n",
            "Epoch: 146 | Batch_idx: 30 |  Loss: (0.0588) | Acc: (97.83%) (3882/3968)\n",
            "Epoch: 146 | Batch_idx: 40 |  Loss: (0.0602) | Acc: (97.77%) (5131/5248)\n",
            "Epoch: 146 | Batch_idx: 50 |  Loss: (0.0638) | Acc: (97.64%) (6374/6528)\n",
            "Epoch: 146 | Batch_idx: 60 |  Loss: (0.0646) | Acc: (97.59%) (7620/7808)\n",
            "Epoch: 146 | Batch_idx: 70 |  Loss: (0.0622) | Acc: (97.72%) (8881/9088)\n",
            "Epoch: 146 | Batch_idx: 80 |  Loss: (0.0609) | Acc: (97.76%) (10136/10368)\n",
            "Epoch: 146 | Batch_idx: 90 |  Loss: (0.0609) | Acc: (97.79%) (11391/11648)\n",
            "Epoch: 146 | Batch_idx: 100 |  Loss: (0.0588) | Acc: (97.90%) (12656/12928)\n",
            "Epoch: 146 | Batch_idx: 110 |  Loss: (0.0601) | Acc: (97.85%) (13903/14208)\n",
            "Epoch: 146 | Batch_idx: 120 |  Loss: (0.0597) | Acc: (97.86%) (15156/15488)\n",
            "Epoch: 146 | Batch_idx: 130 |  Loss: (0.0590) | Acc: (97.89%) (16415/16768)\n",
            "Epoch: 146 | Batch_idx: 140 |  Loss: (0.0603) | Acc: (97.89%) (17667/18048)\n",
            "Epoch: 146 | Batch_idx: 150 |  Loss: (0.0597) | Acc: (97.90%) (18923/19328)\n",
            "Epoch: 146 | Batch_idx: 160 |  Loss: (0.0601) | Acc: (97.90%) (20175/20608)\n",
            "Epoch: 146 | Batch_idx: 170 |  Loss: (0.0598) | Acc: (97.91%) (21431/21888)\n",
            "Epoch: 146 | Batch_idx: 180 |  Loss: (0.0601) | Acc: (97.92%) (22686/23168)\n",
            "Epoch: 146 | Batch_idx: 190 |  Loss: (0.0610) | Acc: (97.93%) (23941/24448)\n",
            "Epoch: 146 | Batch_idx: 200 |  Loss: (0.0618) | Acc: (97.90%) (25188/25728)\n",
            "Epoch: 146 | Batch_idx: 210 |  Loss: (0.0627) | Acc: (97.87%) (26433/27008)\n",
            "Epoch: 146 | Batch_idx: 220 |  Loss: (0.0622) | Acc: (97.87%) (27686/28288)\n",
            "Epoch: 146 | Batch_idx: 230 |  Loss: (0.0622) | Acc: (97.88%) (28941/29568)\n",
            "Epoch: 146 | Batch_idx: 240 |  Loss: (0.0611) | Acc: (97.91%) (30204/30848)\n",
            "Epoch: 146 | Batch_idx: 250 |  Loss: (0.0607) | Acc: (97.93%) (31462/32128)\n",
            "Epoch: 146 | Batch_idx: 260 |  Loss: (0.0607) | Acc: (97.91%) (32711/33408)\n",
            "Epoch: 146 | Batch_idx: 270 |  Loss: (0.0609) | Acc: (97.93%) (33970/34688)\n",
            "Epoch: 146 | Batch_idx: 280 |  Loss: (0.0608) | Acc: (97.93%) (35223/35968)\n",
            "Epoch: 146 | Batch_idx: 290 |  Loss: (0.0612) | Acc: (97.93%) (36477/37248)\n",
            "Epoch: 146 | Batch_idx: 300 |  Loss: (0.0607) | Acc: (97.94%) (37733/38528)\n",
            "Epoch: 146 | Batch_idx: 310 |  Loss: (0.0604) | Acc: (97.94%) (38989/39808)\n",
            "Epoch: 146 | Batch_idx: 320 |  Loss: (0.0599) | Acc: (97.96%) (40251/41088)\n",
            "Epoch: 146 | Batch_idx: 330 |  Loss: (0.0598) | Acc: (97.97%) (41508/42368)\n",
            "Epoch: 146 | Batch_idx: 340 |  Loss: (0.0596) | Acc: (97.98%) (42768/43648)\n",
            "Epoch: 146 | Batch_idx: 350 |  Loss: (0.0594) | Acc: (98.00%) (44029/44928)\n",
            "Epoch: 146 | Batch_idx: 360 |  Loss: (0.0593) | Acc: (97.99%) (45280/46208)\n",
            "Epoch: 146 | Batch_idx: 370 |  Loss: (0.0591) | Acc: (98.01%) (46542/47488)\n",
            "Epoch: 146 | Batch_idx: 380 |  Loss: (0.0589) | Acc: (98.02%) (47801/48768)\n",
            "Epoch: 146 | Batch_idx: 390 |  Loss: (0.0587) | Acc: (98.02%) (49009/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3381) | Acc: (91.57%) (9157/10000)\n",
            "Epoch: 147 | Batch_idx: 0 |  Loss: (0.0731) | Acc: (96.88%) (124/128)\n",
            "Epoch: 147 | Batch_idx: 10 |  Loss: (0.0534) | Acc: (97.94%) (1379/1408)\n",
            "Epoch: 147 | Batch_idx: 20 |  Loss: (0.0600) | Acc: (97.95%) (2633/2688)\n",
            "Epoch: 147 | Batch_idx: 30 |  Loss: (0.0559) | Acc: (98.06%) (3891/3968)\n",
            "Epoch: 147 | Batch_idx: 40 |  Loss: (0.0543) | Acc: (98.13%) (5150/5248)\n",
            "Epoch: 147 | Batch_idx: 50 |  Loss: (0.0522) | Acc: (98.24%) (6413/6528)\n",
            "Epoch: 147 | Batch_idx: 60 |  Loss: (0.0527) | Acc: (98.16%) (7664/7808)\n",
            "Epoch: 147 | Batch_idx: 70 |  Loss: (0.0515) | Acc: (98.22%) (8926/9088)\n",
            "Epoch: 147 | Batch_idx: 80 |  Loss: (0.0538) | Acc: (98.19%) (10180/10368)\n",
            "Epoch: 147 | Batch_idx: 90 |  Loss: (0.0530) | Acc: (98.24%) (11443/11648)\n",
            "Epoch: 147 | Batch_idx: 100 |  Loss: (0.0519) | Acc: (98.24%) (12701/12928)\n",
            "Epoch: 147 | Batch_idx: 110 |  Loss: (0.0523) | Acc: (98.24%) (13958/14208)\n",
            "Epoch: 147 | Batch_idx: 120 |  Loss: (0.0515) | Acc: (98.26%) (15218/15488)\n",
            "Epoch: 147 | Batch_idx: 130 |  Loss: (0.0525) | Acc: (98.22%) (16470/16768)\n",
            "Epoch: 147 | Batch_idx: 140 |  Loss: (0.0538) | Acc: (98.19%) (17722/18048)\n",
            "Epoch: 147 | Batch_idx: 150 |  Loss: (0.0539) | Acc: (98.19%) (18979/19328)\n",
            "Epoch: 147 | Batch_idx: 160 |  Loss: (0.0542) | Acc: (98.18%) (20233/20608)\n",
            "Epoch: 147 | Batch_idx: 170 |  Loss: (0.0542) | Acc: (98.17%) (21488/21888)\n",
            "Epoch: 147 | Batch_idx: 180 |  Loss: (0.0551) | Acc: (98.16%) (22742/23168)\n",
            "Epoch: 147 | Batch_idx: 190 |  Loss: (0.0546) | Acc: (98.17%) (24001/24448)\n",
            "Epoch: 147 | Batch_idx: 200 |  Loss: (0.0548) | Acc: (98.16%) (25255/25728)\n",
            "Epoch: 147 | Batch_idx: 210 |  Loss: (0.0548) | Acc: (98.16%) (26512/27008)\n",
            "Epoch: 147 | Batch_idx: 220 |  Loss: (0.0553) | Acc: (98.14%) (27763/28288)\n",
            "Epoch: 147 | Batch_idx: 230 |  Loss: (0.0559) | Acc: (98.13%) (29016/29568)\n",
            "Epoch: 147 | Batch_idx: 240 |  Loss: (0.0563) | Acc: (98.10%) (30263/30848)\n",
            "Epoch: 147 | Batch_idx: 250 |  Loss: (0.0561) | Acc: (98.12%) (31523/32128)\n",
            "Epoch: 147 | Batch_idx: 260 |  Loss: (0.0558) | Acc: (98.14%) (32785/33408)\n",
            "Epoch: 147 | Batch_idx: 270 |  Loss: (0.0554) | Acc: (98.15%) (34047/34688)\n",
            "Epoch: 147 | Batch_idx: 280 |  Loss: (0.0550) | Acc: (98.18%) (35313/35968)\n",
            "Epoch: 147 | Batch_idx: 290 |  Loss: (0.0552) | Acc: (98.18%) (36570/37248)\n",
            "Epoch: 147 | Batch_idx: 300 |  Loss: (0.0551) | Acc: (98.18%) (37825/38528)\n",
            "Epoch: 147 | Batch_idx: 310 |  Loss: (0.0546) | Acc: (98.20%) (39091/39808)\n",
            "Epoch: 147 | Batch_idx: 320 |  Loss: (0.0549) | Acc: (98.18%) (40341/41088)\n",
            "Epoch: 147 | Batch_idx: 330 |  Loss: (0.0550) | Acc: (98.18%) (41597/42368)\n",
            "Epoch: 147 | Batch_idx: 340 |  Loss: (0.0549) | Acc: (98.19%) (42858/43648)\n",
            "Epoch: 147 | Batch_idx: 350 |  Loss: (0.0549) | Acc: (98.20%) (44118/44928)\n",
            "Epoch: 147 | Batch_idx: 360 |  Loss: (0.0553) | Acc: (98.18%) (45366/46208)\n",
            "Epoch: 147 | Batch_idx: 370 |  Loss: (0.0551) | Acc: (98.18%) (46622/47488)\n",
            "Epoch: 147 | Batch_idx: 380 |  Loss: (0.0549) | Acc: (98.18%) (47880/48768)\n",
            "Epoch: 147 | Batch_idx: 390 |  Loss: (0.0549) | Acc: (98.18%) (49092/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3404) | Acc: (91.54%) (9154/10000)\n",
            "Epoch: 148 | Batch_idx: 0 |  Loss: (0.0262) | Acc: (98.44%) (126/128)\n",
            "Epoch: 148 | Batch_idx: 10 |  Loss: (0.0551) | Acc: (98.15%) (1382/1408)\n",
            "Epoch: 148 | Batch_idx: 20 |  Loss: (0.0598) | Acc: (98.10%) (2637/2688)\n",
            "Epoch: 148 | Batch_idx: 30 |  Loss: (0.0663) | Acc: (97.86%) (3883/3968)\n",
            "Epoch: 148 | Batch_idx: 40 |  Loss: (0.0624) | Acc: (97.94%) (5140/5248)\n",
            "Epoch: 148 | Batch_idx: 50 |  Loss: (0.0619) | Acc: (97.93%) (6393/6528)\n",
            "Epoch: 148 | Batch_idx: 60 |  Loss: (0.0627) | Acc: (97.90%) (7644/7808)\n",
            "Epoch: 148 | Batch_idx: 70 |  Loss: (0.0610) | Acc: (97.95%) (8902/9088)\n",
            "Epoch: 148 | Batch_idx: 80 |  Loss: (0.0589) | Acc: (98.01%) (10162/10368)\n",
            "Epoch: 148 | Batch_idx: 90 |  Loss: (0.0590) | Acc: (98.03%) (11418/11648)\n",
            "Epoch: 148 | Batch_idx: 100 |  Loss: (0.0599) | Acc: (98.00%) (12669/12928)\n",
            "Epoch: 148 | Batch_idx: 110 |  Loss: (0.0590) | Acc: (98.01%) (13925/14208)\n",
            "Epoch: 148 | Batch_idx: 120 |  Loss: (0.0581) | Acc: (98.03%) (15183/15488)\n",
            "Epoch: 148 | Batch_idx: 130 |  Loss: (0.0567) | Acc: (98.07%) (16444/16768)\n",
            "Epoch: 148 | Batch_idx: 140 |  Loss: (0.0562) | Acc: (98.07%) (17699/18048)\n",
            "Epoch: 148 | Batch_idx: 150 |  Loss: (0.0568) | Acc: (98.08%) (18956/19328)\n",
            "Epoch: 148 | Batch_idx: 160 |  Loss: (0.0575) | Acc: (98.09%) (20214/20608)\n",
            "Epoch: 148 | Batch_idx: 170 |  Loss: (0.0565) | Acc: (98.13%) (21479/21888)\n",
            "Epoch: 148 | Batch_idx: 180 |  Loss: (0.0564) | Acc: (98.13%) (22735/23168)\n",
            "Epoch: 148 | Batch_idx: 190 |  Loss: (0.0559) | Acc: (98.15%) (23996/24448)\n",
            "Epoch: 148 | Batch_idx: 200 |  Loss: (0.0561) | Acc: (98.15%) (25252/25728)\n",
            "Epoch: 148 | Batch_idx: 210 |  Loss: (0.0561) | Acc: (98.13%) (26504/27008)\n",
            "Epoch: 148 | Batch_idx: 220 |  Loss: (0.0560) | Acc: (98.13%) (27760/28288)\n",
            "Epoch: 148 | Batch_idx: 230 |  Loss: (0.0559) | Acc: (98.12%) (29013/29568)\n",
            "Epoch: 148 | Batch_idx: 240 |  Loss: (0.0552) | Acc: (98.16%) (30279/30848)\n",
            "Epoch: 148 | Batch_idx: 250 |  Loss: (0.0562) | Acc: (98.14%) (31531/32128)\n",
            "Epoch: 148 | Batch_idx: 260 |  Loss: (0.0556) | Acc: (98.16%) (32792/33408)\n",
            "Epoch: 148 | Batch_idx: 270 |  Loss: (0.0556) | Acc: (98.14%) (34044/34688)\n",
            "Epoch: 148 | Batch_idx: 280 |  Loss: (0.0553) | Acc: (98.15%) (35304/35968)\n",
            "Epoch: 148 | Batch_idx: 290 |  Loss: (0.0550) | Acc: (98.16%) (36564/37248)\n",
            "Epoch: 148 | Batch_idx: 300 |  Loss: (0.0551) | Acc: (98.16%) (37819/38528)\n",
            "Epoch: 148 | Batch_idx: 310 |  Loss: (0.0550) | Acc: (98.15%) (39073/39808)\n",
            "Epoch: 148 | Batch_idx: 320 |  Loss: (0.0550) | Acc: (98.16%) (40331/41088)\n",
            "Epoch: 148 | Batch_idx: 330 |  Loss: (0.0549) | Acc: (98.15%) (41586/42368)\n",
            "Epoch: 148 | Batch_idx: 340 |  Loss: (0.0549) | Acc: (98.15%) (42839/43648)\n",
            "Epoch: 148 | Batch_idx: 350 |  Loss: (0.0550) | Acc: (98.15%) (44096/44928)\n",
            "Epoch: 148 | Batch_idx: 360 |  Loss: (0.0546) | Acc: (98.15%) (45353/46208)\n",
            "Epoch: 148 | Batch_idx: 370 |  Loss: (0.0546) | Acc: (98.15%) (46608/47488)\n",
            "Epoch: 148 | Batch_idx: 380 |  Loss: (0.0543) | Acc: (98.15%) (47868/48768)\n",
            "Epoch: 148 | Batch_idx: 390 |  Loss: (0.0538) | Acc: (98.18%) (49089/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3412) | Acc: (91.49%) (9149/10000)\n",
            "Epoch: 149 | Batch_idx: 0 |  Loss: (0.0184) | Acc: (100.00%) (128/128)\n",
            "Epoch: 149 | Batch_idx: 10 |  Loss: (0.0417) | Acc: (98.30%) (1384/1408)\n",
            "Epoch: 149 | Batch_idx: 20 |  Loss: (0.0485) | Acc: (98.18%) (2639/2688)\n",
            "Epoch: 149 | Batch_idx: 30 |  Loss: (0.0549) | Acc: (97.91%) (3885/3968)\n",
            "Epoch: 149 | Batch_idx: 40 |  Loss: (0.0529) | Acc: (98.09%) (5148/5248)\n",
            "Epoch: 149 | Batch_idx: 50 |  Loss: (0.0580) | Acc: (97.93%) (6393/6528)\n",
            "Epoch: 149 | Batch_idx: 60 |  Loss: (0.0584) | Acc: (97.93%) (7646/7808)\n",
            "Epoch: 149 | Batch_idx: 70 |  Loss: (0.0560) | Acc: (98.04%) (8910/9088)\n",
            "Epoch: 149 | Batch_idx: 80 |  Loss: (0.0553) | Acc: (98.09%) (10170/10368)\n",
            "Epoch: 149 | Batch_idx: 90 |  Loss: (0.0562) | Acc: (98.05%) (11421/11648)\n",
            "Epoch: 149 | Batch_idx: 100 |  Loss: (0.0569) | Acc: (98.04%) (12675/12928)\n",
            "Epoch: 149 | Batch_idx: 110 |  Loss: (0.0572) | Acc: (98.01%) (13925/14208)\n",
            "Epoch: 149 | Batch_idx: 120 |  Loss: (0.0565) | Acc: (97.99%) (15177/15488)\n",
            "Epoch: 149 | Batch_idx: 130 |  Loss: (0.0568) | Acc: (98.01%) (16435/16768)\n",
            "Epoch: 149 | Batch_idx: 140 |  Loss: (0.0564) | Acc: (98.02%) (17691/18048)\n",
            "Epoch: 149 | Batch_idx: 150 |  Loss: (0.0567) | Acc: (98.03%) (18947/19328)\n",
            "Epoch: 149 | Batch_idx: 160 |  Loss: (0.0559) | Acc: (98.07%) (20210/20608)\n",
            "Epoch: 149 | Batch_idx: 170 |  Loss: (0.0550) | Acc: (98.11%) (21475/21888)\n",
            "Epoch: 149 | Batch_idx: 180 |  Loss: (0.0542) | Acc: (98.14%) (22736/23168)\n",
            "Epoch: 149 | Batch_idx: 190 |  Loss: (0.0553) | Acc: (98.11%) (23985/24448)\n",
            "Epoch: 149 | Batch_idx: 200 |  Loss: (0.0558) | Acc: (98.10%) (25240/25728)\n",
            "Epoch: 149 | Batch_idx: 210 |  Loss: (0.0562) | Acc: (98.12%) (26501/27008)\n",
            "Epoch: 149 | Batch_idx: 220 |  Loss: (0.0558) | Acc: (98.13%) (27760/28288)\n",
            "Epoch: 149 | Batch_idx: 230 |  Loss: (0.0569) | Acc: (98.09%) (29003/29568)\n",
            "Epoch: 149 | Batch_idx: 240 |  Loss: (0.0567) | Acc: (98.10%) (30262/30848)\n",
            "Epoch: 149 | Batch_idx: 250 |  Loss: (0.0563) | Acc: (98.10%) (31519/32128)\n",
            "Epoch: 149 | Batch_idx: 260 |  Loss: (0.0564) | Acc: (98.11%) (32775/33408)\n",
            "Epoch: 149 | Batch_idx: 270 |  Loss: (0.0569) | Acc: (98.11%) (34031/34688)\n",
            "Epoch: 149 | Batch_idx: 280 |  Loss: (0.0573) | Acc: (98.10%) (35286/35968)\n",
            "Epoch: 149 | Batch_idx: 290 |  Loss: (0.0566) | Acc: (98.13%) (36550/37248)\n",
            "Epoch: 149 | Batch_idx: 300 |  Loss: (0.0570) | Acc: (98.12%) (37805/38528)\n",
            "Epoch: 149 | Batch_idx: 310 |  Loss: (0.0572) | Acc: (98.11%) (39055/39808)\n",
            "Epoch: 149 | Batch_idx: 320 |  Loss: (0.0583) | Acc: (98.07%) (40296/41088)\n",
            "Epoch: 149 | Batch_idx: 330 |  Loss: (0.0581) | Acc: (98.07%) (41552/42368)\n",
            "Epoch: 149 | Batch_idx: 340 |  Loss: (0.0578) | Acc: (98.08%) (42812/43648)\n",
            "Epoch: 149 | Batch_idx: 350 |  Loss: (0.0579) | Acc: (98.07%) (44063/44928)\n",
            "Epoch: 149 | Batch_idx: 360 |  Loss: (0.0579) | Acc: (98.07%) (45315/46208)\n",
            "Epoch: 149 | Batch_idx: 370 |  Loss: (0.0581) | Acc: (98.06%) (46566/47488)\n",
            "Epoch: 149 | Batch_idx: 380 |  Loss: (0.0582) | Acc: (98.05%) (47817/48768)\n",
            "Epoch: 149 | Batch_idx: 390 |  Loss: (0.0578) | Acc: (98.06%) (49029/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3416) | Acc: (91.51%) (9151/10000)\n",
            "Epoch: 150 | Batch_idx: 0 |  Loss: (0.0482) | Acc: (98.44%) (126/128)\n",
            "Epoch: 150 | Batch_idx: 10 |  Loss: (0.0467) | Acc: (98.37%) (1385/1408)\n",
            "Epoch: 150 | Batch_idx: 20 |  Loss: (0.0527) | Acc: (98.18%) (2639/2688)\n",
            "Epoch: 150 | Batch_idx: 30 |  Loss: (0.0538) | Acc: (98.19%) (3896/3968)\n",
            "Epoch: 150 | Batch_idx: 40 |  Loss: (0.0532) | Acc: (98.30%) (5159/5248)\n",
            "Epoch: 150 | Batch_idx: 50 |  Loss: (0.0535) | Acc: (98.33%) (6419/6528)\n",
            "Epoch: 150 | Batch_idx: 60 |  Loss: (0.0525) | Acc: (98.32%) (7677/7808)\n",
            "Epoch: 150 | Batch_idx: 70 |  Loss: (0.0528) | Acc: (98.28%) (8932/9088)\n",
            "Epoch: 150 | Batch_idx: 80 |  Loss: (0.0517) | Acc: (98.26%) (10188/10368)\n",
            "Epoch: 150 | Batch_idx: 90 |  Loss: (0.0530) | Acc: (98.25%) (11444/11648)\n",
            "Epoch: 150 | Batch_idx: 100 |  Loss: (0.0530) | Acc: (98.24%) (12701/12928)\n",
            "Epoch: 150 | Batch_idx: 110 |  Loss: (0.0522) | Acc: (98.25%) (13960/14208)\n",
            "Epoch: 150 | Batch_idx: 120 |  Loss: (0.0529) | Acc: (98.21%) (15210/15488)\n",
            "Epoch: 150 | Batch_idx: 130 |  Loss: (0.0531) | Acc: (98.19%) (16465/16768)\n",
            "Epoch: 150 | Batch_idx: 140 |  Loss: (0.0536) | Acc: (98.20%) (17723/18048)\n",
            "Epoch: 150 | Batch_idx: 150 |  Loss: (0.0533) | Acc: (98.21%) (18982/19328)\n",
            "Epoch: 150 | Batch_idx: 160 |  Loss: (0.0527) | Acc: (98.24%) (20245/20608)\n",
            "Epoch: 150 | Batch_idx: 170 |  Loss: (0.0533) | Acc: (98.20%) (21494/21888)\n",
            "Epoch: 150 | Batch_idx: 180 |  Loss: (0.0531) | Acc: (98.20%) (22751/23168)\n",
            "Epoch: 150 | Batch_idx: 190 |  Loss: (0.0541) | Acc: (98.17%) (24000/24448)\n",
            "Epoch: 150 | Batch_idx: 200 |  Loss: (0.0535) | Acc: (98.17%) (25257/25728)\n",
            "Epoch: 150 | Batch_idx: 210 |  Loss: (0.0534) | Acc: (98.18%) (26516/27008)\n",
            "Epoch: 150 | Batch_idx: 220 |  Loss: (0.0533) | Acc: (98.18%) (27774/28288)\n",
            "Epoch: 150 | Batch_idx: 230 |  Loss: (0.0531) | Acc: (98.19%) (29033/29568)\n",
            "Epoch: 150 | Batch_idx: 240 |  Loss: (0.0531) | Acc: (98.19%) (30290/30848)\n",
            "Epoch: 150 | Batch_idx: 250 |  Loss: (0.0531) | Acc: (98.19%) (31545/32128)\n",
            "Epoch: 150 | Batch_idx: 260 |  Loss: (0.0526) | Acc: (98.20%) (32806/33408)\n",
            "Epoch: 150 | Batch_idx: 270 |  Loss: (0.0531) | Acc: (98.20%) (34062/34688)\n",
            "Epoch: 150 | Batch_idx: 280 |  Loss: (0.0533) | Acc: (98.19%) (35317/35968)\n",
            "Epoch: 150 | Batch_idx: 290 |  Loss: (0.0535) | Acc: (98.18%) (36571/37248)\n",
            "Epoch: 150 | Batch_idx: 300 |  Loss: (0.0535) | Acc: (98.18%) (37826/38528)\n",
            "Epoch: 150 | Batch_idx: 310 |  Loss: (0.0536) | Acc: (98.18%) (39084/39808)\n",
            "Epoch: 150 | Batch_idx: 320 |  Loss: (0.0541) | Acc: (98.16%) (40331/41088)\n",
            "Epoch: 150 | Batch_idx: 330 |  Loss: (0.0544) | Acc: (98.14%) (41582/42368)\n",
            "Epoch: 150 | Batch_idx: 340 |  Loss: (0.0541) | Acc: (98.17%) (42848/43648)\n",
            "Epoch: 150 | Batch_idx: 350 |  Loss: (0.0541) | Acc: (98.17%) (44104/44928)\n",
            "Epoch: 150 | Batch_idx: 360 |  Loss: (0.0545) | Acc: (98.14%) (45348/46208)\n",
            "Epoch: 150 | Batch_idx: 370 |  Loss: (0.0540) | Acc: (98.16%) (46615/47488)\n",
            "Epoch: 150 | Batch_idx: 380 |  Loss: (0.0538) | Acc: (98.17%) (47874/48768)\n",
            "Epoch: 150 | Batch_idx: 390 |  Loss: (0.0536) | Acc: (98.18%) (49090/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3414) | Acc: (91.62%) (9162/10000)\n",
            "Epoch: 151 | Batch_idx: 0 |  Loss: (0.0971) | Acc: (97.66%) (125/128)\n",
            "Epoch: 151 | Batch_idx: 10 |  Loss: (0.0608) | Acc: (97.73%) (1376/1408)\n",
            "Epoch: 151 | Batch_idx: 20 |  Loss: (0.0526) | Acc: (98.07%) (2636/2688)\n",
            "Epoch: 151 | Batch_idx: 30 |  Loss: (0.0494) | Acc: (98.24%) (3898/3968)\n",
            "Epoch: 151 | Batch_idx: 40 |  Loss: (0.0549) | Acc: (98.08%) (5147/5248)\n",
            "Epoch: 151 | Batch_idx: 50 |  Loss: (0.0538) | Acc: (98.10%) (6404/6528)\n",
            "Epoch: 151 | Batch_idx: 60 |  Loss: (0.0525) | Acc: (98.10%) (7660/7808)\n",
            "Epoch: 151 | Batch_idx: 70 |  Loss: (0.0519) | Acc: (98.16%) (8921/9088)\n",
            "Epoch: 151 | Batch_idx: 80 |  Loss: (0.0522) | Acc: (98.16%) (10177/10368)\n",
            "Epoch: 151 | Batch_idx: 90 |  Loss: (0.0527) | Acc: (98.18%) (11436/11648)\n",
            "Epoch: 151 | Batch_idx: 100 |  Loss: (0.0519) | Acc: (98.22%) (12698/12928)\n",
            "Epoch: 151 | Batch_idx: 110 |  Loss: (0.0521) | Acc: (98.23%) (13956/14208)\n",
            "Epoch: 151 | Batch_idx: 120 |  Loss: (0.0529) | Acc: (98.23%) (15214/15488)\n",
            "Epoch: 151 | Batch_idx: 130 |  Loss: (0.0530) | Acc: (98.22%) (16470/16768)\n",
            "Epoch: 151 | Batch_idx: 140 |  Loss: (0.0522) | Acc: (98.22%) (17727/18048)\n",
            "Epoch: 151 | Batch_idx: 150 |  Loss: (0.0517) | Acc: (98.25%) (18990/19328)\n",
            "Epoch: 151 | Batch_idx: 160 |  Loss: (0.0520) | Acc: (98.24%) (20246/20608)\n",
            "Epoch: 151 | Batch_idx: 170 |  Loss: (0.0518) | Acc: (98.25%) (21506/21888)\n",
            "Epoch: 151 | Batch_idx: 180 |  Loss: (0.0510) | Acc: (98.27%) (22767/23168)\n",
            "Epoch: 151 | Batch_idx: 190 |  Loss: (0.0521) | Acc: (98.22%) (24013/24448)\n",
            "Epoch: 151 | Batch_idx: 200 |  Loss: (0.0527) | Acc: (98.20%) (25265/25728)\n",
            "Epoch: 151 | Batch_idx: 210 |  Loss: (0.0525) | Acc: (98.20%) (26522/27008)\n",
            "Epoch: 151 | Batch_idx: 220 |  Loss: (0.0525) | Acc: (98.20%) (27778/28288)\n",
            "Epoch: 151 | Batch_idx: 230 |  Loss: (0.0524) | Acc: (98.19%) (29034/29568)\n",
            "Epoch: 151 | Batch_idx: 240 |  Loss: (0.0521) | Acc: (98.22%) (30300/30848)\n",
            "Epoch: 151 | Batch_idx: 250 |  Loss: (0.0519) | Acc: (98.24%) (31561/32128)\n",
            "Epoch: 151 | Batch_idx: 260 |  Loss: (0.0523) | Acc: (98.23%) (32818/33408)\n",
            "Epoch: 151 | Batch_idx: 270 |  Loss: (0.0518) | Acc: (98.25%) (34081/34688)\n",
            "Epoch: 151 | Batch_idx: 280 |  Loss: (0.0524) | Acc: (98.23%) (35330/35968)\n",
            "Epoch: 151 | Batch_idx: 290 |  Loss: (0.0525) | Acc: (98.22%) (36585/37248)\n",
            "Epoch: 151 | Batch_idx: 300 |  Loss: (0.0530) | Acc: (98.21%) (37838/38528)\n",
            "Epoch: 151 | Batch_idx: 310 |  Loss: (0.0537) | Acc: (98.19%) (39089/39808)\n",
            "Epoch: 151 | Batch_idx: 320 |  Loss: (0.0539) | Acc: (98.18%) (40340/41088)\n",
            "Epoch: 151 | Batch_idx: 330 |  Loss: (0.0541) | Acc: (98.18%) (41595/42368)\n",
            "Epoch: 151 | Batch_idx: 340 |  Loss: (0.0544) | Acc: (98.16%) (42847/43648)\n",
            "Epoch: 151 | Batch_idx: 350 |  Loss: (0.0543) | Acc: (98.16%) (44101/44928)\n",
            "Epoch: 151 | Batch_idx: 360 |  Loss: (0.0544) | Acc: (98.16%) (45359/46208)\n",
            "Epoch: 151 | Batch_idx: 370 |  Loss: (0.0546) | Acc: (98.16%) (46614/47488)\n",
            "Epoch: 151 | Batch_idx: 380 |  Loss: (0.0553) | Acc: (98.12%) (47853/48768)\n",
            "Epoch: 151 | Batch_idx: 390 |  Loss: (0.0552) | Acc: (98.13%) (49066/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3437) | Acc: (91.65%) (9165/10000)\n",
            "Epoch: 152 | Batch_idx: 0 |  Loss: (0.0483) | Acc: (98.44%) (126/128)\n",
            "Epoch: 152 | Batch_idx: 10 |  Loss: (0.0504) | Acc: (98.08%) (1381/1408)\n",
            "Epoch: 152 | Batch_idx: 20 |  Loss: (0.0606) | Acc: (98.07%) (2636/2688)\n",
            "Epoch: 152 | Batch_idx: 30 |  Loss: (0.0547) | Acc: (98.11%) (3893/3968)\n",
            "Epoch: 152 | Batch_idx: 40 |  Loss: (0.0551) | Acc: (98.06%) (5146/5248)\n",
            "Epoch: 152 | Batch_idx: 50 |  Loss: (0.0535) | Acc: (98.16%) (6408/6528)\n",
            "Epoch: 152 | Batch_idx: 60 |  Loss: (0.0520) | Acc: (98.23%) (7670/7808)\n",
            "Epoch: 152 | Batch_idx: 70 |  Loss: (0.0518) | Acc: (98.22%) (8926/9088)\n",
            "Epoch: 152 | Batch_idx: 80 |  Loss: (0.0512) | Acc: (98.26%) (10188/10368)\n",
            "Epoch: 152 | Batch_idx: 90 |  Loss: (0.0541) | Acc: (98.12%) (11429/11648)\n",
            "Epoch: 152 | Batch_idx: 100 |  Loss: (0.0534) | Acc: (98.14%) (12687/12928)\n",
            "Epoch: 152 | Batch_idx: 110 |  Loss: (0.0539) | Acc: (98.12%) (13941/14208)\n",
            "Epoch: 152 | Batch_idx: 120 |  Loss: (0.0535) | Acc: (98.12%) (15197/15488)\n",
            "Epoch: 152 | Batch_idx: 130 |  Loss: (0.0538) | Acc: (98.13%) (16454/16768)\n",
            "Epoch: 152 | Batch_idx: 140 |  Loss: (0.0531) | Acc: (98.17%) (17717/18048)\n",
            "Epoch: 152 | Batch_idx: 150 |  Loss: (0.0520) | Acc: (98.20%) (18981/19328)\n",
            "Epoch: 152 | Batch_idx: 160 |  Loss: (0.0530) | Acc: (98.18%) (20232/20608)\n",
            "Epoch: 152 | Batch_idx: 170 |  Loss: (0.0523) | Acc: (98.20%) (21495/21888)\n",
            "Epoch: 152 | Batch_idx: 180 |  Loss: (0.0519) | Acc: (98.23%) (22757/23168)\n",
            "Epoch: 152 | Batch_idx: 190 |  Loss: (0.0517) | Acc: (98.24%) (24018/24448)\n",
            "Epoch: 152 | Batch_idx: 200 |  Loss: (0.0518) | Acc: (98.24%) (25276/25728)\n",
            "Epoch: 152 | Batch_idx: 210 |  Loss: (0.0522) | Acc: (98.24%) (26532/27008)\n",
            "Epoch: 152 | Batch_idx: 220 |  Loss: (0.0528) | Acc: (98.21%) (27781/28288)\n",
            "Epoch: 152 | Batch_idx: 230 |  Loss: (0.0524) | Acc: (98.22%) (29042/29568)\n",
            "Epoch: 152 | Batch_idx: 240 |  Loss: (0.0523) | Acc: (98.23%) (30301/30848)\n",
            "Epoch: 152 | Batch_idx: 250 |  Loss: (0.0526) | Acc: (98.22%) (31556/32128)\n",
            "Epoch: 152 | Batch_idx: 260 |  Loss: (0.0534) | Acc: (98.20%) (32805/33408)\n",
            "Epoch: 152 | Batch_idx: 270 |  Loss: (0.0536) | Acc: (98.17%) (34054/34688)\n",
            "Epoch: 152 | Batch_idx: 280 |  Loss: (0.0533) | Acc: (98.18%) (35315/35968)\n",
            "Epoch: 152 | Batch_idx: 290 |  Loss: (0.0529) | Acc: (98.21%) (36581/37248)\n",
            "Epoch: 152 | Batch_idx: 300 |  Loss: (0.0529) | Acc: (98.21%) (37839/38528)\n",
            "Epoch: 152 | Batch_idx: 310 |  Loss: (0.0530) | Acc: (98.22%) (39098/39808)\n",
            "Epoch: 152 | Batch_idx: 320 |  Loss: (0.0527) | Acc: (98.23%) (40360/41088)\n",
            "Epoch: 152 | Batch_idx: 330 |  Loss: (0.0527) | Acc: (98.22%) (41615/42368)\n",
            "Epoch: 152 | Batch_idx: 340 |  Loss: (0.0527) | Acc: (98.21%) (42867/43648)\n",
            "Epoch: 152 | Batch_idx: 350 |  Loss: (0.0525) | Acc: (98.23%) (44132/44928)\n",
            "Epoch: 152 | Batch_idx: 360 |  Loss: (0.0524) | Acc: (98.23%) (45388/46208)\n",
            "Epoch: 152 | Batch_idx: 370 |  Loss: (0.0526) | Acc: (98.22%) (46641/47488)\n",
            "Epoch: 152 | Batch_idx: 380 |  Loss: (0.0527) | Acc: (98.21%) (47897/48768)\n",
            "Epoch: 152 | Batch_idx: 390 |  Loss: (0.0528) | Acc: (98.22%) (49110/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3406) | Acc: (91.58%) (9158/10000)\n",
            "Epoch: 153 | Batch_idx: 0 |  Loss: (0.0214) | Acc: (99.22%) (127/128)\n",
            "Epoch: 153 | Batch_idx: 10 |  Loss: (0.0441) | Acc: (98.37%) (1385/1408)\n",
            "Epoch: 153 | Batch_idx: 20 |  Loss: (0.0403) | Acc: (98.51%) (2648/2688)\n",
            "Epoch: 153 | Batch_idx: 30 |  Loss: (0.0498) | Acc: (98.24%) (3898/3968)\n",
            "Epoch: 153 | Batch_idx: 40 |  Loss: (0.0547) | Acc: (98.04%) (5145/5248)\n",
            "Epoch: 153 | Batch_idx: 50 |  Loss: (0.0514) | Acc: (98.18%) (6409/6528)\n",
            "Epoch: 153 | Batch_idx: 60 |  Loss: (0.0527) | Acc: (98.17%) (7665/7808)\n",
            "Epoch: 153 | Batch_idx: 70 |  Loss: (0.0517) | Acc: (98.20%) (8924/9088)\n",
            "Epoch: 153 | Batch_idx: 80 |  Loss: (0.0514) | Acc: (98.22%) (10183/10368)\n",
            "Epoch: 153 | Batch_idx: 90 |  Loss: (0.0524) | Acc: (98.25%) (11444/11648)\n",
            "Epoch: 153 | Batch_idx: 100 |  Loss: (0.0520) | Acc: (98.23%) (12699/12928)\n",
            "Epoch: 153 | Batch_idx: 110 |  Loss: (0.0533) | Acc: (98.17%) (13948/14208)\n",
            "Epoch: 153 | Batch_idx: 120 |  Loss: (0.0526) | Acc: (98.19%) (15208/15488)\n",
            "Epoch: 153 | Batch_idx: 130 |  Loss: (0.0529) | Acc: (98.18%) (16462/16768)\n",
            "Epoch: 153 | Batch_idx: 140 |  Loss: (0.0529) | Acc: (98.18%) (17720/18048)\n",
            "Epoch: 153 | Batch_idx: 150 |  Loss: (0.0521) | Acc: (98.19%) (18978/19328)\n",
            "Epoch: 153 | Batch_idx: 160 |  Loss: (0.0522) | Acc: (98.19%) (20235/20608)\n",
            "Epoch: 153 | Batch_idx: 170 |  Loss: (0.0518) | Acc: (98.18%) (21489/21888)\n",
            "Epoch: 153 | Batch_idx: 180 |  Loss: (0.0519) | Acc: (98.16%) (22741/23168)\n",
            "Epoch: 153 | Batch_idx: 190 |  Loss: (0.0520) | Acc: (98.15%) (23995/24448)\n",
            "Epoch: 153 | Batch_idx: 200 |  Loss: (0.0529) | Acc: (98.11%) (25243/25728)\n",
            "Epoch: 153 | Batch_idx: 210 |  Loss: (0.0533) | Acc: (98.11%) (26498/27008)\n",
            "Epoch: 153 | Batch_idx: 220 |  Loss: (0.0530) | Acc: (98.13%) (27760/28288)\n",
            "Epoch: 153 | Batch_idx: 230 |  Loss: (0.0531) | Acc: (98.14%) (29017/29568)\n",
            "Epoch: 153 | Batch_idx: 240 |  Loss: (0.0538) | Acc: (98.12%) (30268/30848)\n",
            "Epoch: 153 | Batch_idx: 250 |  Loss: (0.0543) | Acc: (98.10%) (31518/32128)\n",
            "Epoch: 153 | Batch_idx: 260 |  Loss: (0.0543) | Acc: (98.11%) (32775/33408)\n",
            "Epoch: 153 | Batch_idx: 270 |  Loss: (0.0543) | Acc: (98.11%) (34031/34688)\n",
            "Epoch: 153 | Batch_idx: 280 |  Loss: (0.0553) | Acc: (98.07%) (35274/35968)\n",
            "Epoch: 153 | Batch_idx: 290 |  Loss: (0.0551) | Acc: (98.08%) (36534/37248)\n",
            "Epoch: 153 | Batch_idx: 300 |  Loss: (0.0551) | Acc: (98.10%) (37796/38528)\n",
            "Epoch: 153 | Batch_idx: 310 |  Loss: (0.0545) | Acc: (98.12%) (39060/39808)\n",
            "Epoch: 153 | Batch_idx: 320 |  Loss: (0.0547) | Acc: (98.11%) (40310/41088)\n",
            "Epoch: 153 | Batch_idx: 330 |  Loss: (0.0544) | Acc: (98.12%) (41570/42368)\n",
            "Epoch: 153 | Batch_idx: 340 |  Loss: (0.0546) | Acc: (98.11%) (42824/43648)\n",
            "Epoch: 153 | Batch_idx: 350 |  Loss: (0.0547) | Acc: (98.11%) (44077/44928)\n",
            "Epoch: 153 | Batch_idx: 360 |  Loss: (0.0547) | Acc: (98.11%) (45334/46208)\n",
            "Epoch: 153 | Batch_idx: 370 |  Loss: (0.0544) | Acc: (98.11%) (46592/47488)\n",
            "Epoch: 153 | Batch_idx: 380 |  Loss: (0.0543) | Acc: (98.12%) (47853/48768)\n",
            "Epoch: 153 | Batch_idx: 390 |  Loss: (0.0545) | Acc: (98.12%) (49059/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3415) | Acc: (91.66%) (9166/10000)\n",
            "Epoch: 154 | Batch_idx: 0 |  Loss: (0.0258) | Acc: (99.22%) (127/128)\n",
            "Epoch: 154 | Batch_idx: 10 |  Loss: (0.0491) | Acc: (98.37%) (1385/1408)\n",
            "Epoch: 154 | Batch_idx: 20 |  Loss: (0.0581) | Acc: (98.10%) (2637/2688)\n",
            "Epoch: 154 | Batch_idx: 30 |  Loss: (0.0624) | Acc: (98.08%) (3892/3968)\n",
            "Epoch: 154 | Batch_idx: 40 |  Loss: (0.0601) | Acc: (98.09%) (5148/5248)\n",
            "Epoch: 154 | Batch_idx: 50 |  Loss: (0.0579) | Acc: (98.12%) (6405/6528)\n",
            "Epoch: 154 | Batch_idx: 60 |  Loss: (0.0559) | Acc: (98.16%) (7664/7808)\n",
            "Epoch: 154 | Batch_idx: 70 |  Loss: (0.0562) | Acc: (98.14%) (8919/9088)\n",
            "Epoch: 154 | Batch_idx: 80 |  Loss: (0.0561) | Acc: (98.14%) (10175/10368)\n",
            "Epoch: 154 | Batch_idx: 90 |  Loss: (0.0565) | Acc: (98.07%) (11423/11648)\n",
            "Epoch: 154 | Batch_idx: 100 |  Loss: (0.0548) | Acc: (98.12%) (12685/12928)\n",
            "Epoch: 154 | Batch_idx: 110 |  Loss: (0.0555) | Acc: (98.06%) (13933/14208)\n",
            "Epoch: 154 | Batch_idx: 120 |  Loss: (0.0554) | Acc: (98.09%) (15192/15488)\n",
            "Epoch: 154 | Batch_idx: 130 |  Loss: (0.0558) | Acc: (98.09%) (16447/16768)\n",
            "Epoch: 154 | Batch_idx: 140 |  Loss: (0.0562) | Acc: (98.06%) (17698/18048)\n",
            "Epoch: 154 | Batch_idx: 150 |  Loss: (0.0566) | Acc: (98.05%) (18952/19328)\n",
            "Epoch: 154 | Batch_idx: 160 |  Loss: (0.0570) | Acc: (98.03%) (20203/20608)\n",
            "Epoch: 154 | Batch_idx: 170 |  Loss: (0.0568) | Acc: (98.04%) (21460/21888)\n",
            "Epoch: 154 | Batch_idx: 180 |  Loss: (0.0566) | Acc: (98.03%) (22712/23168)\n",
            "Epoch: 154 | Batch_idx: 190 |  Loss: (0.0563) | Acc: (98.04%) (23970/24448)\n",
            "Epoch: 154 | Batch_idx: 200 |  Loss: (0.0560) | Acc: (98.05%) (25226/25728)\n",
            "Epoch: 154 | Batch_idx: 210 |  Loss: (0.0558) | Acc: (98.06%) (26485/27008)\n",
            "Epoch: 154 | Batch_idx: 220 |  Loss: (0.0562) | Acc: (98.05%) (27736/28288)\n",
            "Epoch: 154 | Batch_idx: 230 |  Loss: (0.0572) | Acc: (98.01%) (28981/29568)\n",
            "Epoch: 154 | Batch_idx: 240 |  Loss: (0.0576) | Acc: (98.02%) (30237/30848)\n",
            "Epoch: 154 | Batch_idx: 250 |  Loss: (0.0574) | Acc: (98.02%) (31493/32128)\n",
            "Epoch: 154 | Batch_idx: 260 |  Loss: (0.0576) | Acc: (98.01%) (32744/33408)\n",
            "Epoch: 154 | Batch_idx: 270 |  Loss: (0.0572) | Acc: (98.04%) (34007/34688)\n",
            "Epoch: 154 | Batch_idx: 280 |  Loss: (0.0567) | Acc: (98.05%) (35268/35968)\n",
            "Epoch: 154 | Batch_idx: 290 |  Loss: (0.0567) | Acc: (98.06%) (36527/37248)\n",
            "Epoch: 154 | Batch_idx: 300 |  Loss: (0.0567) | Acc: (98.07%) (37784/38528)\n",
            "Epoch: 154 | Batch_idx: 310 |  Loss: (0.0565) | Acc: (98.08%) (39045/39808)\n",
            "Epoch: 154 | Batch_idx: 320 |  Loss: (0.0568) | Acc: (98.07%) (40297/41088)\n",
            "Epoch: 154 | Batch_idx: 330 |  Loss: (0.0569) | Acc: (98.08%) (41554/42368)\n",
            "Epoch: 154 | Batch_idx: 340 |  Loss: (0.0571) | Acc: (98.06%) (42803/43648)\n",
            "Epoch: 154 | Batch_idx: 350 |  Loss: (0.0572) | Acc: (98.05%) (44051/44928)\n",
            "Epoch: 154 | Batch_idx: 360 |  Loss: (0.0571) | Acc: (98.04%) (45303/46208)\n",
            "Epoch: 154 | Batch_idx: 370 |  Loss: (0.0570) | Acc: (98.05%) (46561/47488)\n",
            "Epoch: 154 | Batch_idx: 380 |  Loss: (0.0567) | Acc: (98.06%) (47823/48768)\n",
            "Epoch: 154 | Batch_idx: 390 |  Loss: (0.0567) | Acc: (98.06%) (49032/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3410) | Acc: (91.72%) (9172/10000)\n",
            "Epoch: 155 | Batch_idx: 0 |  Loss: (0.0285) | Acc: (98.44%) (126/128)\n",
            "Epoch: 155 | Batch_idx: 10 |  Loss: (0.0436) | Acc: (98.30%) (1384/1408)\n",
            "Epoch: 155 | Batch_idx: 20 |  Loss: (0.0430) | Acc: (98.55%) (2649/2688)\n",
            "Epoch: 155 | Batch_idx: 30 |  Loss: (0.0456) | Acc: (98.41%) (3905/3968)\n",
            "Epoch: 155 | Batch_idx: 40 |  Loss: (0.0479) | Acc: (98.44%) (5166/5248)\n",
            "Epoch: 155 | Batch_idx: 50 |  Loss: (0.0523) | Acc: (98.27%) (6415/6528)\n",
            "Epoch: 155 | Batch_idx: 60 |  Loss: (0.0524) | Acc: (98.25%) (7671/7808)\n",
            "Epoch: 155 | Batch_idx: 70 |  Loss: (0.0526) | Acc: (98.26%) (8930/9088)\n",
            "Epoch: 155 | Batch_idx: 80 |  Loss: (0.0525) | Acc: (98.24%) (10186/10368)\n",
            "Epoch: 155 | Batch_idx: 90 |  Loss: (0.0539) | Acc: (98.23%) (11442/11648)\n",
            "Epoch: 155 | Batch_idx: 100 |  Loss: (0.0535) | Acc: (98.23%) (12699/12928)\n",
            "Epoch: 155 | Batch_idx: 110 |  Loss: (0.0532) | Acc: (98.22%) (13955/14208)\n",
            "Epoch: 155 | Batch_idx: 120 |  Loss: (0.0540) | Acc: (98.19%) (15208/15488)\n",
            "Epoch: 155 | Batch_idx: 130 |  Loss: (0.0550) | Acc: (98.12%) (16453/16768)\n",
            "Epoch: 155 | Batch_idx: 140 |  Loss: (0.0545) | Acc: (98.15%) (17714/18048)\n",
            "Epoch: 155 | Batch_idx: 150 |  Loss: (0.0552) | Acc: (98.14%) (18968/19328)\n",
            "Epoch: 155 | Batch_idx: 160 |  Loss: (0.0558) | Acc: (98.14%) (20225/20608)\n",
            "Epoch: 155 | Batch_idx: 170 |  Loss: (0.0552) | Acc: (98.17%) (21487/21888)\n",
            "Epoch: 155 | Batch_idx: 180 |  Loss: (0.0557) | Acc: (98.15%) (22740/23168)\n",
            "Epoch: 155 | Batch_idx: 190 |  Loss: (0.0551) | Acc: (98.17%) (24000/24448)\n",
            "Epoch: 155 | Batch_idx: 200 |  Loss: (0.0549) | Acc: (98.16%) (25254/25728)\n",
            "Epoch: 155 | Batch_idx: 210 |  Loss: (0.0548) | Acc: (98.14%) (26507/27008)\n",
            "Epoch: 155 | Batch_idx: 220 |  Loss: (0.0546) | Acc: (98.16%) (27767/28288)\n",
            "Epoch: 155 | Batch_idx: 230 |  Loss: (0.0543) | Acc: (98.16%) (29023/29568)\n",
            "Epoch: 155 | Batch_idx: 240 |  Loss: (0.0540) | Acc: (98.17%) (30283/30848)\n",
            "Epoch: 155 | Batch_idx: 250 |  Loss: (0.0536) | Acc: (98.19%) (31545/32128)\n",
            "Epoch: 155 | Batch_idx: 260 |  Loss: (0.0533) | Acc: (98.19%) (32804/33408)\n",
            "Epoch: 155 | Batch_idx: 270 |  Loss: (0.0538) | Acc: (98.17%) (34053/34688)\n",
            "Epoch: 155 | Batch_idx: 280 |  Loss: (0.0540) | Acc: (98.16%) (35306/35968)\n",
            "Epoch: 155 | Batch_idx: 290 |  Loss: (0.0550) | Acc: (98.12%) (36547/37248)\n",
            "Epoch: 155 | Batch_idx: 300 |  Loss: (0.0552) | Acc: (98.11%) (37799/38528)\n",
            "Epoch: 155 | Batch_idx: 310 |  Loss: (0.0550) | Acc: (98.12%) (39058/39808)\n",
            "Epoch: 155 | Batch_idx: 320 |  Loss: (0.0547) | Acc: (98.11%) (40310/41088)\n",
            "Epoch: 155 | Batch_idx: 330 |  Loss: (0.0551) | Acc: (98.11%) (41566/42368)\n",
            "Epoch: 155 | Batch_idx: 340 |  Loss: (0.0551) | Acc: (98.10%) (42820/43648)\n",
            "Epoch: 155 | Batch_idx: 350 |  Loss: (0.0555) | Acc: (98.09%) (44070/44928)\n",
            "Epoch: 155 | Batch_idx: 360 |  Loss: (0.0556) | Acc: (98.10%) (45328/46208)\n",
            "Epoch: 155 | Batch_idx: 370 |  Loss: (0.0553) | Acc: (98.10%) (46585/47488)\n",
            "Epoch: 155 | Batch_idx: 380 |  Loss: (0.0551) | Acc: (98.11%) (47848/48768)\n",
            "Epoch: 155 | Batch_idx: 390 |  Loss: (0.0548) | Acc: (98.12%) (49062/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3369) | Acc: (91.63%) (9163/10000)\n",
            "Epoch: 156 | Batch_idx: 0 |  Loss: (0.0535) | Acc: (98.44%) (126/128)\n",
            "Epoch: 156 | Batch_idx: 10 |  Loss: (0.0621) | Acc: (98.08%) (1381/1408)\n",
            "Epoch: 156 | Batch_idx: 20 |  Loss: (0.0530) | Acc: (98.18%) (2639/2688)\n",
            "Epoch: 156 | Batch_idx: 30 |  Loss: (0.0511) | Acc: (98.16%) (3895/3968)\n",
            "Epoch: 156 | Batch_idx: 40 |  Loss: (0.0514) | Acc: (98.13%) (5150/5248)\n",
            "Epoch: 156 | Batch_idx: 50 |  Loss: (0.0511) | Acc: (98.12%) (6405/6528)\n",
            "Epoch: 156 | Batch_idx: 60 |  Loss: (0.0518) | Acc: (98.16%) (7664/7808)\n",
            "Epoch: 156 | Batch_idx: 70 |  Loss: (0.0506) | Acc: (98.21%) (8925/9088)\n",
            "Epoch: 156 | Batch_idx: 80 |  Loss: (0.0512) | Acc: (98.20%) (10181/10368)\n",
            "Epoch: 156 | Batch_idx: 90 |  Loss: (0.0516) | Acc: (98.19%) (11437/11648)\n",
            "Epoch: 156 | Batch_idx: 100 |  Loss: (0.0520) | Acc: (98.23%) (12699/12928)\n",
            "Epoch: 156 | Batch_idx: 110 |  Loss: (0.0520) | Acc: (98.22%) (13955/14208)\n",
            "Epoch: 156 | Batch_idx: 120 |  Loss: (0.0540) | Acc: (98.19%) (15207/15488)\n",
            "Epoch: 156 | Batch_idx: 130 |  Loss: (0.0540) | Acc: (98.18%) (16462/16768)\n",
            "Epoch: 156 | Batch_idx: 140 |  Loss: (0.0549) | Acc: (98.14%) (17713/18048)\n",
            "Epoch: 156 | Batch_idx: 150 |  Loss: (0.0541) | Acc: (98.16%) (18973/19328)\n",
            "Epoch: 156 | Batch_idx: 160 |  Loss: (0.0537) | Acc: (98.17%) (20231/20608)\n",
            "Epoch: 156 | Batch_idx: 170 |  Loss: (0.0544) | Acc: (98.15%) (21482/21888)\n",
            "Epoch: 156 | Batch_idx: 180 |  Loss: (0.0537) | Acc: (98.16%) (22742/23168)\n",
            "Epoch: 156 | Batch_idx: 190 |  Loss: (0.0530) | Acc: (98.20%) (24007/24448)\n",
            "Epoch: 156 | Batch_idx: 200 |  Loss: (0.0526) | Acc: (98.21%) (25268/25728)\n",
            "Epoch: 156 | Batch_idx: 210 |  Loss: (0.0529) | Acc: (98.20%) (26522/27008)\n",
            "Epoch: 156 | Batch_idx: 220 |  Loss: (0.0528) | Acc: (98.20%) (27780/28288)\n",
            "Epoch: 156 | Batch_idx: 230 |  Loss: (0.0539) | Acc: (98.17%) (29028/29568)\n",
            "Epoch: 156 | Batch_idx: 240 |  Loss: (0.0537) | Acc: (98.18%) (30286/30848)\n",
            "Epoch: 156 | Batch_idx: 250 |  Loss: (0.0534) | Acc: (98.18%) (31544/32128)\n",
            "Epoch: 156 | Batch_idx: 260 |  Loss: (0.0539) | Acc: (98.17%) (32796/33408)\n",
            "Epoch: 156 | Batch_idx: 270 |  Loss: (0.0543) | Acc: (98.15%) (34047/34688)\n",
            "Epoch: 156 | Batch_idx: 280 |  Loss: (0.0544) | Acc: (98.16%) (35305/35968)\n",
            "Epoch: 156 | Batch_idx: 290 |  Loss: (0.0545) | Acc: (98.15%) (36559/37248)\n",
            "Epoch: 156 | Batch_idx: 300 |  Loss: (0.0551) | Acc: (98.14%) (37810/38528)\n",
            "Epoch: 156 | Batch_idx: 310 |  Loss: (0.0549) | Acc: (98.13%) (39063/39808)\n",
            "Epoch: 156 | Batch_idx: 320 |  Loss: (0.0545) | Acc: (98.14%) (40325/41088)\n",
            "Epoch: 156 | Batch_idx: 330 |  Loss: (0.0550) | Acc: (98.13%) (41576/42368)\n",
            "Epoch: 156 | Batch_idx: 340 |  Loss: (0.0548) | Acc: (98.12%) (42829/43648)\n",
            "Epoch: 156 | Batch_idx: 350 |  Loss: (0.0545) | Acc: (98.14%) (44094/44928)\n",
            "Epoch: 156 | Batch_idx: 360 |  Loss: (0.0544) | Acc: (98.16%) (45356/46208)\n",
            "Epoch: 156 | Batch_idx: 370 |  Loss: (0.0544) | Acc: (98.15%) (46609/47488)\n",
            "Epoch: 156 | Batch_idx: 380 |  Loss: (0.0542) | Acc: (98.16%) (47869/48768)\n",
            "Epoch: 156 | Batch_idx: 390 |  Loss: (0.0543) | Acc: (98.15%) (49076/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3439) | Acc: (91.56%) (9156/10000)\n",
            "Epoch: 157 | Batch_idx: 0 |  Loss: (0.0596) | Acc: (97.66%) (125/128)\n",
            "Epoch: 157 | Batch_idx: 10 |  Loss: (0.0632) | Acc: (97.73%) (1376/1408)\n",
            "Epoch: 157 | Batch_idx: 20 |  Loss: (0.0510) | Acc: (98.36%) (2644/2688)\n",
            "Epoch: 157 | Batch_idx: 30 |  Loss: (0.0516) | Acc: (98.41%) (3905/3968)\n",
            "Epoch: 157 | Batch_idx: 40 |  Loss: (0.0522) | Acc: (98.27%) (5157/5248)\n",
            "Epoch: 157 | Batch_idx: 50 |  Loss: (0.0527) | Acc: (98.28%) (6416/6528)\n",
            "Epoch: 157 | Batch_idx: 60 |  Loss: (0.0534) | Acc: (98.30%) (7675/7808)\n",
            "Epoch: 157 | Batch_idx: 70 |  Loss: (0.0531) | Acc: (98.24%) (8928/9088)\n",
            "Epoch: 157 | Batch_idx: 80 |  Loss: (0.0535) | Acc: (98.23%) (10184/10368)\n",
            "Epoch: 157 | Batch_idx: 90 |  Loss: (0.0532) | Acc: (98.18%) (11436/11648)\n",
            "Epoch: 157 | Batch_idx: 100 |  Loss: (0.0546) | Acc: (98.15%) (12689/12928)\n",
            "Epoch: 157 | Batch_idx: 110 |  Loss: (0.0537) | Acc: (98.17%) (13948/14208)\n",
            "Epoch: 157 | Batch_idx: 120 |  Loss: (0.0525) | Acc: (98.21%) (15210/15488)\n",
            "Epoch: 157 | Batch_idx: 130 |  Loss: (0.0527) | Acc: (98.20%) (16466/16768)\n",
            "Epoch: 157 | Batch_idx: 140 |  Loss: (0.0535) | Acc: (98.19%) (17721/18048)\n",
            "Epoch: 157 | Batch_idx: 150 |  Loss: (0.0538) | Acc: (98.18%) (18977/19328)\n",
            "Epoch: 157 | Batch_idx: 160 |  Loss: (0.0532) | Acc: (98.21%) (20240/20608)\n",
            "Epoch: 157 | Batch_idx: 170 |  Loss: (0.0533) | Acc: (98.21%) (21497/21888)\n",
            "Epoch: 157 | Batch_idx: 180 |  Loss: (0.0532) | Acc: (98.20%) (22752/23168)\n",
            "Epoch: 157 | Batch_idx: 190 |  Loss: (0.0534) | Acc: (98.18%) (24003/24448)\n",
            "Epoch: 157 | Batch_idx: 200 |  Loss: (0.0538) | Acc: (98.17%) (25256/25728)\n",
            "Epoch: 157 | Batch_idx: 210 |  Loss: (0.0539) | Acc: (98.16%) (26512/27008)\n",
            "Epoch: 157 | Batch_idx: 220 |  Loss: (0.0538) | Acc: (98.17%) (27769/28288)\n",
            "Epoch: 157 | Batch_idx: 230 |  Loss: (0.0535) | Acc: (98.17%) (29026/29568)\n",
            "Epoch: 157 | Batch_idx: 240 |  Loss: (0.0532) | Acc: (98.18%) (30287/30848)\n",
            "Epoch: 157 | Batch_idx: 250 |  Loss: (0.0529) | Acc: (98.19%) (31546/32128)\n",
            "Epoch: 157 | Batch_idx: 260 |  Loss: (0.0536) | Acc: (98.16%) (32792/33408)\n",
            "Epoch: 157 | Batch_idx: 270 |  Loss: (0.0535) | Acc: (98.16%) (34050/34688)\n",
            "Epoch: 157 | Batch_idx: 280 |  Loss: (0.0542) | Acc: (98.13%) (35297/35968)\n",
            "Epoch: 157 | Batch_idx: 290 |  Loss: (0.0546) | Acc: (98.13%) (36553/37248)\n",
            "Epoch: 157 | Batch_idx: 300 |  Loss: (0.0551) | Acc: (98.12%) (37804/38528)\n",
            "Epoch: 157 | Batch_idx: 310 |  Loss: (0.0551) | Acc: (98.11%) (39054/39808)\n",
            "Epoch: 157 | Batch_idx: 320 |  Loss: (0.0553) | Acc: (98.10%) (40306/41088)\n",
            "Epoch: 157 | Batch_idx: 330 |  Loss: (0.0550) | Acc: (98.10%) (41565/42368)\n",
            "Epoch: 157 | Batch_idx: 340 |  Loss: (0.0548) | Acc: (98.11%) (42824/43648)\n",
            "Epoch: 157 | Batch_idx: 350 |  Loss: (0.0550) | Acc: (98.10%) (44076/44928)\n",
            "Epoch: 157 | Batch_idx: 360 |  Loss: (0.0547) | Acc: (98.11%) (45336/46208)\n",
            "Epoch: 157 | Batch_idx: 370 |  Loss: (0.0545) | Acc: (98.12%) (46597/47488)\n",
            "Epoch: 157 | Batch_idx: 380 |  Loss: (0.0546) | Acc: (98.11%) (47848/48768)\n",
            "Epoch: 157 | Batch_idx: 390 |  Loss: (0.0544) | Acc: (98.11%) (49057/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3396) | Acc: (91.57%) (9157/10000)\n",
            "Epoch: 158 | Batch_idx: 0 |  Loss: (0.0791) | Acc: (97.66%) (125/128)\n",
            "Epoch: 158 | Batch_idx: 10 |  Loss: (0.0440) | Acc: (98.30%) (1384/1408)\n",
            "Epoch: 158 | Batch_idx: 20 |  Loss: (0.0524) | Acc: (97.92%) (2632/2688)\n",
            "Epoch: 158 | Batch_idx: 30 |  Loss: (0.0514) | Acc: (98.03%) (3890/3968)\n",
            "Epoch: 158 | Batch_idx: 40 |  Loss: (0.0508) | Acc: (98.06%) (5146/5248)\n",
            "Epoch: 158 | Batch_idx: 50 |  Loss: (0.0489) | Acc: (98.18%) (6409/6528)\n",
            "Epoch: 158 | Batch_idx: 60 |  Loss: (0.0506) | Acc: (98.09%) (7659/7808)\n",
            "Epoch: 158 | Batch_idx: 70 |  Loss: (0.0536) | Acc: (98.04%) (8910/9088)\n",
            "Epoch: 158 | Batch_idx: 80 |  Loss: (0.0522) | Acc: (98.10%) (10171/10368)\n",
            "Epoch: 158 | Batch_idx: 90 |  Loss: (0.0505) | Acc: (98.19%) (11437/11648)\n",
            "Epoch: 158 | Batch_idx: 100 |  Loss: (0.0507) | Acc: (98.20%) (12695/12928)\n",
            "Epoch: 158 | Batch_idx: 110 |  Loss: (0.0500) | Acc: (98.23%) (13956/14208)\n",
            "Epoch: 158 | Batch_idx: 120 |  Loss: (0.0496) | Acc: (98.26%) (15218/15488)\n",
            "Epoch: 158 | Batch_idx: 130 |  Loss: (0.0508) | Acc: (98.22%) (16469/16768)\n",
            "Epoch: 158 | Batch_idx: 140 |  Loss: (0.0511) | Acc: (98.22%) (17727/18048)\n",
            "Epoch: 158 | Batch_idx: 150 |  Loss: (0.0509) | Acc: (98.19%) (18978/19328)\n",
            "Epoch: 158 | Batch_idx: 160 |  Loss: (0.0505) | Acc: (98.19%) (20235/20608)\n",
            "Epoch: 158 | Batch_idx: 170 |  Loss: (0.0506) | Acc: (98.20%) (21494/21888)\n",
            "Epoch: 158 | Batch_idx: 180 |  Loss: (0.0508) | Acc: (98.18%) (22746/23168)\n",
            "Epoch: 158 | Batch_idx: 190 |  Loss: (0.0511) | Acc: (98.15%) (23995/24448)\n",
            "Epoch: 158 | Batch_idx: 200 |  Loss: (0.0516) | Acc: (98.15%) (25252/25728)\n",
            "Epoch: 158 | Batch_idx: 210 |  Loss: (0.0513) | Acc: (98.16%) (26512/27008)\n",
            "Epoch: 158 | Batch_idx: 220 |  Loss: (0.0514) | Acc: (98.18%) (27773/28288)\n",
            "Epoch: 158 | Batch_idx: 230 |  Loss: (0.0510) | Acc: (98.18%) (29031/29568)\n",
            "Epoch: 158 | Batch_idx: 240 |  Loss: (0.0510) | Acc: (98.20%) (30293/30848)\n",
            "Epoch: 158 | Batch_idx: 250 |  Loss: (0.0511) | Acc: (98.20%) (31550/32128)\n",
            "Epoch: 158 | Batch_idx: 260 |  Loss: (0.0514) | Acc: (98.19%) (32803/33408)\n",
            "Epoch: 158 | Batch_idx: 270 |  Loss: (0.0516) | Acc: (98.19%) (34061/34688)\n",
            "Epoch: 158 | Batch_idx: 280 |  Loss: (0.0523) | Acc: (98.16%) (35306/35968)\n",
            "Epoch: 158 | Batch_idx: 290 |  Loss: (0.0523) | Acc: (98.16%) (36564/37248)\n",
            "Epoch: 158 | Batch_idx: 300 |  Loss: (0.0525) | Acc: (98.17%) (37823/38528)\n",
            "Epoch: 158 | Batch_idx: 310 |  Loss: (0.0521) | Acc: (98.19%) (39087/39808)\n",
            "Epoch: 158 | Batch_idx: 320 |  Loss: (0.0515) | Acc: (98.21%) (40352/41088)\n",
            "Epoch: 158 | Batch_idx: 330 |  Loss: (0.0516) | Acc: (98.20%) (41605/42368)\n",
            "Epoch: 158 | Batch_idx: 340 |  Loss: (0.0515) | Acc: (98.20%) (42863/43648)\n",
            "Epoch: 158 | Batch_idx: 350 |  Loss: (0.0511) | Acc: (98.22%) (44130/44928)\n",
            "Epoch: 158 | Batch_idx: 360 |  Loss: (0.0511) | Acc: (98.21%) (45383/46208)\n",
            "Epoch: 158 | Batch_idx: 370 |  Loss: (0.0511) | Acc: (98.22%) (46642/47488)\n",
            "Epoch: 158 | Batch_idx: 380 |  Loss: (0.0508) | Acc: (98.23%) (47906/48768)\n",
            "Epoch: 158 | Batch_idx: 390 |  Loss: (0.0514) | Acc: (98.22%) (49112/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3429) | Acc: (91.68%) (9168/10000)\n",
            "Epoch: 159 | Batch_idx: 0 |  Loss: (0.0310) | Acc: (98.44%) (126/128)\n",
            "Epoch: 159 | Batch_idx: 10 |  Loss: (0.0667) | Acc: (97.30%) (1370/1408)\n",
            "Epoch: 159 | Batch_idx: 20 |  Loss: (0.0606) | Acc: (97.77%) (2628/2688)\n",
            "Epoch: 159 | Batch_idx: 30 |  Loss: (0.0575) | Acc: (97.88%) (3884/3968)\n",
            "Epoch: 159 | Batch_idx: 40 |  Loss: (0.0598) | Acc: (97.79%) (5132/5248)\n",
            "Epoch: 159 | Batch_idx: 50 |  Loss: (0.0565) | Acc: (97.95%) (6394/6528)\n",
            "Epoch: 159 | Batch_idx: 60 |  Loss: (0.0558) | Acc: (98.07%) (7657/7808)\n",
            "Epoch: 159 | Batch_idx: 70 |  Loss: (0.0562) | Acc: (98.09%) (8914/9088)\n",
            "Epoch: 159 | Batch_idx: 80 |  Loss: (0.0542) | Acc: (98.13%) (10174/10368)\n",
            "Epoch: 159 | Batch_idx: 90 |  Loss: (0.0558) | Acc: (98.05%) (11421/11648)\n",
            "Epoch: 159 | Batch_idx: 100 |  Loss: (0.0561) | Acc: (98.07%) (12678/12928)\n",
            "Epoch: 159 | Batch_idx: 110 |  Loss: (0.0554) | Acc: (98.10%) (13938/14208)\n",
            "Epoch: 159 | Batch_idx: 120 |  Loss: (0.0549) | Acc: (98.12%) (15197/15488)\n",
            "Epoch: 159 | Batch_idx: 130 |  Loss: (0.0562) | Acc: (98.05%) (16441/16768)\n",
            "Epoch: 159 | Batch_idx: 140 |  Loss: (0.0564) | Acc: (98.05%) (17696/18048)\n",
            "Epoch: 159 | Batch_idx: 150 |  Loss: (0.0570) | Acc: (98.04%) (18949/19328)\n",
            "Epoch: 159 | Batch_idx: 160 |  Loss: (0.0571) | Acc: (98.04%) (20205/20608)\n",
            "Epoch: 159 | Batch_idx: 170 |  Loss: (0.0560) | Acc: (98.08%) (21467/21888)\n",
            "Epoch: 159 | Batch_idx: 180 |  Loss: (0.0565) | Acc: (98.06%) (22719/23168)\n",
            "Epoch: 159 | Batch_idx: 190 |  Loss: (0.0562) | Acc: (98.07%) (23975/24448)\n",
            "Epoch: 159 | Batch_idx: 200 |  Loss: (0.0571) | Acc: (98.05%) (25227/25728)\n",
            "Epoch: 159 | Batch_idx: 210 |  Loss: (0.0564) | Acc: (98.07%) (26488/27008)\n",
            "Epoch: 159 | Batch_idx: 220 |  Loss: (0.0560) | Acc: (98.09%) (27749/28288)\n",
            "Epoch: 159 | Batch_idx: 230 |  Loss: (0.0559) | Acc: (98.08%) (29001/29568)\n",
            "Epoch: 159 | Batch_idx: 240 |  Loss: (0.0561) | Acc: (98.08%) (30256/30848)\n",
            "Epoch: 159 | Batch_idx: 250 |  Loss: (0.0567) | Acc: (98.08%) (31510/32128)\n",
            "Epoch: 159 | Batch_idx: 260 |  Loss: (0.0564) | Acc: (98.09%) (32769/33408)\n",
            "Epoch: 159 | Batch_idx: 270 |  Loss: (0.0560) | Acc: (98.10%) (34028/34688)\n",
            "Epoch: 159 | Batch_idx: 280 |  Loss: (0.0557) | Acc: (98.11%) (35288/35968)\n",
            "Epoch: 159 | Batch_idx: 290 |  Loss: (0.0558) | Acc: (98.09%) (36538/37248)\n",
            "Epoch: 159 | Batch_idx: 300 |  Loss: (0.0555) | Acc: (98.11%) (37801/38528)\n",
            "Epoch: 159 | Batch_idx: 310 |  Loss: (0.0553) | Acc: (98.11%) (39055/39808)\n",
            "Epoch: 159 | Batch_idx: 320 |  Loss: (0.0550) | Acc: (98.11%) (40313/41088)\n",
            "Epoch: 159 | Batch_idx: 330 |  Loss: (0.0551) | Acc: (98.11%) (41569/42368)\n",
            "Epoch: 159 | Batch_idx: 340 |  Loss: (0.0553) | Acc: (98.11%) (42821/43648)\n",
            "Epoch: 159 | Batch_idx: 350 |  Loss: (0.0555) | Acc: (98.10%) (44075/44928)\n",
            "Epoch: 159 | Batch_idx: 360 |  Loss: (0.0550) | Acc: (98.12%) (45341/46208)\n",
            "Epoch: 159 | Batch_idx: 370 |  Loss: (0.0549) | Acc: (98.12%) (46597/47488)\n",
            "Epoch: 159 | Batch_idx: 380 |  Loss: (0.0548) | Acc: (98.12%) (47853/48768)\n",
            "Epoch: 159 | Batch_idx: 390 |  Loss: (0.0549) | Acc: (98.11%) (49055/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3393) | Acc: (91.68%) (9168/10000)\n",
            "Epoch: 160 | Batch_idx: 0 |  Loss: (0.0689) | Acc: (97.66%) (125/128)\n",
            "Epoch: 160 | Batch_idx: 10 |  Loss: (0.0479) | Acc: (98.37%) (1385/1408)\n",
            "Epoch: 160 | Batch_idx: 20 |  Loss: (0.0551) | Acc: (98.18%) (2639/2688)\n",
            "Epoch: 160 | Batch_idx: 30 |  Loss: (0.0556) | Acc: (98.11%) (3893/3968)\n",
            "Epoch: 160 | Batch_idx: 40 |  Loss: (0.0546) | Acc: (98.08%) (5147/5248)\n",
            "Epoch: 160 | Batch_idx: 50 |  Loss: (0.0546) | Acc: (98.12%) (6405/6528)\n",
            "Epoch: 160 | Batch_idx: 60 |  Loss: (0.0554) | Acc: (98.14%) (7663/7808)\n",
            "Epoch: 160 | Batch_idx: 70 |  Loss: (0.0552) | Acc: (98.13%) (8918/9088)\n",
            "Epoch: 160 | Batch_idx: 80 |  Loss: (0.0560) | Acc: (98.12%) (10173/10368)\n",
            "Epoch: 160 | Batch_idx: 90 |  Loss: (0.0569) | Acc: (98.09%) (11426/11648)\n",
            "Epoch: 160 | Batch_idx: 100 |  Loss: (0.0560) | Acc: (98.15%) (12689/12928)\n",
            "Epoch: 160 | Batch_idx: 110 |  Loss: (0.0548) | Acc: (98.17%) (13948/14208)\n",
            "Epoch: 160 | Batch_idx: 120 |  Loss: (0.0544) | Acc: (98.18%) (15206/15488)\n",
            "Epoch: 160 | Batch_idx: 130 |  Loss: (0.0538) | Acc: (98.19%) (16465/16768)\n",
            "Epoch: 160 | Batch_idx: 140 |  Loss: (0.0536) | Acc: (98.21%) (17725/18048)\n",
            "Epoch: 160 | Batch_idx: 150 |  Loss: (0.0535) | Acc: (98.21%) (18982/19328)\n",
            "Epoch: 160 | Batch_idx: 160 |  Loss: (0.0529) | Acc: (98.19%) (20236/20608)\n",
            "Epoch: 160 | Batch_idx: 170 |  Loss: (0.0540) | Acc: (98.18%) (21489/21888)\n",
            "Epoch: 160 | Batch_idx: 180 |  Loss: (0.0540) | Acc: (98.20%) (22750/23168)\n",
            "Epoch: 160 | Batch_idx: 190 |  Loss: (0.0537) | Acc: (98.21%) (24011/24448)\n",
            "Epoch: 160 | Batch_idx: 200 |  Loss: (0.0535) | Acc: (98.22%) (25269/25728)\n",
            "Epoch: 160 | Batch_idx: 210 |  Loss: (0.0528) | Acc: (98.22%) (26528/27008)\n",
            "Epoch: 160 | Batch_idx: 220 |  Loss: (0.0530) | Acc: (98.22%) (27784/28288)\n",
            "Epoch: 160 | Batch_idx: 230 |  Loss: (0.0532) | Acc: (98.21%) (29040/29568)\n",
            "Epoch: 160 | Batch_idx: 240 |  Loss: (0.0531) | Acc: (98.23%) (30302/30848)\n",
            "Epoch: 160 | Batch_idx: 250 |  Loss: (0.0527) | Acc: (98.23%) (31559/32128)\n",
            "Epoch: 160 | Batch_idx: 260 |  Loss: (0.0523) | Acc: (98.25%) (32822/33408)\n",
            "Epoch: 160 | Batch_idx: 270 |  Loss: (0.0516) | Acc: (98.28%) (34090/34688)\n",
            "Epoch: 160 | Batch_idx: 280 |  Loss: (0.0513) | Acc: (98.29%) (35353/35968)\n",
            "Epoch: 160 | Batch_idx: 290 |  Loss: (0.0511) | Acc: (98.29%) (36611/37248)\n",
            "Epoch: 160 | Batch_idx: 300 |  Loss: (0.0512) | Acc: (98.27%) (37863/38528)\n",
            "Epoch: 160 | Batch_idx: 310 |  Loss: (0.0510) | Acc: (98.27%) (39119/39808)\n",
            "Epoch: 160 | Batch_idx: 320 |  Loss: (0.0510) | Acc: (98.27%) (40377/41088)\n",
            "Epoch: 160 | Batch_idx: 330 |  Loss: (0.0512) | Acc: (98.27%) (41633/42368)\n",
            "Epoch: 160 | Batch_idx: 340 |  Loss: (0.0513) | Acc: (98.25%) (42886/43648)\n",
            "Epoch: 160 | Batch_idx: 350 |  Loss: (0.0516) | Acc: (98.26%) (44145/44928)\n",
            "Epoch: 160 | Batch_idx: 360 |  Loss: (0.0515) | Acc: (98.26%) (45403/46208)\n",
            "Epoch: 160 | Batch_idx: 370 |  Loss: (0.0517) | Acc: (98.25%) (46656/47488)\n",
            "Epoch: 160 | Batch_idx: 380 |  Loss: (0.0516) | Acc: (98.25%) (47915/48768)\n",
            "Epoch: 160 | Batch_idx: 390 |  Loss: (0.0517) | Acc: (98.24%) (49122/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3404) | Acc: (91.58%) (9158/10000)\n",
            "Epoch: 161 | Batch_idx: 0 |  Loss: (0.0661) | Acc: (97.66%) (125/128)\n",
            "Epoch: 161 | Batch_idx: 10 |  Loss: (0.0534) | Acc: (98.01%) (1380/1408)\n",
            "Epoch: 161 | Batch_idx: 20 |  Loss: (0.0470) | Acc: (98.40%) (2645/2688)\n",
            "Epoch: 161 | Batch_idx: 30 |  Loss: (0.0438) | Acc: (98.54%) (3910/3968)\n",
            "Epoch: 161 | Batch_idx: 40 |  Loss: (0.0458) | Acc: (98.48%) (5168/5248)\n",
            "Epoch: 161 | Batch_idx: 50 |  Loss: (0.0501) | Acc: (98.28%) (6416/6528)\n",
            "Epoch: 161 | Batch_idx: 60 |  Loss: (0.0500) | Acc: (98.32%) (7677/7808)\n",
            "Epoch: 161 | Batch_idx: 70 |  Loss: (0.0485) | Acc: (98.38%) (8941/9088)\n",
            "Epoch: 161 | Batch_idx: 80 |  Loss: (0.0494) | Acc: (98.36%) (10198/10368)\n",
            "Epoch: 161 | Batch_idx: 90 |  Loss: (0.0506) | Acc: (98.35%) (11456/11648)\n",
            "Epoch: 161 | Batch_idx: 100 |  Loss: (0.0504) | Acc: (98.35%) (12715/12928)\n",
            "Epoch: 161 | Batch_idx: 110 |  Loss: (0.0503) | Acc: (98.37%) (13977/14208)\n",
            "Epoch: 161 | Batch_idx: 120 |  Loss: (0.0509) | Acc: (98.33%) (15230/15488)\n",
            "Epoch: 161 | Batch_idx: 130 |  Loss: (0.0508) | Acc: (98.29%) (16482/16768)\n",
            "Epoch: 161 | Batch_idx: 140 |  Loss: (0.0513) | Acc: (98.27%) (17735/18048)\n",
            "Epoch: 161 | Batch_idx: 150 |  Loss: (0.0511) | Acc: (98.25%) (18990/19328)\n",
            "Epoch: 161 | Batch_idx: 160 |  Loss: (0.0508) | Acc: (98.28%) (20254/20608)\n",
            "Epoch: 161 | Batch_idx: 170 |  Loss: (0.0511) | Acc: (98.25%) (21505/21888)\n",
            "Epoch: 161 | Batch_idx: 180 |  Loss: (0.0504) | Acc: (98.28%) (22769/23168)\n",
            "Epoch: 161 | Batch_idx: 190 |  Loss: (0.0510) | Acc: (98.25%) (24021/24448)\n",
            "Epoch: 161 | Batch_idx: 200 |  Loss: (0.0512) | Acc: (98.25%) (25279/25728)\n",
            "Epoch: 161 | Batch_idx: 210 |  Loss: (0.0508) | Acc: (98.26%) (26537/27008)\n",
            "Epoch: 161 | Batch_idx: 220 |  Loss: (0.0504) | Acc: (98.25%) (27794/28288)\n",
            "Epoch: 161 | Batch_idx: 230 |  Loss: (0.0511) | Acc: (98.23%) (29044/29568)\n",
            "Epoch: 161 | Batch_idx: 240 |  Loss: (0.0510) | Acc: (98.24%) (30304/30848)\n",
            "Epoch: 161 | Batch_idx: 250 |  Loss: (0.0507) | Acc: (98.24%) (31562/32128)\n",
            "Epoch: 161 | Batch_idx: 260 |  Loss: (0.0506) | Acc: (98.25%) (32825/33408)\n",
            "Epoch: 161 | Batch_idx: 270 |  Loss: (0.0503) | Acc: (98.27%) (34087/34688)\n",
            "Epoch: 161 | Batch_idx: 280 |  Loss: (0.0505) | Acc: (98.26%) (35341/35968)\n",
            "Epoch: 161 | Batch_idx: 290 |  Loss: (0.0506) | Acc: (98.24%) (36594/37248)\n",
            "Epoch: 161 | Batch_idx: 300 |  Loss: (0.0507) | Acc: (98.25%) (37853/38528)\n",
            "Epoch: 161 | Batch_idx: 310 |  Loss: (0.0511) | Acc: (98.24%) (39108/39808)\n",
            "Epoch: 161 | Batch_idx: 320 |  Loss: (0.0519) | Acc: (98.23%) (40359/41088)\n",
            "Epoch: 161 | Batch_idx: 330 |  Loss: (0.0523) | Acc: (98.21%) (41610/42368)\n",
            "Epoch: 161 | Batch_idx: 340 |  Loss: (0.0519) | Acc: (98.23%) (42875/43648)\n",
            "Epoch: 161 | Batch_idx: 350 |  Loss: (0.0519) | Acc: (98.23%) (44134/44928)\n",
            "Epoch: 161 | Batch_idx: 360 |  Loss: (0.0524) | Acc: (98.23%) (45388/46208)\n",
            "Epoch: 161 | Batch_idx: 370 |  Loss: (0.0525) | Acc: (98.23%) (46648/47488)\n",
            "Epoch: 161 | Batch_idx: 380 |  Loss: (0.0525) | Acc: (98.23%) (47906/48768)\n",
            "Epoch: 161 | Batch_idx: 390 |  Loss: (0.0527) | Acc: (98.23%) (49116/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3430) | Acc: (91.49%) (9149/10000)\n",
            "Epoch: 162 | Batch_idx: 0 |  Loss: (0.0424) | Acc: (100.00%) (128/128)\n",
            "Epoch: 162 | Batch_idx: 10 |  Loss: (0.0533) | Acc: (98.22%) (1383/1408)\n",
            "Epoch: 162 | Batch_idx: 20 |  Loss: (0.0571) | Acc: (97.95%) (2633/2688)\n",
            "Epoch: 162 | Batch_idx: 30 |  Loss: (0.0522) | Acc: (98.06%) (3891/3968)\n",
            "Epoch: 162 | Batch_idx: 40 |  Loss: (0.0515) | Acc: (98.15%) (5151/5248)\n",
            "Epoch: 162 | Batch_idx: 50 |  Loss: (0.0533) | Acc: (98.12%) (6405/6528)\n",
            "Epoch: 162 | Batch_idx: 60 |  Loss: (0.0510) | Acc: (98.22%) (7669/7808)\n",
            "Epoch: 162 | Batch_idx: 70 |  Loss: (0.0498) | Acc: (98.28%) (8932/9088)\n",
            "Epoch: 162 | Batch_idx: 80 |  Loss: (0.0508) | Acc: (98.26%) (10188/10368)\n",
            "Epoch: 162 | Batch_idx: 90 |  Loss: (0.0508) | Acc: (98.27%) (11447/11648)\n",
            "Epoch: 162 | Batch_idx: 100 |  Loss: (0.0502) | Acc: (98.31%) (12710/12928)\n",
            "Epoch: 162 | Batch_idx: 110 |  Loss: (0.0504) | Acc: (98.30%) (13966/14208)\n",
            "Epoch: 162 | Batch_idx: 120 |  Loss: (0.0507) | Acc: (98.28%) (15221/15488)\n",
            "Epoch: 162 | Batch_idx: 130 |  Loss: (0.0505) | Acc: (98.30%) (16483/16768)\n",
            "Epoch: 162 | Batch_idx: 140 |  Loss: (0.0518) | Acc: (98.27%) (17735/18048)\n",
            "Epoch: 162 | Batch_idx: 150 |  Loss: (0.0522) | Acc: (98.28%) (18996/19328)\n",
            "Epoch: 162 | Batch_idx: 160 |  Loss: (0.0508) | Acc: (98.36%) (20270/20608)\n",
            "Epoch: 162 | Batch_idx: 170 |  Loss: (0.0518) | Acc: (98.34%) (21524/21888)\n",
            "Epoch: 162 | Batch_idx: 180 |  Loss: (0.0523) | Acc: (98.29%) (22771/23168)\n",
            "Epoch: 162 | Batch_idx: 190 |  Loss: (0.0522) | Acc: (98.27%) (24025/24448)\n",
            "Epoch: 162 | Batch_idx: 200 |  Loss: (0.0528) | Acc: (98.24%) (25276/25728)\n",
            "Epoch: 162 | Batch_idx: 210 |  Loss: (0.0536) | Acc: (98.22%) (26526/27008)\n",
            "Epoch: 162 | Batch_idx: 220 |  Loss: (0.0540) | Acc: (98.19%) (27777/28288)\n",
            "Epoch: 162 | Batch_idx: 230 |  Loss: (0.0538) | Acc: (98.19%) (29032/29568)\n",
            "Epoch: 162 | Batch_idx: 240 |  Loss: (0.0534) | Acc: (98.19%) (30291/30848)\n",
            "Epoch: 162 | Batch_idx: 250 |  Loss: (0.0531) | Acc: (98.21%) (31552/32128)\n",
            "Epoch: 162 | Batch_idx: 260 |  Loss: (0.0521) | Acc: (98.25%) (32822/33408)\n",
            "Epoch: 162 | Batch_idx: 270 |  Loss: (0.0522) | Acc: (98.24%) (34077/34688)\n",
            "Epoch: 162 | Batch_idx: 280 |  Loss: (0.0522) | Acc: (98.23%) (35333/35968)\n",
            "Epoch: 162 | Batch_idx: 290 |  Loss: (0.0523) | Acc: (98.23%) (36589/37248)\n",
            "Epoch: 162 | Batch_idx: 300 |  Loss: (0.0523) | Acc: (98.23%) (37846/38528)\n",
            "Epoch: 162 | Batch_idx: 310 |  Loss: (0.0521) | Acc: (98.23%) (39103/39808)\n",
            "Epoch: 162 | Batch_idx: 320 |  Loss: (0.0524) | Acc: (98.23%) (40360/41088)\n",
            "Epoch: 162 | Batch_idx: 330 |  Loss: (0.0524) | Acc: (98.23%) (41618/42368)\n",
            "Epoch: 162 | Batch_idx: 340 |  Loss: (0.0526) | Acc: (98.21%) (42868/43648)\n",
            "Epoch: 162 | Batch_idx: 350 |  Loss: (0.0527) | Acc: (98.21%) (44122/44928)\n",
            "Epoch: 162 | Batch_idx: 360 |  Loss: (0.0529) | Acc: (98.20%) (45376/46208)\n",
            "Epoch: 162 | Batch_idx: 370 |  Loss: (0.0530) | Acc: (98.20%) (46635/47488)\n",
            "Epoch: 162 | Batch_idx: 380 |  Loss: (0.0530) | Acc: (98.20%) (47892/48768)\n",
            "Epoch: 162 | Batch_idx: 390 |  Loss: (0.0528) | Acc: (98.21%) (49105/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3454) | Acc: (91.45%) (9145/10000)\n",
            "Epoch: 163 | Batch_idx: 0 |  Loss: (0.0437) | Acc: (98.44%) (126/128)\n",
            "Epoch: 163 | Batch_idx: 10 |  Loss: (0.0650) | Acc: (98.15%) (1382/1408)\n",
            "Epoch: 163 | Batch_idx: 20 |  Loss: (0.0576) | Acc: (98.18%) (2639/2688)\n",
            "Epoch: 163 | Batch_idx: 30 |  Loss: (0.0552) | Acc: (98.19%) (3896/3968)\n",
            "Epoch: 163 | Batch_idx: 40 |  Loss: (0.0555) | Acc: (98.21%) (5154/5248)\n",
            "Epoch: 163 | Batch_idx: 50 |  Loss: (0.0531) | Acc: (98.25%) (6414/6528)\n",
            "Epoch: 163 | Batch_idx: 60 |  Loss: (0.0548) | Acc: (98.21%) (7668/7808)\n",
            "Epoch: 163 | Batch_idx: 70 |  Loss: (0.0550) | Acc: (98.22%) (8926/9088)\n",
            "Epoch: 163 | Batch_idx: 80 |  Loss: (0.0538) | Acc: (98.24%) (10186/10368)\n",
            "Epoch: 163 | Batch_idx: 90 |  Loss: (0.0535) | Acc: (98.22%) (11441/11648)\n",
            "Epoch: 163 | Batch_idx: 100 |  Loss: (0.0545) | Acc: (98.21%) (12697/12928)\n",
            "Epoch: 163 | Batch_idx: 110 |  Loss: (0.0534) | Acc: (98.25%) (13960/14208)\n",
            "Epoch: 163 | Batch_idx: 120 |  Loss: (0.0536) | Acc: (98.24%) (15215/15488)\n",
            "Epoch: 163 | Batch_idx: 130 |  Loss: (0.0541) | Acc: (98.20%) (16466/16768)\n",
            "Epoch: 163 | Batch_idx: 140 |  Loss: (0.0531) | Acc: (98.24%) (17730/18048)\n",
            "Epoch: 163 | Batch_idx: 150 |  Loss: (0.0527) | Acc: (98.23%) (18986/19328)\n",
            "Epoch: 163 | Batch_idx: 160 |  Loss: (0.0528) | Acc: (98.22%) (20242/20608)\n",
            "Epoch: 163 | Batch_idx: 170 |  Loss: (0.0525) | Acc: (98.24%) (21502/21888)\n",
            "Epoch: 163 | Batch_idx: 180 |  Loss: (0.0536) | Acc: (98.21%) (22753/23168)\n",
            "Epoch: 163 | Batch_idx: 190 |  Loss: (0.0545) | Acc: (98.16%) (23999/24448)\n",
            "Epoch: 163 | Batch_idx: 200 |  Loss: (0.0542) | Acc: (98.16%) (25254/25728)\n",
            "Epoch: 163 | Batch_idx: 210 |  Loss: (0.0540) | Acc: (98.17%) (26513/27008)\n",
            "Epoch: 163 | Batch_idx: 220 |  Loss: (0.0545) | Acc: (98.15%) (27766/28288)\n",
            "Epoch: 163 | Batch_idx: 230 |  Loss: (0.0540) | Acc: (98.17%) (29027/29568)\n",
            "Epoch: 163 | Batch_idx: 240 |  Loss: (0.0542) | Acc: (98.15%) (30276/30848)\n",
            "Epoch: 163 | Batch_idx: 250 |  Loss: (0.0538) | Acc: (98.16%) (31538/32128)\n",
            "Epoch: 163 | Batch_idx: 260 |  Loss: (0.0540) | Acc: (98.16%) (32792/33408)\n",
            "Epoch: 163 | Batch_idx: 270 |  Loss: (0.0535) | Acc: (98.17%) (34054/34688)\n",
            "Epoch: 163 | Batch_idx: 280 |  Loss: (0.0530) | Acc: (98.18%) (35314/35968)\n",
            "Epoch: 163 | Batch_idx: 290 |  Loss: (0.0533) | Acc: (98.16%) (36562/37248)\n",
            "Epoch: 163 | Batch_idx: 300 |  Loss: (0.0533) | Acc: (98.16%) (37820/38528)\n",
            "Epoch: 163 | Batch_idx: 310 |  Loss: (0.0532) | Acc: (98.17%) (39078/39808)\n",
            "Epoch: 163 | Batch_idx: 320 |  Loss: (0.0532) | Acc: (98.16%) (40334/41088)\n",
            "Epoch: 163 | Batch_idx: 330 |  Loss: (0.0529) | Acc: (98.16%) (41590/42368)\n",
            "Epoch: 163 | Batch_idx: 340 |  Loss: (0.0525) | Acc: (98.19%) (42856/43648)\n",
            "Epoch: 163 | Batch_idx: 350 |  Loss: (0.0525) | Acc: (98.20%) (44119/44928)\n",
            "Epoch: 163 | Batch_idx: 360 |  Loss: (0.0527) | Acc: (98.20%) (45378/46208)\n",
            "Epoch: 163 | Batch_idx: 370 |  Loss: (0.0529) | Acc: (98.20%) (46631/47488)\n",
            "Epoch: 163 | Batch_idx: 380 |  Loss: (0.0527) | Acc: (98.20%) (47891/48768)\n",
            "Epoch: 163 | Batch_idx: 390 |  Loss: (0.0530) | Acc: (98.20%) (49102/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3455) | Acc: (91.57%) (9157/10000)\n",
            "Epoch: 164 | Batch_idx: 0 |  Loss: (0.0323) | Acc: (99.22%) (127/128)\n",
            "Epoch: 164 | Batch_idx: 10 |  Loss: (0.0394) | Acc: (98.65%) (1389/1408)\n",
            "Epoch: 164 | Batch_idx: 20 |  Loss: (0.0476) | Acc: (98.44%) (2646/2688)\n",
            "Epoch: 164 | Batch_idx: 30 |  Loss: (0.0502) | Acc: (98.29%) (3900/3968)\n",
            "Epoch: 164 | Batch_idx: 40 |  Loss: (0.0514) | Acc: (98.15%) (5151/5248)\n",
            "Epoch: 164 | Batch_idx: 50 |  Loss: (0.0518) | Acc: (98.13%) (6406/6528)\n",
            "Epoch: 164 | Batch_idx: 60 |  Loss: (0.0496) | Acc: (98.21%) (7668/7808)\n",
            "Epoch: 164 | Batch_idx: 70 |  Loss: (0.0516) | Acc: (98.12%) (8917/9088)\n",
            "Epoch: 164 | Batch_idx: 80 |  Loss: (0.0512) | Acc: (98.10%) (10171/10368)\n",
            "Epoch: 164 | Batch_idx: 90 |  Loss: (0.0513) | Acc: (98.10%) (11427/11648)\n",
            "Epoch: 164 | Batch_idx: 100 |  Loss: (0.0510) | Acc: (98.11%) (12684/12928)\n",
            "Epoch: 164 | Batch_idx: 110 |  Loss: (0.0508) | Acc: (98.13%) (13942/14208)\n",
            "Epoch: 164 | Batch_idx: 120 |  Loss: (0.0508) | Acc: (98.13%) (15198/15488)\n",
            "Epoch: 164 | Batch_idx: 130 |  Loss: (0.0512) | Acc: (98.15%) (16457/16768)\n",
            "Epoch: 164 | Batch_idx: 140 |  Loss: (0.0516) | Acc: (98.14%) (17713/18048)\n",
            "Epoch: 164 | Batch_idx: 150 |  Loss: (0.0511) | Acc: (98.19%) (18979/19328)\n",
            "Epoch: 164 | Batch_idx: 160 |  Loss: (0.0504) | Acc: (98.22%) (20242/20608)\n",
            "Epoch: 164 | Batch_idx: 170 |  Loss: (0.0506) | Acc: (98.23%) (21500/21888)\n",
            "Epoch: 164 | Batch_idx: 180 |  Loss: (0.0505) | Acc: (98.27%) (22767/23168)\n",
            "Epoch: 164 | Batch_idx: 190 |  Loss: (0.0501) | Acc: (98.27%) (24024/24448)\n",
            "Epoch: 164 | Batch_idx: 200 |  Loss: (0.0506) | Acc: (98.25%) (25279/25728)\n",
            "Epoch: 164 | Batch_idx: 210 |  Loss: (0.0513) | Acc: (98.25%) (26535/27008)\n",
            "Epoch: 164 | Batch_idx: 220 |  Loss: (0.0513) | Acc: (98.25%) (27792/28288)\n",
            "Epoch: 164 | Batch_idx: 230 |  Loss: (0.0513) | Acc: (98.26%) (29053/29568)\n",
            "Epoch: 164 | Batch_idx: 240 |  Loss: (0.0516) | Acc: (98.24%) (30304/30848)\n",
            "Epoch: 164 | Batch_idx: 250 |  Loss: (0.0512) | Acc: (98.25%) (31565/32128)\n",
            "Epoch: 164 | Batch_idx: 260 |  Loss: (0.0514) | Acc: (98.24%) (32820/33408)\n",
            "Epoch: 164 | Batch_idx: 270 |  Loss: (0.0515) | Acc: (98.24%) (34077/34688)\n",
            "Epoch: 164 | Batch_idx: 280 |  Loss: (0.0515) | Acc: (98.24%) (35334/35968)\n",
            "Epoch: 164 | Batch_idx: 290 |  Loss: (0.0513) | Acc: (98.25%) (36596/37248)\n",
            "Epoch: 164 | Batch_idx: 300 |  Loss: (0.0509) | Acc: (98.25%) (37853/38528)\n",
            "Epoch: 164 | Batch_idx: 310 |  Loss: (0.0509) | Acc: (98.24%) (39107/39808)\n",
            "Epoch: 164 | Batch_idx: 320 |  Loss: (0.0505) | Acc: (98.25%) (40371/41088)\n",
            "Epoch: 164 | Batch_idx: 330 |  Loss: (0.0507) | Acc: (98.24%) (41624/42368)\n",
            "Epoch: 164 | Batch_idx: 340 |  Loss: (0.0510) | Acc: (98.23%) (42877/43648)\n",
            "Epoch: 164 | Batch_idx: 350 |  Loss: (0.0508) | Acc: (98.24%) (44138/44928)\n",
            "Epoch: 164 | Batch_idx: 360 |  Loss: (0.0508) | Acc: (98.25%) (45399/46208)\n",
            "Epoch: 164 | Batch_idx: 370 |  Loss: (0.0508) | Acc: (98.25%) (46655/47488)\n",
            "Epoch: 164 | Batch_idx: 380 |  Loss: (0.0511) | Acc: (98.23%) (47905/48768)\n",
            "Epoch: 164 | Batch_idx: 390 |  Loss: (0.0509) | Acc: (98.23%) (49117/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3483) | Acc: (91.48%) (9148/10000)\n",
            "0 hours 55 mins 25 secs for training\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
        "start_time = time.time()\n",
        "batch_size = 128\n",
        "learning_rate = 0.1 #Changed the initial Learning rate\n",
        "root_dir = 'drive/app/cifar10/'\n",
        "default_directory = 'drive/app/torch/save_models'\n",
        "\n",
        "# Data Augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
        "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
        "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "# automatically download\n",
        "train_dataset = datasets.CIFAR10(root=root_dir,\n",
        "                                 train=True,\n",
        "                                 transform=transform_train,\n",
        "                                 download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root=root_dir,\n",
        "                                train=False,\n",
        "                                transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
        "                                           num_workers=4)           # CPU loader number\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
        "                                          num_workers=4)            # CPU loader number\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.5),                                       ################### Added\n",
        "\t    \n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.5),                                       ################### Added\n",
        "\t    \n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5), #################### Dropout value set to 0.3\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5), #################### Dropout value set to 0.3\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = VGG()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=1e-4,\n",
        "                                nesterov=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if torch.cuda.device_count() > 0:\n",
        "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "    cudnn.benchmark = True\n",
        "else:\n",
        "    print(\"USE ONLY CPU!\")\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0 \n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "\n",
        "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    model_filename = os.path.join(directory, filename)\n",
        "    torch.save(state, model_filename)\n",
        "    print(\"=> saving checkpoint\")\n",
        "\n",
        "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
        "\n",
        "    model_filename = os.path.join(directory, filename)\n",
        "    if os.path.exists(model_filename):\n",
        "        print(\"=> loading checkpoint\")\n",
        "        state = torch.load(model_filename)\n",
        "        return state\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, 165):#################### Changing epoch\n",
        "\n",
        "    if epoch < 80:\n",
        "        lr = learning_rate\n",
        "    elif epoch < 120:\n",
        "        lr = learning_rate * 0.1\n",
        "    else:\n",
        "        lr = learning_rate * 0.01\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    train(epoch)\n",
        "    save_checkpoint(default_directory, {\n",
        "        'epoch': epoch,\n",
        "        'model': model,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    })\n",
        "    test()  \n",
        "\n",
        "now = time.gmtime(time.time() - start_time)\n",
        "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z3679FisFAmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}